{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f149fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torchvision.models import resnet50\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from natsort import natsorted\n",
    "\n",
    "MAX_FRAMES = 2\n",
    "# define the network\n",
    "IMGM_PATH = 'tutorials/offline_models/resnet/resnet'\n",
    "TRAIN_PATH = 'datasets/train_big_2'\n",
    "VAL_PATH = 'datasets/val_big_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b1c545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Request GPU device 0\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    # If no GPU is available, fall back to CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c5ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskDataset(Dataset):\n",
    "    # todo: this is not the most efficient way to access data, since each time it has to read from the directory \n",
    "    def __init__(self, root_dir,):\n",
    "        self.root_dir = root_dir\n",
    "        # preprocessing steps for pretrained ResNet models\n",
    "        self.transform = transforms.Compose([\n",
    "                            transforms.Resize(224),\n",
    "                            transforms.CenterCrop(224), # todo: to delete for shapenet task; why?\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                        ])\n",
    "\n",
    "        # check the size of the dataset\n",
    "        self.dataset_size = 0\n",
    "        items = os.listdir(self.root_dir)\n",
    "        for item in items:\n",
    "            item_path = os.path.join(self.root_dir, item)\n",
    "            # Check if the item is a directory\n",
    "            if os.path.isdir(item_path):\n",
    "                self.dataset_size += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trial_path = os.path.join(self.root_dir, \"trial%d\"%idx)\n",
    "        images = []\n",
    "        \n",
    "        for fp in natsorted(os.listdir(trial_path)):\n",
    "            fp = os.path.join(trial_path, fp)\n",
    "\n",
    "            if fp[-4:] == '.png':\n",
    "                img = Image.open(fp)\n",
    "                img = self.transform(img)\n",
    "                images.append(img)\n",
    "            elif 'trial_info' in fp:\n",
    "                info = json.load(open(fp))\n",
    "                \n",
    "                actions = self._action_map(info[\"answers\"])\n",
    "\n",
    "                # npads = MAX_FRAMES - len(actions)\n",
    "                # actions.extend([-1 for _ in range(0,npads)])\n",
    "                \n",
    "                instructions = info['instruction']\n",
    "        \n",
    "        images = torch.stack(images)\n",
    "\n",
    "        return images, torch.tensor(actions)\n",
    "    \n",
    "    def _action_map(self, actions):\n",
    "        updated_actions = []\n",
    "        for action in actions:\n",
    "            if action == \"null\":\n",
    "                updated_actions.append(2)\n",
    "            elif action == \"false\":\n",
    "                updated_actions.append(0)\n",
    "            elif action == \"true\":\n",
    "                updated_actions.append(1)\n",
    "        return updated_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "275fb447",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "\n",
    "class CNNRNNNet(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, dim_transformer_ffl=2048, nhead = 16, blocks=2, output_size = 3,):\n",
    "        super().__init__()\n",
    "\n",
    "        # set up the CNN model\n",
    "        self.cnnmodel = torch.load(IMGM_PATH, map_location=device)\n",
    "        # freeze layers of cnn model\n",
    "        for paras in self.cnnmodel.parameters():\n",
    "            paras.requires_grad = False\n",
    "        # get relu activation of last block of resnet50\n",
    "        \n",
    "        self.cnnmodel.layer4[2].relu.register_forward_hook(get_activation('relu'))\n",
    "\n",
    "        self.cnnlayer = torch.nn.Conv2d(2048, hidden_size, 1) # we can also bring the resnet embedding dim to a number different from hidden size\n",
    "\n",
    "        self.input_size = hidden_size*7*7\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.in2hidden = nn.Linear(self.input_size, hidden_size)\n",
    "        self.layer_norm_in = nn.LayerNorm(self.hidden_size)\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = self.hidden_size, \n",
    "            hidden_size = self.hidden_size,\n",
    "            nonlinearity = \"relu\", # guarnatee positive activations\n",
    "            )\n",
    "\n",
    "        self.pos_emb = PositionalEncoding(hidden_size, MAX_FRAMES)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, dim_feedforward=dim_transformer_ffl, nhead=nhead, batch_first=False)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=blocks)\n",
    "        \n",
    "        self.layer_norm_rnn = nn.LayerNorm(self.hidden_size)\n",
    "        self.hidden2output = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_img, hidden_state = None, is_noise = False,):\n",
    "        # preprocess image with resnet\n",
    "        self.batch_size = input_img.shape[0]\n",
    "        self.seq_len = input_img.shape[1]\n",
    "        \n",
    "        x = torch.swapaxes(input_img, 0, 1).float()# (seq_len, batchsize, nc, w, h)\n",
    "        \n",
    "        x_acts = []\n",
    "        cnn_acts = []\n",
    "        for i in range(self.seq_len):\n",
    "            temp = self.cnnmodel(x[i,:,:,:,:])\n",
    "            cnn_acts.append(activation[\"relu\"])\n",
    "            x_act = self.cnnlayer(activation[\"relu\"])\n",
    "            x_acts.append(x_act) # (batchsize, nc, w, h) = (batchsize, 2048, 7,7)\n",
    "\n",
    "        x_acts = torch.stack(x_acts, axis = 0) # (seqlen, batchsize,nc, w,h) rnn_activations\n",
    "        # self.cnn_acts = torch.stack(cnn_acts, axis = 0) # (seqlen, batchsize, nc, w,h) \n",
    "        # self.cnn_acts_down = x_acts\n",
    "        \n",
    "        \n",
    "        x_acts = x_acts.reshape(x_acts.shape[0], self.batch_size, -1) # flatten nc,w,h into one dim\n",
    "        \n",
    "        \n",
    "        \"\"\" RNN METHOD \"\"\"\n",
    "        # if hidden_state == None:\n",
    "        #     self.hidden_state = self.init_hidden(batch_size = self.batch_size)\n",
    "        # hidden_x = self.layer_norm_in(torch.relu(self.in2hidden(x_acts.float()))).to(device)\n",
    "        # rnn_output, _ = self.rnn(hidden_x, self.hidden_state.to(device))\n",
    "        # rnn_output = self.layer_norm_rnn(rnn_output)\n",
    "        # out = self.hidden2output(torch.tanh(rnn_output))\n",
    "        \n",
    "        \"\"\" TRANSFORMER METHOD \"\"\"\n",
    "        hidden_x = self.layer_norm_in(self.pos_emb(self.in2hidden(x_acts.float())))\n",
    "        encoder_output = self.encoder(hidden_x)\n",
    "        out = self.hidden2output(encoder_output)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1, batch_size, self.hidden_size))\n",
    "            \n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding module taken from PyTorch Tutorial\n",
    "    # Link: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = MAX_FRAMES):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f79c38f6-82f8-4aa3-bd7b-f2ef123b3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try warmup (v important)\n",
    "# Learning rate (cosine learning rate)\n",
    "# Ask what worked for his transformer\n",
    "# VINT -> navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5b88101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "train_TD = TaskDataset(TRAIN_PATH)\n",
    "val_TD = TaskDataset(VAL_PATH)\n",
    "                 \n",
    "batch_size = 256\n",
    "data_loaders = [DataLoader(train_TD, batch_size=batch_size, shuffle=True),\n",
    "                DataLoader(val_TD, batch_size=batch_size, shuffle=False)]\n",
    "                \n",
    "model = CNNRNNNet(hidden_size = 256, output_size = 3,).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 5, T_mult=1 )\n",
    "\n",
    "print(len(train_TD))\n",
    "print(len(val_TD))\n",
    "                                                                 # torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8406700-507d-4f18-b598-e89d23450447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "# Calculates the number of correct null action predictions and the number of correct non-null action predictions\n",
    "def correct(preds, targs):\n",
    "    null_idxs = torch.where(targs.cpu() == 2)\n",
    "    non_null_idxs = torch.where(targs.cpu() < 2)\n",
    "    \n",
    "    null_preds = preds[null_idxs]\n",
    "    non_null_preds = preds[non_null_idxs]\n",
    "    \n",
    "#     print('null preds', null_preds)\n",
    "#     print('null trags', targs[null_idxs])\n",
    "    \n",
    "#     print('non null preds', non_null_preds)\n",
    "#     print('non null trags', targs[non_null_idxs])\n",
    "    \n",
    "    # print(sklearn.metrics.classification_report(targs[non_null_idxs].cpu().numpy(), non_null_preds.cpu().numpy()))\n",
    "    \n",
    "    c_null = torch.sum(null_preds == targs[null_idxs])\n",
    "    n_null = len(null_preds)\n",
    "    null_acc = c_null/n_null\n",
    "    \n",
    "    c_non_null = torch.sum(non_null_preds == targs[non_null_idxs])\n",
    "    n_non_null = len(non_null_preds)\n",
    "    non_null_acc = c_non_null/n_non_null\n",
    "    \n",
    "    return null_acc, non_null_acc\n",
    "\n",
    "# Calculates the loss for a forward pass for both null and non-null action predictions (this is to avoid overfitting to null actions)\n",
    "def loss(preds, targs):\n",
    "    # Find indexes of null frames and non-null frames\n",
    "    null_idxs = torch.where(targs.cpu() == 2)\n",
    "    non_null_idxs = torch.where(targs.cpu() < 2)\n",
    "    \n",
    "    # Add batch dimension and reorder into (batch_size, n_classes, seq_len)\n",
    "    null_preds = preds[null_idxs]\n",
    "    non_null_preds = preds[non_null_idxs]\n",
    "\n",
    "    null_loss = criterion(null_preds, targs[null_idxs])\n",
    "    non_null_loss = criterion(non_null_preds, targs[non_null_idxs])\n",
    "    \n",
    "    return null_loss, non_null_loss, len(null_idxs)/len(non_null_idxs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f581fced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "train null acc:  tensor(0., device='cuda:0')\n",
      "train non-null acc:  tensor(0.5000, device='cuda:0')\n",
      "average val null acc 0.00\n",
      "average val non-null acc 0.50\n",
      "epoch:  1\n",
      "train null acc:  tensor(0., device='cuda:0')\n",
      "train non-null acc:  tensor(0.5329, device='cuda:0')\n",
      "average val null acc 0.00\n",
      "average val non-null acc 0.51\n",
      "epoch:  2\n",
      "train null acc:  tensor(0., device='cuda:0')\n",
      "train non-null acc:  tensor(0.5395, device='cuda:0')\n",
      "average val null acc 0.00\n",
      "average val non-null acc 0.51\n",
      "epoch:  3\n",
      "train null acc:  tensor(0., device='cuda:0')\n",
      "train non-null acc:  tensor(0.5789, device='cuda:0')\n",
      "average val null acc 0.00\n",
      "average val non-null acc 0.51\n",
      "epoch:  4\n",
      "train null acc:  tensor(0., device='cuda:0')\n",
      "train non-null acc:  tensor(0.5395, device='cuda:0')\n",
      "average val null acc 0.00\n",
      "average val non-null acc 0.51\n",
      "epoch:  5\n",
      "train null acc:  tensor(0., device='cuda:0')\n",
      "train non-null acc:  tensor(0.5526, device='cuda:0')\n",
      "average val null acc 0.00\n",
      "average val non-null acc 0.51\n"
     ]
    }
   ],
   "source": [
    "all_loss = {'train_null_loss':[],'train_non_null_loss':[], 'val_null_loss':[], 'val_non_null_loss':[]}\n",
    "all_acc = {'train_null_acc':[], 'train_non_null_acc':[], 'val_null_acc':[], 'val_non_null_acc':[]}\n",
    "\n",
    "def validate():\n",
    "    \n",
    "    null_accs = []\n",
    "    non_null_accs = []\n",
    "    null_losses = []\n",
    "    non_null_losses = []\n",
    "\n",
    "    for images, actions in data_loaders[1]:\n",
    "        model.eval()\n",
    "        output = model(images.to(device))\n",
    "        null_loss, non_null_loss, scale = loss(output.permute(1,0,2).reshape(-1,3), actions.type(torch.LongTensor).reshape(-1).to(device))\n",
    "        null_losses.append(null_loss.item())\n",
    "        non_null_losses.append(non_null_loss.item())\n",
    "        \n",
    "        # print(actions.reshape(-1)[0:10])\n",
    "        _, predicted = torch.max(output.data, 2)\n",
    "        predicted = predicted.permute(1,0).reshape(-1)\n",
    "        # print(predicted[0:10])\n",
    "        null_acc, non_null_acc = correct(predicted, actions.reshape(-1).to(device))\n",
    "        null_accs.append(null_acc.cpu())\n",
    "        non_null_accs.append(non_null_acc.cpu())\n",
    "    \n",
    "    all_loss['val_null_loss'].append(sum(null_losses)/len(null_losses))\n",
    "    all_loss['val_non_null_loss'].append(sum(non_null_losses)/len(non_null_losses))\n",
    "    all_acc['val_null_acc'].append(sum(null_accs)/len(null_accs))\n",
    "    all_acc['val_non_null_acc'].append(sum(non_null_accs)/len(non_null_accs))\n",
    "    \n",
    "    print(\"average val null acc %.2f\" % (sum(null_accs)/len(null_accs)))\n",
    "    print(\"average val non-null acc %.2f\" % (sum(non_null_accs)/len(non_null_accs)))\n",
    "\n",
    "for epoch in range(40):\n",
    "    i = 0\n",
    "    \n",
    "    null_accs = []\n",
    "    non_null_accs = []\n",
    "    null_losses = []\n",
    "    non_null_losses = []\n",
    "    for images, actions in data_loaders[0]:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images.to(device)) # (seq_len, batch, n_classes)\n",
    "        null_loss, non_null_loss, scale = loss(output.permute(1,0,2).reshape(-1,3), actions.type(torch.LongTensor).reshape(-1).to(device))\n",
    "        \n",
    "        null_losses.append(null_loss.item())\n",
    "        non_null_losses.append(non_null_loss.item())\n",
    "        # if i%30 != 0:\n",
    "        #     null_loss = 0\n",
    "        null_loss = 0\n",
    "        train_loss = null_loss + non_null_loss # *scale *(1/(scale**(epoch/2)))\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(output.data, 2)\n",
    "        predicted = predicted.permute(1,0).reshape(-1)\n",
    "        null_acc, non_null_acc = correct(predicted, actions.reshape(-1).to(device))\n",
    "        null_accs.append(null_acc.cpu())\n",
    "        non_null_accs.append(non_null_acc.cpu())\n",
    "        \n",
    "        scheduler.step(epoch + i / len(data_loaders[0]))\n",
    "        i += 1\n",
    "\n",
    "    print('epoch: ', epoch)\n",
    "    print('train null acc: ', null_acc)\n",
    "    print('train non-null acc: ', non_null_acc)\n",
    "    validate()\n",
    "    all_loss['train_null_loss'].append(sum(null_losses)/len(null_losses))\n",
    "    all_loss['train_non_null_loss'].append(sum(non_null_losses)/len(non_null_losses))\n",
    "    all_acc['train_null_acc'].append(sum(null_accs)/len(null_accs))\n",
    "    all_acc['train_non_null_acc'].append(sum(non_null_accs)/len(non_null_accs))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee51968-c800-46ae-abeb-f157fe55fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d206a350-6c48-4bb2-99aa-02535c3be585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7643ab9-6f67-4137-b208-a9abacde41bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_dict(dict_arrays, use_xlabel='Epochs', use_ylabel='Value', use_title=None):\n",
    "    # Font size select custom or adjusted on `magnify` value.\n",
    "    font_size = np.interp(0.1, [0.1,1], [10.5,50])\n",
    "\n",
    "    # Font variables dictionary. Keep it in this format for future updates.\n",
    "    font_dict = dict(\n",
    "        family='DejaVu Sans',\n",
    "        color='black',\n",
    "        weight='normal',\n",
    "        size=font_size,\n",
    "        )\n",
    "\n",
    "    # Single plot figure.\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    # Use maximum length of steps. In case each arrya has different lengths.\n",
    "    max_steps = []\n",
    "\n",
    "    # Plot each array.\n",
    "    for index, (use_label, array) in enumerate(dict_arrays.items()):\n",
    "        # Set steps plotted on x-axis - we can use step if 1 unit has different value.\n",
    "        if 0 > 0:\n",
    "            # Offset all steps by start_step.\n",
    "            steps = np.array(range(0, len(array))) * 1 + 0\n",
    "            max_steps = steps if len(max_steps) < len(steps) else max_steps\n",
    "        else:\n",
    "            steps = np.array(range(1, len(array) + 1)) * 1\n",
    "            max_steps = steps if len(max_steps) < len(steps) else max_steps\n",
    "\n",
    "        # Plot array as a single line.\n",
    "        plt.plot(steps, array, linestyle=(['-'] * len(dict_arrays))[index], label=use_label)\n",
    "\n",
    "        # Plots points values.\n",
    "        if ([False] * len(dict_arrays))[index]:\n",
    "            # Loop through each point and plot the label.\n",
    "            for x, y in zip(steps, array):\n",
    "                # Add text label to plot.\n",
    "                plt.text(x, y, str(round(y, 3)), fontdict=font_dict)\n",
    "\n",
    "    # Set horizontal axis name.\n",
    "    plt.xlabel(use_xlabel, fontdict=font_dict)\n",
    "\n",
    "    # Use x ticks with steps or labels.\n",
    "    plt.xticks(max_steps, None, rotation=0)\n",
    "\n",
    "    # Set vertical axis name.\n",
    "    plt.ylabel(use_ylabel, fontdict=font_dict)\n",
    "\n",
    "    # Adjust both axis labels font size at same time.\n",
    "    plt.tick_params(labelsize=font_dict['size'])\n",
    "\n",
    "    # Place legend best position.\n",
    "    plt.legend(loc='best', fontsize=font_dict['size'])\n",
    "\n",
    "    # Adjust font for title.\n",
    "    font_dict['size'] *= 1.8\n",
    "\n",
    "    # Set title of figure.\n",
    "    plt.title(use_title, fontdict=font_dict)\n",
    "\n",
    "    # Rescale `magnify` to be used on inches.\n",
    "    magnify = 0.1\n",
    "    magnify *= 15\n",
    "\n",
    "    # Display grid depending on `use_grid`.\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Make figure nice.\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Get figure object from plot.\n",
    "    fig = plt.gcf()\n",
    "\n",
    "    # Get size of figure.\n",
    "    figsize = fig.get_size_inches()\n",
    "\n",
    "    # Change size depending on height and width variables.\n",
    "    figsize = [figsize[0] * 3 * magnify, figsize[1] * 1 * magnify]\n",
    "\n",
    "    # Set the new figure size with magnify.\n",
    "    fig.set_size_inches(figsize)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ca46c-6f6f-4e5c-b6ca-0bca34ef867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dict(all_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c890f54-12dc-4745-a2c0-2f546d560d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dict(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5732e9ae-83f1-4cad-a55f-056ddc2df4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52377c4a-7321-4ec7-9f38-8ed0de214974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a00e45-0832-4a8d-8fce-669cc839aed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlti kernel",
   "language": "python",
   "name": "mlti-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
