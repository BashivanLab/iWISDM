{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torchvision.models import resnet50\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from natsort import natsorted\n",
    "\n",
    "MAX_FRAMES = 3\n",
    "# define the network\n",
    "IMGM_PATH = 'tutorials/offline_models/resnet/resnet'\n",
    "TRAIN_PATH = 'datasets/train_big_2'\n",
    "VAL_PATH = 'datasets/val_big_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b1c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Request GPU device 0\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    # If no GPU is available, fall back to CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c5ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskDataset(Dataset):\n",
    "    # todo: this is not the most efficient way to access data, since each time it has to read from the directory \n",
    "    def __init__(self, root_dir,):\n",
    "        self.root_dir = root_dir\n",
    "        # preprocessing steps for pretrained ResNet models\n",
    "        self.transform = transforms.Compose([\n",
    "                            transforms.Resize(224),\n",
    "                            transforms.CenterCrop(224), # todo: to delete for shapenet task; why?\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                        ])\n",
    "\n",
    "        # check the size of the dataset\n",
    "        self.dataset_size = 0\n",
    "        items = os.listdir(self.root_dir)\n",
    "        for item in items:\n",
    "            item_path = os.path.join(self.root_dir, item)\n",
    "            # Check if the item is a directory\n",
    "            if os.path.isdir(item_path):\n",
    "                self.dataset_size += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trial_path = os.path.join(self.root_dir, \"trial%d\"%idx)\n",
    "        images = []\n",
    "        \n",
    "        for fp in natsorted(os.listdir(trial_path)):\n",
    "            fp = os.path.join(trial_path, fp)\n",
    "\n",
    "            if fp[-4:] == '.png':\n",
    "                img = Image.open(fp)\n",
    "                img = self.transform(img)\n",
    "                images.append(img)\n",
    "            elif 'trial_info' in fp:\n",
    "                info = json.load(open(fp))\n",
    "                \n",
    "                actions = self._action_map(info[\"answers\"])\n",
    "\n",
    "                # npads = MAX_FRAMES - len(actions)\n",
    "                # actions.extend([-1 for _ in range(0,npads)])\n",
    "                \n",
    "                instructions = info['instruction']\n",
    "        \n",
    "        images = torch.stack(images)\n",
    "\n",
    "        return images, torch.tensor(actions)\n",
    "    \n",
    "    def _action_map(self, actions):\n",
    "        updated_actions = []\n",
    "        for action in actions:\n",
    "            if action == \"null\":\n",
    "                updated_actions.append(2)\n",
    "            elif action == \"false\":\n",
    "                updated_actions.append(0)\n",
    "            elif action == \"true\":\n",
    "                updated_actions.append(1)\n",
    "        return updated_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275fb447",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "\n",
    "class CNNRNNNet(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, dim_transformer_ffl=2048, nhead = 16, blocks=2, output_size = 3,):\n",
    "        super().__init__()\n",
    "\n",
    "        # set up the CNN model\n",
    "        self.cnnmodel = torch.load(IMGM_PATH, map_location=device)\n",
    "        # freeze layers of cnn model\n",
    "        for paras in self.cnnmodel.parameters():\n",
    "            paras.requires_grad = False\n",
    "        # get relu activation of last block of resnet50\n",
    "        \n",
    "        self.cnnmodel.layer4[2].relu.register_forward_hook(get_activation('relu'))\n",
    "\n",
    "        self.cnnlayer = torch.nn.Conv2d(2048, hidden_size, 1) # we can also bring the resnet embedding dim to a number different from hidden size\n",
    "\n",
    "        self.input_size = hidden_size*7*7\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.in2hidden = nn.Linear(self.input_size, hidden_size)\n",
    "        self.layer_norm_in = nn.LayerNorm(self.hidden_size)\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = self.hidden_size, \n",
    "            hidden_size = self.hidden_size,\n",
    "            nonlinearity = \"relu\", # guarnatee positive activations\n",
    "            )\n",
    "\n",
    "        self.pos_emb = PositionalEncoding(hidden_size, MAX_FRAMES)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, dim_feedforward=dim_transformer_ffl, nhead=nhead, batch_first=False)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=blocks)\n",
    "        \n",
    "        self.layer_norm_rnn = nn.LayerNorm(self.hidden_size)\n",
    "        self.hidden2output = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_img, hidden_state = None, is_noise = False,):\n",
    "        # preprocess image with resnet\n",
    "        self.batch_size = input_img.shape[0]\n",
    "        self.seq_len = input_img.shape[1]\n",
    "        \n",
    "        x = torch.swapaxes(input_img, 0, 1).float()# (seq_len, batchsize, nc, w, h)\n",
    "        \n",
    "        x_acts = []\n",
    "        cnn_acts = []\n",
    "        for i in range(self.seq_len):\n",
    "            temp = self.cnnmodel(x[i,:,:,:,:])\n",
    "            cnn_acts.append(activation[\"relu\"])\n",
    "            x_act = self.cnnlayer(activation[\"relu\"])\n",
    "            x_acts.append(x_act) # (batchsize, nc, w, h) = (batchsize, 2048, 7,7)\n",
    "\n",
    "        x_acts = torch.stack(x_acts, axis = 0) # (seqlen, batchsize,nc, w,h) rnn_activations\n",
    "        # self.cnn_acts = torch.stack(cnn_acts, axis = 0) # (seqlen, batchsize, nc, w,h) \n",
    "        # self.cnn_acts_down = x_acts\n",
    "        \n",
    "        \n",
    "        x_acts = x_acts.reshape(x_acts.shape[0], self.batch_size, -1) # flatten nc,w,h into one dim\n",
    "        \n",
    "        \n",
    "        \"\"\" RNN METHOD \"\"\"\n",
    "        # if hidden_state == None:\n",
    "        #     self.hidden_state = self.init_hidden(batch_size = self.batch_size)\n",
    "        # hidden_x = self.layer_norm_in(torch.relu(self.in2hidden(x_acts.float()))).to(device)\n",
    "        # rnn_output, _ = self.rnn(hidden_x, self.hidden_state.to(device))\n",
    "        # rnn_output = self.layer_norm_rnn(rnn_output)\n",
    "        # out = self.hidden2output(torch.tanh(rnn_output))\n",
    "        \n",
    "        \"\"\" TRANSFORMER METHOD \"\"\"\n",
    "        hidden_x = self.layer_norm_in(self.pos_emb(self.in2hidden(x_acts.float())))\n",
    "        encoder_output = self.encoder(hidden_x)\n",
    "        out = self.hidden2output(encoder_output)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1, batch_size, self.hidden_size))\n",
    "            \n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding module taken from PyTorch Tutorial\n",
    "    # Link: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = MAX_FRAMES):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c38f6-82f8-4aa3-bd7b-f2ef123b3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try warmup (v important)\n",
    "# Learning rate (cosine learning rate)\n",
    "# Ask what worked for his transformer\n",
    "# VINT -> navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b88101",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_TD = TaskDataset(TRAIN_PATH)\n",
    "val_TD = TaskDataset(VAL_PATH)\n",
    "                 \n",
    "batch_size = 256\n",
    "data_loaders = [DataLoader(train_TD, batch_size=batch_size, shuffle=True),\n",
    "                DataLoader(val_TD, batch_size=batch_size, shuffle=False)]\n",
    "                \n",
    "model = CNNRNNNet(hidden_size = 256, output_size = 3,).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 5, T_mult=1 )\n",
    "\n",
    "print(len(train_TD))\n",
    "print(len(val_TD))\n",
    "                                                                 # torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8406700-507d-4f18-b598-e89d23450447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "# Calculates the number of correct null action predictions and the number of correct non-null action predictions\n",
    "def correct(preds, targs):\n",
    "    null_idxs = torch.where(targs.cpu() == 2)\n",
    "    non_null_idxs = torch.where(targs.cpu() < 2)\n",
    "    \n",
    "    null_preds = preds[null_idxs]\n",
    "    non_null_preds = preds[non_null_idxs]\n",
    "    \n",
    "#     print('null preds', null_preds)\n",
    "#     print('null trags', targs[null_idxs])\n",
    "    \n",
    "#     print('non null preds', non_null_preds)\n",
    "#     print('non null trags', targs[non_null_idxs])\n",
    "    \n",
    "    print(sklearn.metrics.classification_report(targs[non_null_idxs].cpu().numpy(), non_null_preds.cpu().numpy()))\n",
    "    \n",
    "    c_null = torch.sum(null_preds == targs[null_idxs])\n",
    "    n_null = len(null_preds)\n",
    "    null_acc = c_null/n_null\n",
    "    \n",
    "    c_non_null = torch.sum(non_null_preds == targs[non_null_idxs])\n",
    "    n_non_null = len(non_null_preds)\n",
    "    non_null_acc = c_non_null/n_non_null\n",
    "    \n",
    "    return null_acc, non_null_acc\n",
    "\n",
    "# Calculates the loss for a forward pass for both null and non-null action predictions (this is to avoid overfitting to null actions)\n",
    "def loss(preds, targs):\n",
    "    # Find indexes of null frames and non-null frames\n",
    "    null_idxs = torch.where(targs.cpu() == 2)\n",
    "    non_null_idxs = torch.where(targs.cpu() < 2)\n",
    "    \n",
    "    # Add batch dimension and reorder into (batch_size, n_classes, seq_len)\n",
    "    null_preds = preds[null_idxs]\n",
    "    non_null_preds = preds[non_null_idxs]\n",
    "\n",
    "    null_loss = criterion(null_preds, targs[null_idxs])\n",
    "    non_null_loss = criterion(non_null_preds, targs[non_null_idxs])\n",
    "    \n",
    "    return null_loss, non_null_loss, len(null_idxs)/len(non_null_idxs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f581fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss = {'train_null_loss':[],'train_non_null_loss':[], 'val_null_loss':[], 'val_non_null_loss':[]}\n",
    "all_acc = {'train_null_acc':[], 'train_non_null_acc':[], 'val_null_acc':[], 'val_non_null_acc':[]}\n",
    "\n",
    "def validate():\n",
    "    \n",
    "    null_accs = []\n",
    "    non_null_accs = []\n",
    "    null_losses = []\n",
    "    non_null_losses = []\n",
    "\n",
    "    for images, actions in data_loaders[1]:\n",
    "        model.eval()\n",
    "        output = model(images.to(device))\n",
    "        null_loss, non_null_loss, scale = loss(output.permute(1,0,2).reshape(-1,3), actions.type(torch.LongTensor).reshape(-1).to(device))\n",
    "        null_losses.append(null_loss.item())\n",
    "        non_null_losses.append(non_null_loss.item())\n",
    "        \n",
    "        # print(actions.reshape(-1)[0:10])\n",
    "        _, predicted = torch.max(output.data, 2)\n",
    "        predicted = predicted.permute(1,0).reshape(-1)\n",
    "        # print(predicted[0:10])\n",
    "        null_acc, non_null_acc = correct(predicted, actions.reshape(-1).to(device))\n",
    "        null_accs.append(null_acc.cpu())\n",
    "        non_null_accs.append(non_null_acc.cpu())\n",
    "    \n",
    "    all_loss['val_null_loss'].append(sum(null_losses)/len(null_losses))\n",
    "    all_loss['val_non_null_loss'].append(sum(non_null_losses)/len(non_null_losses))\n",
    "    all_acc['val_null_acc'].append(sum(null_accs)/len(null_accs))\n",
    "    all_acc['val_non_null_acc'].append(sum(non_null_accs)/len(non_null_accs))\n",
    "    \n",
    "    print(\"average val null acc %.2f\" % (sum(null_accs)/len(null_accs)))\n",
    "    print(\"average val non-null acc %.2f\" % (sum(non_null_accs)/len(non_null_accs)))\n",
    "\n",
    "for epoch in range(40):\n",
    "    i = 0\n",
    "    \n",
    "    null_accs = []\n",
    "    non_null_accs = []\n",
    "    null_losses = []\n",
    "    non_null_losses = []\n",
    "    for images, actions in data_loaders[0]:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images.to(device)) # (seq_len, batch, n_classes)\n",
    "        null_loss, non_null_loss, scale = loss(output.permute(1,0,2).reshape(-1,3), actions.type(torch.LongTensor).reshape(-1).to(device))\n",
    "        \n",
    "        null_losses.append(null_loss.item())\n",
    "        non_null_losses.append(non_null_loss.item())\n",
    "        # if i%30 != 0:\n",
    "        #     null_loss = 0\n",
    "        null_loss = 0\n",
    "        train_loss = null_loss + non_null_loss # *scale *(1/(scale**(epoch/2)))\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(output.data, 2)\n",
    "        predicted = predicted.permute(1,0).reshape(-1)\n",
    "        null_acc, non_null_acc = correct(predicted, actions.reshape(-1).to(device))\n",
    "        null_accs.append(null_acc.cpu())\n",
    "        non_null_accs.append(non_null_acc.cpu())\n",
    "        \n",
    "        scheduler.step(epoch + i / len(data_loaders[0]))\n",
    "        i += 1\n",
    "\n",
    "    print('epoch: ', epoch)\n",
    "    print('train null acc: ', null_acc)\n",
    "    print('train non-null acc: ', non_null_acc)\n",
    "    validate()\n",
    "    all_loss['train_null_loss'].append(sum(null_losses)/len(null_losses))\n",
    "    all_loss['train_non_null_loss'].append(sum(non_null_losses)/len(non_null_losses))\n",
    "    all_acc['train_null_acc'].append(sum(null_accs)/len(null_accs))\n",
    "    all_acc['train_non_null_acc'].append(sum(non_null_accs)/len(non_null_accs))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cee51968-c800-46ae-abeb-f157fe55fcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.89      0.72       141\n",
      "           1       0.67      0.26      0.38       115\n",
      "\n",
      "    accuracy                           0.61       256\n",
      "   macro avg       0.63      0.58      0.55       256\n",
      "weighted avg       0.63      0.61      0.56       256\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.88      0.62       115\n",
      "           1       0.70      0.23      0.35       141\n",
      "\n",
      "    accuracy                           0.52       256\n",
      "   macro avg       0.59      0.56      0.49       256\n",
      "weighted avg       0.60      0.52      0.47       256\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.88      0.63       116\n",
      "           1       0.71      0.25      0.37       140\n",
      "\n",
      "    accuracy                           0.54       256\n",
      "   macro avg       0.60      0.56      0.50       256\n",
      "weighted avg       0.61      0.54      0.49       256\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.91      0.66       117\n",
      "           1       0.78      0.29      0.42       139\n",
      "\n",
      "    accuracy                           0.57       256\n",
      "   macro avg       0.65      0.60      0.54       256\n",
      "weighted avg       0.66      0.57      0.53       256\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m null_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m non_null_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, actions \u001b[38;5;129;01min\u001b[39;00m data_loaders[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     13\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(images\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/mlti-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/mlti-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/mlti-env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mlti-env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mlti-env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/mlti-env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/mlti-env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/mlti-env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d206a350-6c48-4bb2-99aa-02535c3be585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7643ab9-6f67-4137-b208-a9abacde41bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_dict(dict_arrays, use_xlabel='Epochs', use_ylabel='Value', use_title=None):\n",
    "    # Font size select custom or adjusted on `magnify` value.\n",
    "    font_size = np.interp(0.1, [0.1,1], [10.5,50])\n",
    "\n",
    "    # Font variables dictionary. Keep it in this format for future updates.\n",
    "    font_dict = dict(\n",
    "        family='DejaVu Sans',\n",
    "        color='black',\n",
    "        weight='normal',\n",
    "        size=font_size,\n",
    "        )\n",
    "\n",
    "    # Single plot figure.\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    # Use maximum length of steps. In case each arrya has different lengths.\n",
    "    max_steps = []\n",
    "\n",
    "    # Plot each array.\n",
    "    for index, (use_label, array) in enumerate(dict_arrays.items()):\n",
    "        # Set steps plotted on x-axis - we can use step if 1 unit has different value.\n",
    "        if 0 > 0:\n",
    "            # Offset all steps by start_step.\n",
    "            steps = np.array(range(0, len(array))) * 1 + 0\n",
    "            max_steps = steps if len(max_steps) < len(steps) else max_steps\n",
    "        else:\n",
    "            steps = np.array(range(1, len(array) + 1)) * 1\n",
    "            max_steps = steps if len(max_steps) < len(steps) else max_steps\n",
    "\n",
    "        # Plot array as a single line.\n",
    "        plt.plot(steps, array, linestyle=(['-'] * len(dict_arrays))[index], label=use_label)\n",
    "\n",
    "        # Plots points values.\n",
    "        if ([False] * len(dict_arrays))[index]:\n",
    "            # Loop through each point and plot the label.\n",
    "            for x, y in zip(steps, array):\n",
    "                # Add text label to plot.\n",
    "                plt.text(x, y, str(round(y, 3)), fontdict=font_dict)\n",
    "\n",
    "    # Set horizontal axis name.\n",
    "    plt.xlabel(use_xlabel, fontdict=font_dict)\n",
    "\n",
    "    # Use x ticks with steps or labels.\n",
    "    plt.xticks(max_steps, None, rotation=0)\n",
    "\n",
    "    # Set vertical axis name.\n",
    "    plt.ylabel(use_ylabel, fontdict=font_dict)\n",
    "\n",
    "    # Adjust both axis labels font size at same time.\n",
    "    plt.tick_params(labelsize=font_dict['size'])\n",
    "\n",
    "    # Place legend best position.\n",
    "    plt.legend(loc='best', fontsize=font_dict['size'])\n",
    "\n",
    "    # Adjust font for title.\n",
    "    font_dict['size'] *= 1.8\n",
    "\n",
    "    # Set title of figure.\n",
    "    plt.title(use_title, fontdict=font_dict)\n",
    "\n",
    "    # Rescale `magnify` to be used on inches.\n",
    "    magnify = 0.1\n",
    "    magnify *= 15\n",
    "\n",
    "    # Display grid depending on `use_grid`.\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Make figure nice.\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Get figure object from plot.\n",
    "    fig = plt.gcf()\n",
    "\n",
    "    # Get size of figure.\n",
    "    figsize = fig.get_size_inches()\n",
    "\n",
    "    # Change size depending on height and width variables.\n",
    "    figsize = [figsize[0] * 3 * magnify, figsize[1] * 1 * magnify]\n",
    "\n",
    "    # Set the new figure size with magnify.\n",
    "    fig.set_size_inches(figsize)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ca46c-6f6f-4e5c-b6ca-0bca34ef867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dict(all_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c890f54-12dc-4745-a2c0-2f546d560d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dict(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5732e9ae-83f1-4cad-a55f-056ddc2df4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52377c4a-7321-4ec7-9f38-8ed0de214974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a00e45-0832-4a8d-8fce-669cc839aed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlti kernel",
   "language": "python",
   "name": "mlti-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
