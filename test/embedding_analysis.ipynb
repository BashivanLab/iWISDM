{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "import os\n",
    "from PIL import Image\n",
    "from scipy.stats import pearsonr\n",
    "import json\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasgomez/opt/miniconda3/envs/bashlab_cogenv/lib/python3.8/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# EMB_DIR = '../datasets/embeddings'\n",
    "TRAIN_DIR = '../datasets/train_big'\n",
    "VAL_DIR = '../datasets/val_big'\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "resnext = torchvision.models.resnext101_32x8d(torchvision.models.ResNeXt101_32X8D_Weights.IMAGENET1K_V2).to(device)\n",
    "resnet = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path, size):\n",
    "    transform = transforms.Compose([\n",
    "                                transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                            ])\n",
    "\n",
    "    imgs = []\n",
    "    cats = []\n",
    "\n",
    "    i = 0\n",
    "    for trial_fp in os.listdir(path):\n",
    "        if 'trial' not in trial_fp:\n",
    "            continue\n",
    "\n",
    "        trial_fp = os.path.join(path, trial_fp)\n",
    "        \n",
    "        for fp in os.listdir(trial_fp):\n",
    "            fp = os.path.join(trial_fp, fp)\n",
    "            \n",
    "            if fp[-5:] == '2.png':\n",
    "                img = Image.open(fp)\n",
    "                img = transform(img)\n",
    "                imgs.append(img)\n",
    "            elif 'trial_info' in fp:\n",
    "                cat = json.load(open(fp))['objects'][-1]['category']\n",
    "                cats.append(cat)\n",
    "        i += 1\n",
    "        if i == size:\n",
    "            return imgs, cats\n",
    "    return imgs, cats\n",
    "\n",
    "imgs_train, labels_train = read_data(TRAIN_DIR, 5000)\n",
    "imgs_val, labels_val = read_data(VAL_DIR, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, imgs):\n",
    "        self.imgs = imgs\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[idx].to(device)\n",
    "    \n",
    "trainDL = DataLoader(ImageDataset(imgs_train), batch_size=128, shuffle=False)\n",
    "valDL = DataLoader(ImageDataset(imgs_val), batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnext.eval()\n",
    "resnet.eval()\n",
    "\n",
    "def img_encoder(model, images):\n",
    "    activation = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    model.avgpool.register_forward_hook(get_activation('layer'))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embs = model(images)\n",
    "        embs = torch.squeeze(activation['layer'])\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embs = []\n",
    "val_embs = []\n",
    "\n",
    "for t_images in trainDL:\n",
    "    train_embs.append(img_encoder(resnet, t_images))\n",
    "\n",
    "for v_images in valDL:\n",
    "    val_embs.append(img_encoder(resnet, v_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embs = torch.cat(tuple(train_embs)).cpu()\n",
    "val_embs = torch.cat(tuple(val_embs)).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_objects' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lucasgomez/Desktop/Neuro/Bashvian/COGEnv/COG_v3_shapenet-main/test/embedding_analysis.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lucasgomez/Desktop/Neuro/Bashvian/COGEnv/COG_v3_shapenet-main/test/embedding_analysis.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_embs \u001b[39m=\u001b[39m img_encoder(resnext, train_objects)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucasgomez/Desktop/Neuro/Bashvian/COGEnv/COG_v3_shapenet-main/test/embedding_analysis.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstruct_RDM\u001b[39m(activations):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucasgomez/Desktop/Neuro/Bashvian/COGEnv/COG_v3_shapenet-main/test/embedding_analysis.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \tnum_images \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(activations)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_objects' is not defined"
     ]
    }
   ],
   "source": [
    "train_embs = img_encoder(resnext, train_objects)\n",
    "\n",
    "def construct_RDM(activations):\n",
    "\tnum_images = len(activations)\n",
    "\tprint(num_images)\n",
    "\tRDM = np.zeros((num_images, num_images))\n",
    "\n",
    "\tfor x in range(num_images):\n",
    "\t\tfor y in range(num_images):\n",
    "\t\t\tif x<=y:\n",
    "\t\t\t\tcorrel = 1 - (pearsonr(activations[x].cpu().detach().numpy().flatten(), activations[y].cpu().detach().numpy().flatten()))[0]\n",
    "\t\t\t\tRDM[x][y] = correl\n",
    "\t\t\t\tRDM[y][x] = correl\n",
    "\treturn RDM.astype(float)\n",
    "\n",
    "rdm = construct_RDM(train_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lucasgomez/Desktop/Neuro/Bashvian/COGEnv/COG_v3_shapenet-main/test/embedding_analysis.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lucasgomez/Desktop/Neuro/Bashvian/COGEnv/COG_v3_shapenet-main/test/embedding_analysis.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rdm\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rdm' is not defined"
     ]
    }
   ],
   "source": [
    "rdm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = [cat + ' ' + str(x) for cat in cats for x in [1,2,3] ]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=rdm.shape)\n",
    "cax = ax.matshow(rdm, interpolation='nearest')\n",
    "ax.grid(False)\n",
    "plt.xticks(range(rdm.shape[0]), labels, rotation=90)\n",
    "plt.yticks(range(rdm.shape[0]), labels)\n",
    "fig.colorbar(cax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 2048])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#batchxchannelxWxH. take mean over W and H dims to get batchxchannel in the output\n",
    "train_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=50, max_iter=10000, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=50, max_iter=10000, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=50, max_iter=10000, solver='saga')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = LogisticRegression(fit_intercept=True,\n",
    "                        multi_class='auto',\n",
    "                        penalty='l2', #ridge regression\n",
    "                        solver='saga',\n",
    "                        max_iter=10000,\n",
    "                        C=50)\n",
    "\n",
    "clf.fit(train_embs, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 1.0\n",
      "Val acc: 0.948\n"
     ]
    }
   ],
   "source": [
    "print('Train acc:', clf.score(train_embs, labels_train))\n",
    "print('Val acc:', clf.score(val_embs, labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[75,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 60,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 65,  0,  0,  0,  0,  4],\n",
       "       [ 0,  2,  0, 57,  1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, 59,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 54,  0,  0],\n",
       "       [ 0,  2,  1,  0,  0,  0, 55,  0],\n",
       "       [ 2,  0,  3, 10,  0,  0,  1, 49]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = clf.predict(val_embs)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_true=labels_val, \n",
    "                         y_pred = predictions, \n",
    "                        labels = clf.classes_)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lucasgomez/Desktop/Neuro/Bashvian/COGEnv/COG_v3_shapenet-main/test/embedding_analysis.ipynb Cell 18\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucasgomez/Desktop/Neuro/Bashvian/COGEnv/COG_v3_shapenet-main/test/embedding_analysis.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# !pip -q install seaborn\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucasgomez/Desktop/Neuro/Bashvian/COGEnv/COG_v3_shapenet-main/test/embedding_analysis.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lucasgomez/Desktop/Neuro/Bashvian/COGEnv/COG_v3_shapenet-main/test/embedding_analysis.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m,\u001b[39m10\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucasgomez/Desktop/Neuro/Bashvian/COGEnv/COG_v3_shapenet-main/test/embedding_analysis.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m sns\u001b[39m.\u001b[39mheatmap(cm, annot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucasgomez/Desktop/Neuro/Bashvian/COGEnv/COG_v3_shapenet-main/test/embedding_analysis.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             linewidths\u001b[39m=\u001b[39m\u001b[39m.5\u001b[39m, square \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, cmap \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mBlues_r\u001b[39m\u001b[39m'\u001b[39m);\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucasgomez/Desktop/Neuro/Bashvian/COGEnv/COG_v3_shapenet-main/test/embedding_analysis.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mActual label\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# !pip -q install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, \n",
    "            linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(clf.score(val_X, val_y))\n",
    "plt.title(all_sample_title);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# clf = make_pipeline(StandardScaler(), LinearSVC(dual=\"auto\", random_state=0, tol=1e-5))\n",
    "# clf.fit(X, y)\n",
    "# print(clf.named_steps['linearsvc'].coef_)\n",
    "# print(clf.named_steps['linearsvc'].intercept_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bashlab_cogenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
