{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Modeling with MULTFS\n",
    "\n",
    "#### Overview:\n",
    "This notebook aims to provided an example of training a PyTorch Model on a premade task dataset\n",
    "- Text-embedder: ```all-mpnet-base-v2 (pretrained Sentence Transformer)```\n",
    "- Image-embedder: ```vit_b_16 (pretrained Vision Transformer)``` \n",
    "- Decoder/Classifier: Transformer Decoder only Model trained on both text and image embeddings to out put action class\n",
    "\n",
    "\n",
    "#### Dataset:\n",
    "- 50,000 trials over X(TODO) different tasks\n",
    "    - Train: 30,000\n",
    "    - Validation: 10,000\n",
    "    - Test: 10,000\n",
    "\n",
    "*See randTaskGen.ipynb notebook for the dataset generation*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasgomez/opt/miniconda3/envs/bashlab_cogenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-08 10:45:15.517318: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import math\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from cognitive.task_bank import CompareLocTemporal\n",
    "from cognitive import task_generator as tg\n",
    "from cognitive import constants as const\n",
    "from cognitive import stim_generator as sg\n",
    "from cognitive import info_generator as ig\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimuli Directory:  data/MULTIF_5_stim/MULTIF_5_stim\n",
      "OrderedDict([((0, 0), [(0.0, 0.5), (0.0, 0.5)]), ((0, 1), [(0.0, 0.5), (0.5, 1.0)]), ((1, 0), [(0.5, 1.0), (0.0, 0.5)]), ((1, 1), [(0.5, 1.0), (0.5, 1.0)])])\n"
     ]
    }
   ],
   "source": [
    "DIR = 'datasets/trials_reg'  # Dataset Directory\n",
    "STIM_DIR =  'data/MULTIF_5_stim/MULTIF_5_stim'   # '/Users/lucasgomez/Desktop/Neuro/Bashvian/COGEnv/COG_v3_shapenet-main/data/MULTIF_5_stim/MULTIF_5_stim' # stimulus set\n",
    "BATCH_SIZE = 1\n",
    "MAX_FRAMES = 6  # the max possible frames across tasks\n",
    "VIT_OUT_DIM = 1000 # vision transformer output dimension\n",
    "LM_OUT_DIM = 768 # language model output dimension\n",
    "TRAIN_VAL_TEST_SPLIT = (0.6,0.15,0.15)\n",
    "\n",
    "const.DATA = const.Data(dir_path=STIM_DIR)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "- Read in the pregenerated task trials organized into frames, instructions, and correct actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "infos = []\n",
    "\n",
    "# for trial_fp in os.listdir(DIR):\n",
    "#     trial_fp = os.path.join(DIR, trial_fp)\n",
    "#     imgs = []\n",
    "#     for fp in os.listdir(trial_fp):\n",
    "#         fp = os.path.join(trial_fp, fp)\n",
    "#         if fp[-4:] == '.png':\n",
    "#             imgs.append(np.rollaxis(np.array(Image.open(fp), dtype=np.float32),2,0))\n",
    "#         else:\n",
    "#             infos.append(json.load(open(fp)))\n",
    "#     if len(imgs) > MAX_FRAMES:\n",
    "#         raise Exception(trial_fp + \" contains more frames than the set maximum (MAX_FRAMES) !!!\")\n",
    "#     frames.append(np.array(imgs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instructions = [x['instruction'] for x in infos]\n",
    "# raw_target_actions = [x['answers'] for x in infos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Target Actions\n",
    "- Encodes the target actions into one hot encoding vectors corresponding to the actions ```true, false, and null```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_target_actions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m action_map \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfalse\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnull\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m2\u001b[39m}\n\u001b[1;32m      3\u001b[0m target_actions \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfor\u001b[39;00m actions \u001b[39min\u001b[39;00m raw_target_actions:\n\u001b[1;32m      6\u001b[0m     encoded \u001b[39m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m action \u001b[39min\u001b[39;00m actions:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_target_actions' is not defined"
     ]
    }
   ],
   "source": [
    "action_map = {'true': 0, 'false': 1, 'null': 2}\n",
    "\n",
    "target_actions = []\n",
    "\n",
    "for actions in raw_target_actions:\n",
    "    encoded = []\n",
    "    for action in actions:\n",
    "        encoded.append(action_map[action])\n",
    "    target_actions.append(encoded)\n",
    "\n",
    "target_actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionsDataset(Dataset):\n",
    "  \"\"\"\n",
    "    Pytorch Dataset class to load the Instructions Data\n",
    "\n",
    "    Data members:\n",
    "      instructions: list of instructions\n",
    "      n_ins: number of instructions in the dataset\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __len__: returns n_ins\n",
    "      __getitem__: returns an instruction\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, x):\n",
    "\n",
    "    self.instructions = x\n",
    "\n",
    "    self.n_ins = len(self.instructions)\n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "      Returns number of instructions in the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return self.n_ins\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "      Given an index return a instruction at that index\n",
    "    \"\"\"\n",
    "\n",
    "    return self.instructions[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionsCollator(object):\n",
    "  \"\"\"\n",
    "    Data Collator used for GPT2 in a classificaiton tasks\n",
    "\n",
    "    Args:\n",
    "      use_tokenizer :\n",
    "        Transformer type tokenizer used to process raw text into numbers.\n",
    "\n",
    "    Data members:\n",
    "      use_tokenizer: Tokenizer to be used inside the class.\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __call__: tokenize input\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "  def __init__(self, use_tokenizer):\n",
    "\n",
    "    self.use_tokenizer = use_tokenizer\n",
    "\n",
    "    return\n",
    "\n",
    "  def __call__(self, instructions):\n",
    "    \"\"\"\n",
    "        Tokenizes input\n",
    "    \"\"\"\n",
    "\n",
    "    # Call tokenizer\n",
    "    inputs = self.use_tokenizer(instructions, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'InstructionsCollator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39msentence-transformers/all-mpnet-base-v2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Create data collator to encode text and labels into numbers.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m InstructionsCollator \u001b[39m=\u001b[39m InstructionsCollator(use_tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Create pytorch dataset for instructions\u001b[39;00m\n\u001b[1;32m      9\u001b[0m ins_train_dataset \u001b[39m=\u001b[39m InstructionsDataset(instructions)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'InstructionsCollator' is not defined"
     ]
    }
   ],
   "source": [
    "# Pretrained Language Model and Tokenizer \n",
    "lm_encoder = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Create data collator to encode text and labels into numbers.\n",
    "InstructionsCollator = InstructionsCollator(use_tokenizer=tokenizer)\n",
    "\n",
    "# Create pytorch dataset for instructions\n",
    "ins_train_dataset = InstructionsDataset(instructions)\n",
    "\n",
    "# Move pytorch dataset into dataloader \n",
    "ins_train_dataloader = DataLoader(ins_train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=InstructionsCollator)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frames Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FramesDataset(Dataset):\n",
    "  \"\"\"\n",
    "    Pytorch Dataset class to load the Frame Data\n",
    "\n",
    "    Data members:\n",
    "      frames``ist of frames\n",
    "      n_imgs: number of iamges in the dataset\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __len__: returns n_imgs\n",
    "      __getitem__: returns an frame\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, x):\n",
    "\n",
    "    self.frames = x\n",
    "\n",
    "    self.n_imgs = len(self.frames)\n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "      Returns number of frames in the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return self.n_imgs\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "      Given an index return a frame\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.tensor(self.frames[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained Vision Transformer\n",
    "vit_encoder = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.DEFAULT)\n",
    "\n",
    "# Create pytorch dataset for instructions\n",
    "frames_train_dataset = FramesDataset(frames)\n",
    "\n",
    "# Move pytorch dataset into dataloader \n",
    "frames_train_dataloader = DataLoader(frames_train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_embedder(instruction, encoder):\n",
    "    #Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        lm_output = encoder(**instruction)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(lm_output, instruction['attention_mask'])\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        \n",
    "    return sentence_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Embedder\n",
    "- We pad frames based on max possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_embedder(frames, encoder):\n",
    "    with torch.no_grad():\n",
    "        vit_out = encoder(torch.tensor(frames))\n",
    "\n",
    "    npads = MAX_FRAMES-len(vit_out)\n",
    "    pad = torch.ones((npads, vit_out.shape[1]))\n",
    "    vit_out = torch.cat((vit_out, pad))\n",
    "\n",
    "    return vit_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ins_train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m lm_embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m img_embeddings \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m i,f \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(ins_train_dataloader,frames_train_dataloader):\n\u001b[1;32m      5\u001b[0m    f \u001b[39m=\u001b[39m f[\u001b[39m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m    lm_embeddings\u001b[39m.\u001b[39mappend(lm_embedder(i, lm_encoder))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ins_train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "lm_embeddings = []\n",
    "img_embeddings = []\n",
    "\n",
    "for i,f in zip(ins_train_dataloader,frames_train_dataloader):\n",
    "   f = f[0]\n",
    "   lm_embeddings.append(lm_embedder(i, lm_encoder))\n",
    "   img_embeddings.append(img_embedder(f, vit_encoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsDataset(Dataset):\n",
    "  \"\"\"\n",
    "    Pytorch Dataset class to load the embedded data\n",
    "\n",
    "    Data members:\n",
    "      lm_embeddings: list of language model embeddings\n",
    "      img_embeddings: list of language model embeddings\n",
    "      n_embs: number of embeddings in the dataset\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __len__: returns n_ins\n",
    "      __getitem__: returns an instruction\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, lm_embeddings, img_embeddings, actions):\n",
    "\n",
    "    self.lm_embeddings = lm_embeddings\n",
    "    self.img_embeddings = img_embeddings\n",
    "    self.actions = actions\n",
    "\n",
    "    self.n_embs = len(self.lm_embeddings)\n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "      Returns number of instructions in the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return self.n_embs\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "      Given an index return a instruction at that index\n",
    "    \"\"\"\n",
    "\n",
    "    # Map One Hot to\n",
    "    # actions = [np.where(action == 1)[0][0] for action in self.actions[idx]]\n",
    "    \n",
    "\n",
    "    return {'instruction':self.lm_embeddings[idx], 'frames':self.img_embeddings[idx], 'actions':torch.tensor(self.actions[idx], dtype=torch.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pytorch dataset for train, val, test data\n",
    "\n",
    "train_split_lm, test_split_lm, train_split_img, test_split_img, train_split_ta, test_split_ta = train_test_split(lm_embeddings, img_embeddings, target_actions,test_size=0.2, random_state=1)\n",
    "\n",
    "train_split_lm, val_split_lm, train_split_img, val_split_img, train_split_ta, val_split_ta  = train_test_split(train_split_lm, train_split_img, train_split_ta, test_size=0.25, random_state=1) \n",
    "\n",
    "\n",
    "train_dataset = EmbeddingsDataset(train_split_lm, train_split_img, train_split_ta)\n",
    "val_dataset = EmbeddingsDataset(val_split_lm, val_split_img, val_split_ta)\n",
    "test_dataset  = EmbeddingsDataset(test_split_lm, test_split_img, test_split_ta)\n",
    "\n",
    "# Move pytorch datasets into dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import projections\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CausalMatchTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch based transformer decoder model\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize Model with Params\n",
    "    def __init__(self, nframes=MAX_FRAMES, blocks=3, nhead=5, emb_dim=VIT_OUT_DIM, classes=3, device=device):\n",
    "        super().__init__()\n",
    "\n",
    "        # Device\n",
    "        self.device = device\n",
    "\n",
    "        # Embedding Dimension\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        # Number of frames\n",
    "        self.nframes = nframes\n",
    "\n",
    "        # Frame Position Embedder Layer\n",
    "        self.pos_emb = nn.Parameter(torch.Tensor(nframes,emb_dim)).to(device)\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "           self.pos_emb,\n",
    "           gain=torch.nn.init.calculate_gain(\"linear\"))\n",
    "\n",
    "        # Instruction Dim Projection Layer\n",
    "        self.lm_linear_layer = nn.Linear(LM_OUT_DIM, emb_dim).to(device)\n",
    "\n",
    "        # Decoder Layers\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=nhead, batch_first=True).to(device)\n",
    "        self.decoder_layers = _get_clones(self.decoder_layer, blocks)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layers, num_layers=blocks).to(device)\n",
    "\n",
    "        # Action classifier\n",
    "        self.classifier = nn.Linear(emb_dim, classes).to(device)\n",
    "\n",
    "    # Function for forward pass\n",
    "    def forward(self, instruction, frames, mask, padding_mask):\n",
    "\n",
    "        # Project instruction embedding\n",
    "        instruction = self.lm_linear_layer(instruction)\n",
    "\n",
    "        # Add the frame position embedding\n",
    "        for i in range(len(frames)):\n",
    "            frames[i] += self.pos_emb[i,:]\n",
    "\n",
    "        # Apply each Decoder Layer (block)\n",
    "        for layer in self.decoder_layers:\n",
    "            frames = layer(frames, instruction, tgt_mask=mask, tgt_key_padding_mask=padding_mask, ) \n",
    "\n",
    "        # Pass through linear layer for classification\n",
    "        output = self.classifier(frames)\n",
    "\n",
    "        return output\n",
    " \n",
    "# Creates a list of torch duplicate torch modules\n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "# Creates a square Sequential/Causal mask of size sz\n",
    "def generate_causal_mask(sz: int) -> Tensor:\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "# Generates a padding masks for each sequence in a batch\n",
    "def generate_pad_mask(batch):\n",
    "\n",
    "    pad_tensor = torch.ones((batch.shape[2])).to(device)\n",
    "\n",
    "    mask = np.zeros((batch.shape[0],batch.shape[1]))\n",
    "\n",
    "    for s in range(0, batch.shape[0]):\n",
    "        for v in range(0, batch[s].shape[0]):\n",
    "            new_s = torch.all(batch[s][v] == pad_tensor)\n",
    "            mask[s][v] = new_s\n",
    "\n",
    "    return torch.tensor(mask).bool().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m CausalMatchTransformer(nframes\u001b[39m=\u001b[39mMAX_FRAMES,\n\u001b[1;32m      2\u001b[0m                                blocks\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m      3\u001b[0m                                nhead\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[1;32m      4\u001b[0m                                emb_dim\u001b[39m=\u001b[39mVIT_OUT_DIM,\n\u001b[0;32m----> 5\u001b[0m                                classes\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(action_map\u001b[39m.\u001b[39mkeys()),\n\u001b[1;32m      6\u001b[0m                                device\u001b[39m=\u001b[39mdevice)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'action_map' is not defined"
     ]
    }
   ],
   "source": [
    "model = CausalMatchTransformer(nframes=MAX_FRAMES,\n",
    "                               blocks=3,\n",
    "                               nhead=5,\n",
    "                               emb_dim=VIT_OUT_DIM,\n",
    "                               classes=len(action_map.keys()),\n",
    "                               device=device).float().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configurations\n",
    "epochs = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(\n",
    "    (p for p in model.parameters() if p.requires_grad), lr=lr\n",
    ")\n",
    "\n",
    "mask = generate_causal_mask(MAX_FRAMES).to(device)\n",
    "\n",
    "# torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "epoch=0\n",
      "epoch=1\n",
      "epoch=2\n",
      "epoch=3\n",
      "epoch=4\n",
      "epoch=5\n",
      "epoch=6\n",
      "epoch=7\n",
      "epoch=8\n",
      "epoch=9\n"
     ]
    }
   ],
   "source": [
    "# Training and validation loop\n",
    "\n",
    "# Store the average loss after each epoch\n",
    "all_loss = {'train_loss':[], 'val_loss':[]}\n",
    "all_acc = {'train_acc':[], 'val_acc':[]}\n",
    "\n",
    "print(\"starting\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch={epoch}\")\n",
    "\n",
    "    # Epoch stat trackers\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_count = 0\n",
    "    for idx, batch in enumerate(iter(train_dataloader)):\n",
    "\n",
    "        # Inputs and Targets\n",
    "        instruction = batch['instruction']\n",
    "        frames = batch['frames']\n",
    "        targets = batch['actions']\n",
    "\n",
    "        # Frame Padding\n",
    "        padding_mask = generate_pad_mask(batch=frames)\n",
    "        pad_indexes = np.argwhere(np.array(padding_mask) == False)[:,1]\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = model(instruction, frames, mask, padding_mask)\n",
    "        predictions = predictions[:,pad_indexes]\n",
    "\n",
    "        # Get Loss by permuting the predictions into correct shape (batch_size, n_classes, seq_len)\n",
    "        loss = criterion(predictions.permute(0, 2, 1), targets.long())\n",
    "\n",
    "        # Track stats\n",
    "        # torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)\n",
    "        correct = predictions.argmax(dim=-1) == targets\n",
    "        acc = correct.sum().item() / correct.size(0)\n",
    "        epoch_correct += correct.sum().item()\n",
    "        epoch_count += correct.size(0)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate on validation set every 5 epochs\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        # Turn off gradient calcs\n",
    "        with torch.no_grad():\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_correct = 0\n",
    "            val_epoch_count = 0\n",
    "\n",
    "            for idx, batch in enumerate(iter(val_dataloader)):\n",
    "                # Inputs and Targets\n",
    "                instruction = batch['instruction']\n",
    "                frames = batch['frames']\n",
    "                targets = batch['actions']\n",
    "                \n",
    "                # Frame Padding\n",
    "                padding_mask = generate_pad_mask(batch=frames)\n",
    "                pad_indexes = np.argwhere(np.array(padding_mask) == False)[:,1]\n",
    "\n",
    "                # Get predictions\n",
    "                predictions = model(instruction, frames, mask, padding_mask)\n",
    "                predictions = predictions[:,pad_indexes]\n",
    "\n",
    "                # Get Loss\n",
    "                loss = criterion(predictions.permute(0, 2, 1), targets.long())\n",
    "\n",
    "                correct = predictions.argmax(dim=-1) == targets\n",
    "                acc = correct.sum().item() / correct.size(0)\n",
    "\n",
    "                val_epoch_correct += correct.sum().item()\n",
    "                val_epoch_count += correct.size(0)\n",
    "                val_epoch_loss += loss.item()\n",
    "\n",
    "        # Track loss and acc ever 5 epochs\n",
    "        avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "        avg_val_loss = val_epoch_loss / len(val_dataloader)\n",
    "\n",
    "    \n",
    "        all_loss['val_loss'].append(avg_val_loss)\n",
    "        all_acc['val_acc'].append(val_epoch_correct / val_epoch_count)\n",
    "\n",
    "    all_acc['train_acc'].append(epoch_correct / epoch_count)\n",
    "    all_loss['train_loss'].append(avg_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_acc': [4.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  3.9983333333333335,\n",
       "  4.0,\n",
       "  4.0],\n",
       " 'val_acc': [4.0, 4.0, 4.0, 4.0, 4.0]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.8\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_correct = 0\n",
    "    test_count = 0\n",
    "\n",
    "    for idx, batch in enumerate(iter(test_dataloader)):\n",
    "        # Inputs and Targets\n",
    "        instruction = batch['instruction']\n",
    "        frames = batch['frames']\n",
    "        targets = batch['actions']\n",
    "\n",
    "        # Frame Padding\n",
    "        padding_mask = generate_pad_mask(batch=frames)\n",
    "        pad_indexes = np.argwhere(np.array(padding_mask) == False)[:,1]\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = model(instruction, frames, mask, padding_mask)\n",
    "        predictions = predictions[:,pad_indexes]\n",
    "\n",
    "        correct = predictions.argmax(dim=-1) == targets\n",
    "\n",
    "        test_correct += correct.sum().item()\n",
    "        test_count += correct.size(1)\n",
    "\n",
    "print(\"Test Accuracy: \", round(test_correct/test_count,4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_encoder = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.DEFAULT)\n",
    "\n",
    "lm_encoder = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "def gen_comp_loc(lm_embedder, img_embedder):\n",
    "    f1 = random.randint(0, 4)\n",
    "    f2 = random.randint(f1+1,5)\n",
    "\n",
    "    task = CompareLocTemporal(whens=['last'+str(f1),'last'+str(f2)])\n",
    "\n",
    "    frame_info = ig.FrameInfo(task, task.generate_objset())\n",
    "    compo_info = ig.TaskInfoCompo(task, frame_info)\n",
    "    objset = compo_info.frame_info.objset\n",
    "\n",
    "    frames = []\n",
    "    for i, (epoch, frame) in enumerate(zip(sg.render(objset, 224), compo_info.frame_info)):\n",
    "        if not any('ending' in description for description in frame.description):\n",
    "            sg.add_fixation_cue(epoch)\n",
    "        img = np.rollaxis(np.array(Image.fromarray(epoch, 'RGB'), dtype=np.float32),2,0)\n",
    "        frames.append(img)\n",
    "\n",
    "    frames = torch.tensor(frames)\n",
    "\n",
    "    print(frames.shape)\n",
    "\n",
    "    frames = img_embedder(frames, vit_encoder)\n",
    "\n",
    "    _, compo_example, _ = compo_info.get_examples()\n",
    "\n",
    "    instruction = compo_example['instruction']\n",
    "\n",
    "    instruction = tokenizer(instruction, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    instruction = lm_embedder(instruction, lm_encoder)\n",
    "\n",
    "    actions = compo_example['answers']\n",
    "\n",
    "    action_map = {'true': 0, 'false': 1, 'null': 2}\n",
    "\n",
    "    target_actions = []\n",
    "    for action in actions:\n",
    "        target_actions.append(action_map[action])\n",
    "\n",
    "    return instruction, frames, torch.tensor(target_actions, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/778mhwmd7kj96mffltwb2qjw0000gn/T/ipykernel_92204/3025187430.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vit_out = encoder(torch.tensor(frames))\n"
     ]
    }
   ],
   "source": [
    "_,_,_ = gen_comp_loc(lm_embedder, img_embedder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bashlab_cogenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
