{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'datasets\\\\trials'\n",
    "BATCH_SIZE = 1\n",
    "MAX_FRAMES = 2\n",
    "VIT_OUT_DIM = 1000\n",
    "LM_OUT_DIM = 768\n",
    "ACT_TOKEN = '[ACT]'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3]).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "infos = []\n",
    "\n",
    "for trial_fp in os.listdir(DIR):\n",
    "    trial_fp = os.path.join(DIR, trial_fp)\n",
    "    imgs = []\n",
    "    for fp in os.listdir(trial_fp):\n",
    "        fp = os.path.join(trial_fp, fp)\n",
    "        if fp[-4:] == '.png':\n",
    "            imgs.append(np.rollaxis(np.array(Image.open(fp), dtype=np.float32),2,0))\n",
    "        else:\n",
    "            infos.append(json.load(open(fp)))\n",
    "    frames.append(np.array(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = [x['instruction'] for x in infos]\n",
    "target_actions = [x['answers'] for x in infos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionsDataset(Dataset):\n",
    "  \"\"\"\n",
    "    Pytorch Dataset class to load the Instructions Data\n",
    "\n",
    "    Data members:\n",
    "      instructions: list of instructions\n",
    "      n_ins: number of instructions in the dataset\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __len__: returns n_ins\n",
    "      __getitem__: returns an instruction\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, x):\n",
    "\n",
    "    self.instructions = x\n",
    "\n",
    "    self.n_ins = len(self.instructions)\n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "      Returns number of instructions in the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return self.n_ins\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "      Given an index return a instruction at that index\n",
    "    \"\"\"\n",
    "\n",
    "    return self.instructions[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionsCollator(object):\n",
    "  \"\"\"\n",
    "    Data Collator used for GPT2 in a classificaiton tasks\n",
    "\n",
    "    Args:\n",
    "      use_tokenizer :\n",
    "        Transformer type tokenizer used to process raw text into numbers.\n",
    "\n",
    "    Data members:\n",
    "      use_tokenizer: Tokenizer to be used inside the class.\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __call__: tokenize input\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "  def __init__(self, use_tokenizer):\n",
    "\n",
    "    self.use_tokenizer = use_tokenizer\n",
    "\n",
    "    return\n",
    "\n",
    "  def __call__(self, instructions):\n",
    "    \"\"\"\n",
    "        Tokenizes input\n",
    "    \"\"\"\n",
    "\n",
    "    # Call tokenizer\n",
    "    inputs = self.use_tokenizer(instructions, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_encoder = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Create data collator to encode text and labels into numbers.\n",
    "InstructionsCollator = InstructionsCollator(use_tokenizer=tokenizer)\n",
    "\n",
    "# Create pytorch dataset for instructions\n",
    "ins_train_dataset = InstructionsDataset(instructions)\n",
    "\n",
    "# Move pytorch dataset into dataloader \n",
    "ins_train_dataloader = DataLoader(ins_train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=InstructionsCollator)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frames Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FramesDataset(Dataset):\n",
    "  \"\"\"\n",
    "    Pytorch Dataset class to load the Frame Data\n",
    "\n",
    "    Data members:\n",
    "      frames``ist of frames\n",
    "      n_imgs: number of iamges in the dataset\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __len__: returns n_imgs\n",
    "      __getitem__: returns an frame\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, x):\n",
    "\n",
    "    self.frames = x\n",
    "\n",
    "    self.n_imgs = len(self.frames)\n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "      Returns number of frames in the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return self.n_imgs\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "      Given an index return a frame\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.tensor(self.frames[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_encoder = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.DEFAULT)\n",
    "\n",
    "# Create pytorch dataset for instructions\n",
    "frames_train_dataset = FramesDataset(frames)\n",
    "\n",
    "# Move pytorch dataset into dataloader \n",
    "frames_train_dataloader = DataLoader(frames_train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_embedder(instruction, encoder):\n",
    "    #Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        lm_output = encoder(**instruction)\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(lm_output, instruction['attention_mask'])\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        \n",
    "    return sentence_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_embedder(frames, encoder):\n",
    "    with torch.no_grad():\n",
    "        vit_out = encoder(torch.tensor(frames))\n",
    "    return vit_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_31244\\2308170862.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vit_out = encoder(torch.tensor(frames))\n"
     ]
    }
   ],
   "source": [
    "lm_embeddings = []\n",
    "img_embeddings = []\n",
    "\n",
    "for i,f in zip(ins_train_dataloader,frames_train_dataloader):\n",
    "   f = f[0]\n",
    "   lm_embeddings.append(lm_embedder(i, lm_encoder))\n",
    "   img_embeddings.append(img_embedder(f, vit_encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_embeddings[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import projections\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CausalMatchTransformer(nn.Module):\n",
    "\n",
    "    # Initialize Model with Params\n",
    "    def __init__(self, nframes=MAX_FRAMES, blocks=3, nhead=5, emb_dim=VIT_OUT_DIM, device=device):\n",
    "        super().__init__()\n",
    "\n",
    "        # Device\n",
    "        self.device = device\n",
    "\n",
    "        # Number of frames\n",
    "        self.nframes = nframes\n",
    "\n",
    "        # Frame Position Embedder Layer\n",
    "        self.pos_emb = nn.Embedding(nframes,emb_dim)\n",
    "        self.pos_emb.weight = nn.init.xavier_uniform_(self.pos_emb.weight)\n",
    "\n",
    "        # Instruction Dim Projection Layer\n",
    "        self.lm_linear_layer = nn.Linear(LM_OUT_DIM, emb_dim).to(device)\n",
    "\n",
    "        # Decoder Layers\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=nhead, batch_first=True).to(device)\n",
    "        self.decoder_layers = _get_clones(self.decoder_layer, blocks)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layers, num_layers=blocks).to(device)\n",
    "\n",
    "    # Function for forward pass\n",
    "    def forward(self, instruction, frames, mask):\n",
    "\n",
    "        output = self.lm_linear_layer(instruction)\n",
    "\n",
    "        for i in range(len(frames)):\n",
    "            frames[i] += self.pos_emb(torch.tensor([i]))[0]\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            output = layer(output, frames, memory_mask=mask, memory_is_causal=True) \n",
    "            # (tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None, tgt_is_causal=False, memory_is_causal=False)\n",
    "\n",
    "        return output\n",
    "\n",
    "    # Generates a padding masks for each sequence in a batch\n",
    "    def generate_pad_mask(self, batch):\n",
    "\n",
    "        pad_tensor = torch.ones((batch.shape[2])).to(device)\n",
    "\n",
    "        mask = np.zeros((batch.shape[0],batch.shape[1]))\n",
    "\n",
    "        for s in range(0, batch.shape[0]):\n",
    "            for v in range(0, batch[s].shape[0]):\n",
    "                new_s = torch.all(batch[s][v] == pad_tensor)\n",
    "                mask[s][v] = new_s\n",
    "\n",
    "        return torch.tensor(mask).bool().to(self.device)\n",
    "    \n",
    "# Creates a list of torch duplicate torch modules\n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "def generate_causal_mask(sz: int) -> Tensor:\n",
    "    \n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "def generate_pad_mask(batch):\n",
    "\n",
    "    pad_tensor = torch.ones((batch.shape[2])).to(device)\n",
    "    \n",
    "    mask = np.zeros((batch.shape[0],batch.shape[1]))\n",
    "\n",
    "    for s in range(0, batch.shape[0]):\n",
    "        for v in range(0, batch[s].shape[0]):\n",
    "            new_s = torch.all(batch[s][v] == pad_tensor)\n",
    "            mask[s][v] = new_s\n",
    "\n",
    "    return torch.tensor(mask).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = generate_causal_mask(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x214ed820d70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = CausalMatchTransformer(nframes=MAX_FRAMES,\n",
    "                               blocks=3,\n",
    "                               nhead=5,\n",
    "                               emb_dim=VIT_OUT_DIM,\n",
    "                               device=device).float().to(device)\n",
    "\n",
    "# Configurations\n",
    "epochs = 50\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(\n",
    "    (p for p in model.parameters() if p.requires_grad), lr=lr\n",
    ")\n",
    "\n",
    "# mask = generate_causal_mask(nframes).to(device)\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6380, -0.2778, -0.4223,  ..., -0.9591, -0.6478,  0.9478],\n",
      "        [-0.8547, -0.1672, -0.2223,  ..., -1.2684, -0.4001,  0.6226]])\n",
      "tensor([[-0.6701, -0.2735, -0.4988,  ..., -1.0212, -0.6825,  0.9605],\n",
      "        [-0.8064, -0.0988, -0.2203,  ..., -1.2056, -0.3230,  0.6901]],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8207,  0.1376, -0.8893, -0.0928,  1.1168, -2.0958, -0.4657, -0.5178,\n",
       "          0.1308, -1.0753, -0.1561, -1.0498,  0.8365, -1.2998,  1.9882,  1.1052,\n",
       "         -1.2403, -1.1389,  1.3018,  0.5328,  0.3162,  0.6033,  1.4248,  0.9096,\n",
       "          2.1418,  0.6704, -0.0414, -0.5233, -0.8893, -0.4926, -0.3808,  0.3360,\n",
       "          0.1819, -2.3062, -0.0175, -1.2074,  0.3069,  0.7717, -1.2177,  0.6617,\n",
       "          0.3042, -1.6231, -0.5853,  0.2076, -0.2835,  0.5075, -1.1996,  0.4792,\n",
       "         -1.3614,  0.3271, -0.2035, -1.1899, -0.7082,  0.7151,  0.2385, -1.1350,\n",
       "         -1.7064,  0.4775,  0.5305,  0.6212,  0.4186,  0.1375, -1.2108, -0.2361,\n",
       "          0.4878, -0.1315, -1.1718,  0.6085, -1.1749,  0.1685, -0.2346,  1.1782,\n",
       "          0.0969,  0.3035,  1.0352, -1.0169,  0.6193,  1.3302,  0.3539,  0.8594,\n",
       "         -0.3301, -0.9113, -1.0015, -0.5427,  0.6428, -0.3642,  1.0155, -0.3540,\n",
       "         -1.3382, -1.3834, -1.2648, -0.5145, -1.5135, -1.1487, -1.7215,  0.7069,\n",
       "          0.1486,  0.9219,  0.4566,  1.3680, -1.0976,  0.7164, -0.6008, -0.0703,\n",
       "         -0.7112, -0.2439, -0.7954,  0.0992, -1.6245,  0.4790,  1.0325, -1.0043,\n",
       "          1.0426, -0.0353, -1.3245, -1.1551, -0.1430,  0.3116, -1.3001,  0.3371,\n",
       "          0.6026,  2.3046, -0.2596, -1.6007,  0.3674, -1.0751,  0.2332, -0.2319,\n",
       "          0.1272,  0.3870, -0.5786, -1.3214,  1.7737, -0.8075, -0.2824,  0.8391,\n",
       "          1.1704,  1.1657, -1.7113, -0.2739,  0.3586, -0.9479,  0.5199, -0.3647,\n",
       "          0.6695, -1.5318, -0.4028,  0.2864,  0.4270, -0.7013,  1.2546,  0.1194,\n",
       "          0.1425,  0.7754,  0.2381, -1.3168,  0.6804,  1.1409,  1.4977,  0.2147,\n",
       "         -0.7786, -0.5032,  0.9317,  0.9988,  0.6115,  1.4123, -0.1163,  0.0797,\n",
       "          1.0075, -0.5897,  0.5177,  0.5410,  0.5096, -1.3902, -0.2429, -0.8122,\n",
       "          0.1648,  1.8924,  1.1138,  0.7812, -1.2743,  0.2929,  0.7429,  1.3067,\n",
       "          1.8186, -0.2839, -0.0689,  1.4167, -1.6356, -0.3875,  1.2772, -0.0090,\n",
       "         -0.3477,  0.1021,  0.5470,  0.4022,  0.0330,  0.6351,  0.1406, -0.5107,\n",
       "         -0.1299,  0.8138, -2.4196,  0.7346,  0.8631,  0.8716, -0.1373,  2.5563,\n",
       "          0.2067,  1.3251, -0.2582,  0.8986, -0.3321, -1.1734, -1.0735, -2.3493,\n",
       "          0.1583, -0.1266, -0.7239, -0.7074, -0.9765, -0.1956,  0.3351, -0.4848,\n",
       "          2.2625, -0.8714,  0.7155,  1.1693,  1.1207, -0.2915,  2.6887, -0.6101,\n",
       "         -0.9704, -0.5964,  0.1912,  1.9737,  2.3032,  0.1916,  2.3062,  0.4339,\n",
       "         -1.7176, -0.3909,  0.3202,  0.1888,  1.5783,  1.2550,  0.7346,  0.5107,\n",
       "          1.7267,  1.7616,  0.0626,  0.5587,  0.7453,  1.1772,  0.0774, -0.2698,\n",
       "         -0.1600, -0.5346,  1.6745, -1.1211, -0.6753, -2.0618, -0.3570, -0.2491,\n",
       "         -0.6426,  0.0082,  0.6300, -2.4379,  1.0103, -0.3177, -0.7625, -1.3875,\n",
       "          0.5384, -0.5277,  0.8668, -0.3766,  0.2619,  0.2895, -0.4028, -1.0818,\n",
       "          0.6236,  0.6821,  0.9693, -1.9858, -1.8978,  0.9939, -0.1103, -0.1189,\n",
       "         -0.0999,  0.9985,  0.7236,  1.4023, -0.0925,  0.6762, -1.4504,  0.3972,\n",
       "         -0.1290, -0.2551,  0.6861,  1.9291, -1.2968, -0.9850, -0.0368,  1.3533,\n",
       "         -0.3290,  1.7026, -0.3605, -0.3321,  2.5007, -0.6211,  0.6673,  1.0005,\n",
       "         -1.5644,  0.6065,  0.4687,  0.3871,  0.7629, -0.7697, -1.1393, -0.6090,\n",
       "         -1.0895,  0.2232,  1.3107, -0.8252, -0.8482,  0.1197, -0.4187,  0.0315,\n",
       "         -1.5202, -1.2726, -0.5441,  0.2150, -0.6451,  0.5780, -0.1904,  1.3921,\n",
       "          1.2898, -1.3089, -0.7619, -0.6713, -0.7171, -1.6152,  1.6528, -0.0749,\n",
       "         -2.4202, -1.2142,  1.0136, -0.0095, -0.0553, -0.2794,  0.0625,  0.4875,\n",
       "          0.6251, -1.0580,  1.7655, -0.8414,  1.4429, -0.4050,  1.4328,  1.1504,\n",
       "          0.9165, -1.6325,  0.1815, -0.0238,  0.0831, -0.0440, -1.7372,  0.5081,\n",
       "          0.4866, -0.1019, -0.1632,  1.5866, -0.9593, -0.7607, -0.1613, -1.5947,\n",
       "         -1.0074, -1.2475, -0.9816, -0.3373, -1.5266, -0.9490,  0.8075, -0.2044,\n",
       "          0.6266,  0.1335, -0.6926, -0.0352, -0.5425, -1.3307,  0.2108,  0.0331,\n",
       "          0.4090, -0.7484, -0.4752, -0.4569, -0.7666,  0.1010,  1.3895,  0.0156,\n",
       "          1.4153, -0.7714,  0.0283, -1.2015,  0.9477,  1.4100,  0.4318,  0.4682,\n",
       "         -0.5735,  1.0481,  0.7675,  0.2331, -0.6283,  0.6340,  2.0071,  1.6925,\n",
       "          1.5573, -0.3496, -0.1595,  0.5266,  0.6624,  0.8937,  0.7513, -1.6445,\n",
       "          0.2629,  0.7574, -0.5152, -0.4181,  0.2701,  0.9173,  0.7553,  0.4680,\n",
       "          0.1554, -0.5839, -0.5896,  0.7973,  1.4924, -1.6933, -1.7650,  1.4499,\n",
       "          1.1135, -0.6473,  0.0110, -0.1929, -0.7650,  1.1017,  1.3393,  0.2821,\n",
       "         -0.3832,  1.5866, -0.3117,  2.0105, -0.4487,  1.5143,  1.3723, -1.9040,\n",
       "          0.6400, -0.7795, -1.1550, -1.5673,  0.2365, -0.9331, -1.2890, -1.3483,\n",
       "         -0.4837, -0.7089,  2.1677, -0.5618,  0.3262, -0.6085,  0.3416, -1.6852,\n",
       "         -0.0676,  0.8230,  0.0966, -0.7992,  3.1124, -0.0643, -0.3079, -0.0070,\n",
       "          0.4017,  0.1899,  0.8168,  1.3840, -1.4570, -0.4870, -0.9292, -1.0070,\n",
       "         -0.0945, -0.5726, -0.9188, -0.1637,  0.3663,  0.6712, -0.7272,  0.7827,\n",
       "          1.5429, -0.5885, -0.8271,  0.6611, -0.9378,  1.0041, -3.1247, -2.4287,\n",
       "          0.5495,  1.1673,  1.9703,  0.2261,  0.2416, -1.0187, -1.1914, -0.0579,\n",
       "          0.3548,  0.3315,  0.3400, -0.4708,  1.8738,  0.4638,  0.5266,  0.2335,\n",
       "         -0.1883, -0.3011, -0.0631, -0.1092,  0.6868, -0.5976,  1.3516, -0.5258,\n",
       "          0.0611, -1.6042, -0.0632, -1.2970, -1.2925, -1.3736, -0.5196, -1.2788,\n",
       "         -0.2193, -0.0574, -0.3334, -1.3165, -1.3437,  0.9449, -0.6172, -2.2360,\n",
       "         -0.4717, -1.1834,  0.7514,  1.6958, -1.3844,  0.6089,  0.4204, -0.1905,\n",
       "          1.5905,  0.3588,  0.6200, -0.6021,  0.6752,  1.1703,  1.4399, -0.7546,\n",
       "         -0.8984, -1.4414,  0.0698,  0.3467,  0.4337, -1.0106, -1.0297,  0.6923,\n",
       "          0.1139,  0.4911, -0.4049, -0.5849,  0.6607, -0.0871, -0.2185,  0.2288,\n",
       "          0.8948,  0.3096, -0.3691,  1.8156, -0.8699,  0.3212,  0.2156,  1.2678,\n",
       "         -1.1421,  0.7514, -1.5620, -1.0244,  1.3665,  0.9254,  0.5865,  0.6621,\n",
       "          1.3738,  0.5502,  0.6894,  1.1093, -1.2186, -1.0345,  0.3407,  0.7994,\n",
       "          0.6608, -0.2979,  0.4055,  1.2060,  0.8949,  0.1041, -1.7270, -0.3888,\n",
       "          0.1395, -0.9560,  0.0774, -0.9670,  1.4394,  1.6113, -0.4053, -0.0809,\n",
       "         -1.0485,  0.9877, -0.2503, -0.0732, -0.9724,  0.2713, -0.5425,  1.5266,\n",
       "          0.1751,  0.8286,  0.5334, -2.7560,  0.0565,  0.3955,  0.6210,  0.4807,\n",
       "         -1.7203, -0.4019, -1.7704, -1.2940,  0.3959, -1.8125,  1.8152, -1.1634,\n",
       "          0.7955, -0.1943,  0.3870, -0.1667, -0.4241,  0.4294,  0.3225, -2.6196,\n",
       "         -1.0526, -1.6603,  0.5155, -2.3085, -1.6480,  0.1259, -0.8044,  0.6367,\n",
       "          0.4679, -0.6732, -1.0087,  0.2225, -1.1308,  0.3745, -1.1266,  0.1710,\n",
       "          0.8102,  1.5932, -0.2414,  0.1916, -0.6438,  1.4247,  0.8662, -0.1446,\n",
       "          1.1026, -1.1683, -1.8860, -1.0089,  0.3487,  1.4266,  1.5694,  1.5287,\n",
       "          0.3930,  0.0900, -0.6818,  0.9757, -0.7371, -0.4710, -0.3925, -0.5543,\n",
       "         -1.2115,  0.3188,  0.4695,  0.9051, -1.6580,  1.7222, -0.5182, -0.6431,\n",
       "         -0.8145,  0.0512, -0.5656,  0.3819, -0.0886,  0.7874,  0.6106, -0.6421,\n",
       "         -0.0999,  0.5182, -1.4095,  1.3475, -0.9628, -0.9894,  0.4756, -0.2518,\n",
       "         -0.2823, -0.3445, -1.4744,  0.0504,  0.7553,  0.7101,  0.1270, -0.5716,\n",
       "         -0.0447,  0.7921,  0.8188, -1.3818,  0.2723,  0.0391,  0.9309,  0.0642,\n",
       "          0.6592, -0.5003, -0.5787, -0.1673,  0.2575, -0.1448,  0.6561, -0.1560,\n",
       "          1.6350,  0.3382, -0.0388,  0.9929, -0.0534, -1.7076, -0.2596,  0.0459,\n",
       "          0.1100,  0.3945,  2.2894,  0.2402, -1.6927, -0.0486,  0.8447, -1.1365,\n",
       "          0.2642, -0.6386, -1.6418,  0.7010, -0.3970,  0.7699,  0.6527,  1.2858,\n",
       "          0.0790,  0.4640, -0.3980, -1.0483,  0.1411,  0.0483, -0.7467, -0.2599,\n",
       "          0.8246, -0.4175,  0.6763,  0.9390,  1.1375,  1.5454, -0.5928, -1.9273,\n",
       "         -0.4164, -0.6615,  0.6024,  0.1247,  1.0911,  0.4824, -0.8885,  0.6756,\n",
       "          1.1729, -0.2418, -0.8146,  0.3313,  0.9297, -1.1189, -1.6389, -0.6320,\n",
       "          1.5358,  0.2020, -0.5445,  0.0272,  0.4343, -2.2236, -0.8470,  0.5186,\n",
       "         -0.9014, -1.3212,  0.3989,  0.4239, -1.6938, -1.6930,  1.5923,  0.1549,\n",
       "         -1.0030,  0.8337,  0.0064,  0.0232, -1.3241, -1.8993,  0.0238, -1.5740,\n",
       "          0.4924,  2.3215, -0.9840,  0.7088,  1.3404, -0.0991,  1.2469,  0.3361,\n",
       "         -0.2152,  0.6527,  1.4488,  1.7446,  1.4657, -0.7977, -0.1798,  2.5087,\n",
       "          0.9897, -0.0568, -0.4350,  1.2609,  0.0955,  0.5192, -1.3562, -0.8050,\n",
       "          0.1421,  1.5431, -0.7698, -0.5847,  0.0257, -2.5123,  1.5169, -0.5972,\n",
       "         -1.0783, -0.6869, -1.4003, -0.9234, -0.9902, -0.3046,  0.8207,  1.6030,\n",
       "          0.5293, -0.5939,  0.5294,  1.1465, -0.0699,  1.3459, -1.1644, -0.0794,\n",
       "          0.8690, -0.2430,  0.4354,  0.2981, -0.8772, -0.1509,  1.1967,  1.2092,\n",
       "          0.5523,  0.5252, -1.1944,  1.0620, -0.9389,  0.6371,  1.0198, -0.1749,\n",
       "          0.7331, -0.8325,  2.0744,  0.1447,  0.0133, -0.7083, -0.4709, -1.1207,\n",
       "         -1.2334, -1.7742, -1.4132, -1.3551,  0.3738, -2.2158,  0.0222,  0.4466,\n",
       "         -0.5087, -1.9868,  1.3617, -2.4028, -0.3262,  0.3261,  0.9443, -0.7643,\n",
       "         -0.6284,  1.0897, -0.9152,  0.3414, -0.6408,  2.6476, -0.7174, -2.4279,\n",
       "          1.0710,  0.9328,  0.8287,  0.4642, -0.0399,  2.0164,  1.5348, -1.3227,\n",
       "         -0.1719,  0.1449, -0.2559, -0.8610, -1.4818,  0.2588,  2.3377, -0.7885,\n",
       "          0.8634, -2.6081, -0.1368,  1.1893,  2.1811,  0.7413, -0.5799, -0.6271,\n",
       "          0.3697,  0.6725,  1.0293,  0.4816,  0.3325, -1.5863, -0.3394,  0.9679,\n",
       "         -0.8785, -2.5014, -0.1359,  0.1270,  1.1633, -0.0154,  1.1511, -1.1827,\n",
       "         -1.2838, -0.2492,  0.4402,  1.7279, -0.1057,  0.9045,  0.2506, -0.6121,\n",
       "          0.9334, -1.7544,  0.0925,  0.0488,  0.9351, -1.9925, -1.1836, -0.4196,\n",
       "         -1.0108,  1.3498, -0.9176, -0.2505,  0.4830,  0.1450,  0.1189,  1.6800,\n",
       "         -0.3155,  0.1340, -0.5198, -0.8531,  1.9006,  2.6716, -0.3825,  1.4587,\n",
       "         -0.3353, -1.0718, -1.2997,  0.1398, -0.1368,  1.4126,  0.8002,  1.3482,\n",
       "         -1.1585,  1.1168, -0.4854,  0.9374,  0.4886, -1.2752,  0.7430,  0.0339]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(lm_embeddings[0], img_embeddings[0], mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bashlab_cogenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
