{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Modeling with MULTFS\n",
    "\n",
    "#### Overview:\n",
    "This notebook aims to provided an example of training a PyTorch Model on a premade task dataset\n",
    "- Text-embedder: ```all-mpnet-base-v2 (pretrained Sentence Transformer)```\n",
    "- Image-embedder: ```vit_b_16 (pretrained Vision Transformer)``` \n",
    "- Decoder/Classifier: Transformer Decoder only Model trained on both text and image embeddings to out put action class\n",
    "\n",
    "\n",
    "#### Dataset:\n",
    "- 50,000 trials over X(TODO) different tasks\n",
    "    - Train: 30,000\n",
    "    - Validation: 10,000\n",
    "    - Test: 10,000\n",
    "\n",
    "*See randTaskGen.ipynb notebook for the dataset generation*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasgomez/opt/miniconda3/envs/bashlab_cogenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'datasets/trials'  # Dataset Directory\n",
    "BATCH_SIZE = 1\n",
    "MAX_FRAMES = 3  # the max possible frames across tasks\n",
    "VIT_OUT_DIM = 1000 # vision transformer output dimension\n",
    "LM_OUT_DIM = 768 # language model output dimension\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "- Read in the pregenerated task trials organized into frames, instructions, and correct actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "infos = []\n",
    "\n",
    "for trial_fp in os.listdir(DIR):\n",
    "    trial_fp = os.path.join(DIR, trial_fp)\n",
    "    imgs = []\n",
    "    for fp in os.listdir(trial_fp):\n",
    "        fp = os.path.join(trial_fp, fp)\n",
    "        if fp[-4:] == '.png':\n",
    "            imgs.append(np.rollaxis(np.array(Image.open(fp), dtype=np.float32),2,0))\n",
    "        else:\n",
    "            infos.append(json.load(open(fp)))\n",
    "    frames.append(np.array(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = [x['instruction'] for x in infos]\n",
    "raw_target_actions = [x['answers'] for x in infos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Target Actions\n",
    "- Encodes the target actions into one hot encoding vectors corresponding to the actions ```true, false, and null```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0, 0, 1]), array([0, 1, 0])],\n",
       " [array([0, 0, 1]), array([0, 1, 0])],\n",
       " [array([0, 0, 1]), array([0, 1, 0])],\n",
       " [array([0, 0, 1]), array([0, 1, 0])],\n",
       " [array([0, 0, 1]), array([0, 1, 0])]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_map = {'true': np.array([1,0,0]), 'false': np.array([0,1,0]), 'null': np.array([0,0,1])}\n",
    "\n",
    "target_actions = []\n",
    "\n",
    "for actions in raw_target_actions:\n",
    "    encoded = []\n",
    "    for action in actions:\n",
    "        encoded.append(action_map[action])\n",
    "    target_actions.append(encoded)\n",
    "\n",
    "target_actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionsDataset(Dataset):\n",
    "  \"\"\"\n",
    "    Pytorch Dataset class to load the Instructions Data\n",
    "\n",
    "    Data members:\n",
    "      instructions: list of instructions\n",
    "      n_ins: number of instructions in the dataset\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __len__: returns n_ins\n",
    "      __getitem__: returns an instruction\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, x):\n",
    "\n",
    "    self.instructions = x\n",
    "\n",
    "    self.n_ins = len(self.instructions)\n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "      Returns number of instructions in the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return self.n_ins\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "      Given an index return a instruction at that index\n",
    "    \"\"\"\n",
    "\n",
    "    return self.instructions[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionsCollator(object):\n",
    "  \"\"\"\n",
    "    Data Collator used for GPT2 in a classificaiton tasks\n",
    "\n",
    "    Args:\n",
    "      use_tokenizer :\n",
    "        Transformer type tokenizer used to process raw text into numbers.\n",
    "\n",
    "    Data members:\n",
    "      use_tokenizer: Tokenizer to be used inside the class.\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __call__: tokenize input\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "  def __init__(self, use_tokenizer):\n",
    "\n",
    "    self.use_tokenizer = use_tokenizer\n",
    "\n",
    "    return\n",
    "\n",
    "  def __call__(self, instructions):\n",
    "    \"\"\"\n",
    "        Tokenizes input\n",
    "    \"\"\"\n",
    "\n",
    "    # Call tokenizer\n",
    "    inputs = self.use_tokenizer(instructions, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained Language Model and Tokenizer \n",
    "lm_encoder = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Create data collator to encode text and labels into numbers.\n",
    "InstructionsCollator = InstructionsCollator(use_tokenizer=tokenizer)\n",
    "\n",
    "# Create pytorch dataset for instructions\n",
    "ins_train_dataset = InstructionsDataset(instructions)\n",
    "\n",
    "# Move pytorch dataset into dataloader \n",
    "ins_train_dataloader = DataLoader(ins_train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=InstructionsCollator)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frames Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FramesDataset(Dataset):\n",
    "  \"\"\"\n",
    "    Pytorch Dataset class to load the Frame Data\n",
    "\n",
    "    Data members:\n",
    "      frames``ist of frames\n",
    "      n_imgs: number of iamges in the dataset\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __len__: returns n_imgs\n",
    "      __getitem__: returns an frame\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, x):\n",
    "\n",
    "    self.frames = x\n",
    "\n",
    "    self.n_imgs = len(self.frames)\n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "      Returns number of frames in the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return self.n_imgs\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "      Given an index return a frame\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.tensor(self.frames[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained Vision Transformer\n",
    "vit_encoder = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.DEFAULT)\n",
    "\n",
    "# Create pytorch dataset for instructions\n",
    "frames_train_dataset = FramesDataset(frames)\n",
    "\n",
    "# Move pytorch dataset into dataloader \n",
    "frames_train_dataloader = DataLoader(frames_train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_embedder(instruction, encoder):\n",
    "    #Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        lm_output = encoder(**instruction)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(lm_output, instruction['attention_mask'])\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        \n",
    "    return sentence_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Embedder\n",
    "- We pad frames based on max possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_embedder(frames, encoder):\n",
    "    with torch.no_grad():\n",
    "        vit_out = encoder(torch.tensor(frames))\n",
    "\n",
    "    npads = MAX_FRAMES-len(vit_out)\n",
    "    pad = torch.ones((npads, vit_out.shape[1]))\n",
    "    vit_out = torch.cat((vit_out, pad))\n",
    "\n",
    "    return vit_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "lm_embeddings = []\n",
    "img_embeddings = []\n",
    "\n",
    "for i,f in zip(ins_train_dataloader,frames_train_dataloader):\n",
    "   f = f[0]\n",
    "   lm_embeddings.append(lm_embedder(i, lm_encoder))\n",
    "   img_embeddings.append(img_embedder(f, vit_encoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsDataset(Dataset):\n",
    "  \"\"\"\n",
    "    Pytorch Dataset class to load the embedded data\n",
    "\n",
    "    Data members:\n",
    "      lm_embeddings: list of language model embeddings\n",
    "      img_embeddings: list of language model embeddings\n",
    "      n_embs: number of embeddings in the dataset\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __len__: returns n_ins\n",
    "      __getitem__: returns an instruction\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, lm_embeddings, img_embeddings, actions):\n",
    "\n",
    "    self.lm_embeddings = lm_embeddings\n",
    "    self.img_embeddings = img_embeddings\n",
    "    self.actions = actions\n",
    "\n",
    "    self.n_embs = len(self.lm_embeddings)\n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "      Returns number of instructions in the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return self.n_embs\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "      Given an index return a instruction at that index\n",
    "    \"\"\"\n",
    "\n",
    "    return {'instruction':self.lm_embeddings[idx], 'frames':self.img_embeddings[idx], 'actions':torch.tensor(self.actions[idx], dtype=torch.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pytorch dataset for train and val data\n",
    "train_dataset = EmbeddingsDataset(lm_embeddings[0:-1],img_embeddings[0:-1], target_actions[0:-1])\n",
    "val_dataset = EmbeddingsDataset(lm_embeddings[-1:],img_embeddings[-1:], target_actions[-1:])\n",
    "\n",
    "# Move pytorch datasets into dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import projections\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CausalMatchTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch based transformer decoder model\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize Model with Params\n",
    "    def __init__(self, nframes=MAX_FRAMES, blocks=3, nhead=5, emb_dim=VIT_OUT_DIM, classes=3, device=device):\n",
    "        super().__init__()\n",
    "\n",
    "        # Device\n",
    "        self.device = device\n",
    "\n",
    "        # Embedding Dimension\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        # Number of frames\n",
    "        self.nframes = nframes\n",
    "\n",
    "        # Frame Position Embedder Layer\n",
    "        self.pos_emb = nn.Parameter(torch.Tensor(nframes,emb_dim)).to(device)\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "           self.pos_emb,\n",
    "           gain=torch.nn.init.calculate_gain(\"linear\"))\n",
    "\n",
    "        # Instruction Dim Projection Layer\n",
    "        self.lm_linear_layer = nn.Linear(LM_OUT_DIM, emb_dim).to(device)\n",
    "\n",
    "        # Decoder Layers\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=nhead, batch_first=True).to(device)\n",
    "        self.decoder_layers = _get_clones(self.decoder_layer, blocks)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layers, num_layers=blocks).to(device)\n",
    "\n",
    "        # Action classifier\n",
    "        self.classifier = nn.Linear(emb_dim, classes).to(device)\n",
    "\n",
    "    # Function for forward pass\n",
    "    def forward(self, instruction, frames, mask, padding_mask):\n",
    "\n",
    "        # Project instruction embedding\n",
    "        instruction = self.lm_linear_layer(instruction)\n",
    "\n",
    "        # Add the frame position embedding\n",
    "        for i in range(len(frames)):\n",
    "            frames[i] += self.pos_emb[i,:]\n",
    "\n",
    "        # Apply each Decoder Layer (block)\n",
    "        for layer in self.decoder_layers:\n",
    "            frames = layer(frames, instruction, tgt_mask=mask, tgt_key_padding_mask=padding_mask, ) \n",
    "\n",
    "        # Pass through linear layer for classification\n",
    "        output = self.classifier(frames)\n",
    "\n",
    "        return output\n",
    " \n",
    "# Creates a list of torch duplicate torch modules\n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "# Creates a square Sequential/Causal mask of size sz\n",
    "def generate_causal_mask(sz: int) -> Tensor:\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "# Generates a padding masks for each sequence in a batch\n",
    "def generate_pad_mask(batch):\n",
    "\n",
    "    pad_tensor = torch.ones((batch.shape[2])).to(device)\n",
    "\n",
    "    mask = np.zeros((batch.shape[0],batch.shape[1]))\n",
    "\n",
    "    for s in range(0, batch.shape[0]):\n",
    "        for v in range(0, batch[s].shape[0]):\n",
    "            new_s = torch.all(batch[s][v] == pad_tensor)\n",
    "            mask[s][v] = new_s\n",
    "\n",
    "    return torch.tensor(mask).bool().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "model = CausalMatchTransformer(nframes=MAX_FRAMES,\n",
    "                               blocks=3,\n",
    "                               nhead=5,\n",
    "                               emb_dim=VIT_OUT_DIM,\n",
    "                               device=device).float().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcbd8b82a10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training configurations\n",
    "epochs = 5\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(\n",
    "    (p for p in model.parameters() if p.requires_grad), lr=lr\n",
    ")\n",
    "\n",
    "mask = generate_causal_mask(MAX_FRAMES).to(device)\n",
    "\n",
    "# torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "epoch=0\n",
      "epoch=1\n",
      "epoch=2\n",
      "epoch=3\n",
      "epoch=4\n"
     ]
    }
   ],
   "source": [
    "# Training and validation loop\n",
    "\n",
    "# Store the average loss after each epoch\n",
    "all_loss = {'train_loss':[], 'val_loss':[]}\n",
    "all_acc = {'train_acc':[], 'val_acc':[]}\n",
    "\n",
    "print(\"starting\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch={epoch}\")\n",
    "\n",
    "    # Epoch stat trackers\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_count = 0\n",
    "    for idx, batch in enumerate(iter(train_dataloader)):\n",
    "\n",
    "        # Inputs and Targets\n",
    "        instruction = batch['instruction']\n",
    "        frames = batch['frames']\n",
    "        targets = batch['actions']\n",
    "        \n",
    "        # Frame Padding\n",
    "        padding_mask = generate_pad_mask(batch=frames)\n",
    "        pad_indexes = np.argwhere(np.array(padding_mask) == False)[:,1]\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = model(instruction, frames, mask, padding_mask)\n",
    "        predictions = predictions[:,pad_indexes]\n",
    "\n",
    "        # Get Loss\n",
    "        loss = criterion(predictions, targets)\n",
    "\n",
    "\n",
    "        # Track stats\n",
    "        # torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)\n",
    "        correct = predictions.softmax(dim=1) == targets\n",
    "        acc = correct.sum().item() / correct.size(0)\n",
    "        epoch_correct += correct.sum().item()\n",
    "        epoch_count += correct.size(0)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate on validation set every 5 epochs\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        # Turn off gradient calcs\n",
    "        with torch.no_grad():\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_correct = 0\n",
    "            val_epoch_count = 0\n",
    "\n",
    "            for idx, batch in enumerate(iter(val_dataloader)):\n",
    "                # Inputs and Targets\n",
    "                instruction = batch['instruction']\n",
    "                frames = batch['frames']\n",
    "                targets = batch['actions']\n",
    "                \n",
    "                # Frame Padding\n",
    "                padding_mask = generate_pad_mask(batch=frames)\n",
    "                pad_indexes = np.argwhere(np.array(padding_mask) == False)[:,1]\n",
    "\n",
    "                # Get predictions\n",
    "                predictions = model(instruction, frames, mask, padding_mask)\n",
    "                predictions = predictions[:,pad_indexes]\n",
    "\n",
    "                # Get Loss\n",
    "                loss = criterion(predictions, targets)\n",
    "\n",
    "                correct = predictions.softmax(dim=1) == targets\n",
    "                acc = correct.sum().item() / correct.size(0)\n",
    "\n",
    "                val_epoch_correct += correct.sum().item()\n",
    "                val_epoch_count += correct.size(0)\n",
    "                val_epoch_loss += loss.item()\n",
    "\n",
    "        # Track loss and acc ever 5 epochs\n",
    "        avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "        avg_val_loss = val_epoch_loss / len(val_dataloader)\n",
    "\n",
    "        all_loss['train_loss'].append(avg_train_loss)\n",
    "        all_loss['val_loss'].append(avg_val_loss)\n",
    "\n",
    "        all_acc['train_acc'].append(epoch_correct / epoch_count)\n",
    "        all_acc['val_acc'].append(val_epoch_correct / val_epoch_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bashlab_cogenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
