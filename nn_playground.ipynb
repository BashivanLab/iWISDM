{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "Things you might ahve to do...\n",
    "- add nn.LayerNorm to decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'datasets/trials'\n",
    "BATCH_SIZE = 1\n",
    "MAX_FRAMES = 3\n",
    "VIT_OUT_DIM = 1000\n",
    "LM_OUT_DIM = 768\n",
    "ACT_TOKEN = '[ACT]'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3]).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "infos = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for trial_fp in os.listdir(DIR):\n",
    "    trial_fp = os.path.join(DIR, trial_fp)\n",
    "    imgs = []\n",
    "    for fp in os.listdir(trial_fp):\n",
    "        fp = os.path.join(trial_fp, fp)\n",
    "        if fp[-4:] == '.png':\n",
    "            imgs.append(np.rollaxis(np.array(Image.open(fp), dtype=np.float32),2,0))\n",
    "        else:\n",
    "            infos.append(json.load(open(fp)))\n",
    "    frames.append(np.array(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = [x['instruction'] for x in infos]\n",
    "raw_target_actions = [x['answers'] for x in infos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Target Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0, 0, 1]), array([0, 1, 0])],\n",
       " [array([0, 0, 1]), array([0, 1, 0])],\n",
       " [array([0, 0, 1]), array([0, 1, 0])],\n",
       " [array([0, 0, 1]), array([0, 1, 0])],\n",
       " [array([0, 0, 1]), array([0, 1, 0])]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_map = {'true': np.array([1,0,0]), 'false': np.array([0,1,0]), 'null': np.array([0,0,1])}\n",
    "\n",
    "target_actions = []\n",
    "\n",
    "for actions in raw_target_actions:\n",
    "    encoded = []\n",
    "    for action in actions:\n",
    "        encoded.append(action_map[action])\n",
    "    target_actions.append(encoded)\n",
    "\n",
    "target_actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionsDataset(Dataset):\n",
    "  \"\"\"\n",
    "    Pytorch Dataset class to load the Instructions Data\n",
    "\n",
    "    Data members:\n",
    "      instructions: list of instructions\n",
    "      n_ins: number of instructions in the dataset\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __len__: returns n_ins\n",
    "      __getitem__: returns an instruction\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, x):\n",
    "\n",
    "    self.instructions = x\n",
    "\n",
    "    self.n_ins = len(self.instructions)\n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "      Returns number of instructions in the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return self.n_ins\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "      Given an index return a instruction at that index\n",
    "    \"\"\"\n",
    "\n",
    "    return self.instructions[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionsCollator(object):\n",
    "  \"\"\"\n",
    "    Data Collator used for GPT2 in a classificaiton tasks\n",
    "\n",
    "    Args:\n",
    "      use_tokenizer :\n",
    "        Transformer type tokenizer used to process raw text into numbers.\n",
    "\n",
    "    Data members:\n",
    "      use_tokenizer: Tokenizer to be used inside the class.\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __call__: tokenize input\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "  def __init__(self, use_tokenizer):\n",
    "\n",
    "    self.use_tokenizer = use_tokenizer\n",
    "\n",
    "    return\n",
    "\n",
    "  def __call__(self, instructions):\n",
    "    \"\"\"\n",
    "        Tokenizes input\n",
    "    \"\"\"\n",
    "\n",
    "    # Call tokenizer\n",
    "    inputs = self.use_tokenizer(instructions, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_encoder = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Create data collator to encode text and labels into numbers.\n",
    "InstructionsCollator = InstructionsCollator(use_tokenizer=tokenizer)\n",
    "\n",
    "# Create pytorch dataset for instructions\n",
    "ins_train_dataset = InstructionsDataset(instructions)\n",
    "\n",
    "# Move pytorch dataset into dataloader \n",
    "ins_train_dataloader = DataLoader(ins_train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=InstructionsCollator)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frames Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FramesDataset(Dataset):\n",
    "  \"\"\"\n",
    "    Pytorch Dataset class to load the Frame Data\n",
    "\n",
    "    Data members:\n",
    "      frames``ist of frames\n",
    "      n_imgs: number of iamges in the dataset\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __len__: returns n_imgs\n",
    "      __getitem__: returns an frame\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, x):\n",
    "\n",
    "    self.frames = x\n",
    "\n",
    "    self.n_imgs = len(self.frames)\n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "      Returns number of frames in the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return self.n_imgs\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "      Given an index return a frame\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.tensor(self.frames[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_encoder = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.DEFAULT)\n",
    "\n",
    "# Create pytorch dataset for instructions\n",
    "frames_train_dataset = FramesDataset(frames)\n",
    "\n",
    "# Move pytorch dataset into dataloader \n",
    "frames_train_dataloader = DataLoader(frames_train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_embedder(instruction, encoder):\n",
    "    #Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        lm_output = encoder(**instruction)\n",
    "        # print(lm_output[0].size())\n",
    "        # print(lm_output.pooler_output.shape)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(lm_output, instruction['attention_mask'])\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        \n",
    "    return sentence_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_embedder(frames, encoder):\n",
    "    with torch.no_grad():\n",
    "        vit_out = encoder(torch.tensor(frames))\n",
    "\n",
    "    npads = MAX_FRAMES-len(vit_out)\n",
    "    pad = torch.ones((npads, vit_out.shape[1]))\n",
    "    vit_out = torch.cat((vit_out, pad))\n",
    "\n",
    "    return vit_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_13764\\3025187430.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vit_out = encoder(torch.tensor(frames))\n"
     ]
    }
   ],
   "source": [
    "lm_embeddings = []\n",
    "img_embeddings = []\n",
    "\n",
    "for i,f in zip(ins_train_dataloader,frames_train_dataloader):\n",
    "   f = f[0]\n",
    "   lm_embeddings.append(lm_embedder(i, lm_encoder))\n",
    "   img_embeddings.append(img_embedder(f, vit_encoder))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsDataset(Dataset):\n",
    "  \"\"\"\n",
    "    Pytorch Dataset class to load the embedded data\n",
    "\n",
    "    Data members:\n",
    "      lm_embeddings: list of language model embeddings\n",
    "      img_embeddings: list of language model embeddings\n",
    "      n_embs: number of embeddings in the dataset\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __len__: returns n_ins\n",
    "      __getitem__: returns an instruction\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, lm_embeddings, img_embeddings, actions):\n",
    "\n",
    "    self.lm_embeddings = lm_embeddings\n",
    "    self.img_embeddings = img_embeddings\n",
    "    self.actions = actions\n",
    "\n",
    "    self.n_embs = len(self.lm_embeddings)\n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "      Returns number of instructions in the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return self.n_embs\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "      Given an index return a instruction at that index\n",
    "    \"\"\"\n",
    "\n",
    "    return {'instruction':self.lm_embeddings[idx], 'frames':self.img_embeddings[idx], 'actions':torch.tensor(self.actions[idx], dtype=torch.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pytorch dataset for instructions\n",
    "train_dataset = EmbeddingsDataset(lm_embeddings[0:-1],img_embeddings[0:-1], target_actions[0:-1])\n",
    "val_dataset = EmbeddingsDataset(lm_embeddings[-1:],img_embeddings[-1:], target_actions[-1:])\n",
    "\n",
    "# Move pytorch dataset into dataloader \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_ataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import projections\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CausalMatchTransformer(nn.Module):\n",
    "\n",
    "    # Initialize Model with Params\n",
    "    def __init__(self, nframes=MAX_FRAMES, blocks=3, nhead=5, emb_dim=VIT_OUT_DIM, classes=3, device=device):\n",
    "        super().__init__()\n",
    "\n",
    "        # Device\n",
    "        self.device = device\n",
    "\n",
    "        # Embedding Dimension\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        # Number of frames\n",
    "        self.nframes = nframes\n",
    "\n",
    "        # Frame Position Embedder Layer\n",
    "        self.pos_emb = nn.Parameter(torch.Tensor(nframes,emb_dim)).to(device)\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "           self.pos_emb,\n",
    "           gain=torch.nn.init.calculate_gain(\"linear\"))\n",
    "\n",
    "        # Instruction Dim Projection Layer\n",
    "        self.lm_linear_layer = nn.Linear(LM_OUT_DIM, emb_dim).to(device)\n",
    "\n",
    "        # Decoder Layers\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=nhead, batch_first=True).to(device)\n",
    "        self.decoder_layers = _get_clones(self.decoder_layer, blocks)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layers, num_layers=blocks).to(device)\n",
    "\n",
    "        # Action classifier\n",
    "        self.classifier = nn.Linear(emb_dim, classes).to(device)\n",
    "\n",
    "    # Function for forward pass\n",
    "    def forward(self, instruction, frames, mask, padding_mask):\n",
    "\n",
    "        instruction = self.lm_linear_layer(instruction)\n",
    "\n",
    "        for i in range(len(frames)):\n",
    "            frames[i] += self.pos_emb\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            frames = layer(frames, instruction, tgt_mask=mask, tgt_key_padding_mask=padding_mask, ) \n",
    "\n",
    "        output = self.classifier(frames)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def backpass(self, loss):\n",
    "        print(self.pos_emb)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(), 0.5)\n",
    "\n",
    "    \n",
    "# Creates a list of torch duplicate torch modules\n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "def generate_causal_mask(sz: int) -> Tensor:\n",
    "    \n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "# Generates a padding masks for each sequence in a batch\n",
    "def generate_pad_mask(batch):\n",
    "\n",
    "    pad_tensor = torch.ones((batch.shape[2])).to(device)\n",
    "\n",
    "    mask = np.zeros((batch.shape[0],batch.shape[1]))\n",
    "\n",
    "    for s in range(0, batch.shape[0]):\n",
    "        for v in range(0, batch[s].shape[0]):\n",
    "            new_s = torch.all(batch[s][v] == pad_tensor)\n",
    "            mask[s][v] = new_s\n",
    "\n",
    "    return torch.tensor(mask).bool().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CausalMatchTransformer(nframes=MAX_FRAMES,\n",
    "                               blocks=3,\n",
    "                               nhead=5,\n",
    "                               emb_dim=VIT_OUT_DIM,\n",
    "                               device=device).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1968cac1d70>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configurations\n",
    "epochs = 5\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(\n",
    "    (p for p in model.parameters() if p.requires_grad), lr=lr\n",
    ")\n",
    "\n",
    "mask = generate_causal_mask(MAX_FRAMES).to(device)\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/szagoruyko/pytorchviz.git@master\n",
      "  Cloning https://github.com/szagoruyko/pytorchviz.git (to revision master) to c:\\users\\lucas\\appdata\\local\\temp\\pip-req-build-4o6fcsgb\n",
      "  Resolved https://github.com/szagoruyko/pytorchviz.git to commit 0adcd83af8aa7ab36d6afd139cabbd9df598edb7\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\lucas\\miniconda3\\envs\\bashlab_cogenv\\lib\\site-packages (from torchviz==0.0.2) (2.0.1)\n",
      "Requirement already satisfied: graphviz in c:\\users\\lucas\\miniconda3\\envs\\bashlab_cogenv\\lib\\site-packages (from torchviz==0.0.2) (0.20.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucas\\miniconda3\\envs\\bashlab_cogenv\\lib\\site-packages (from torch->torchviz==0.0.2) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\lucas\\miniconda3\\envs\\bashlab_cogenv\\lib\\site-packages (from torch->torchviz==0.0.2) (4.6.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\lucas\\miniconda3\\envs\\bashlab_cogenv\\lib\\site-packages (from torch->torchviz==0.0.2) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\lucas\\miniconda3\\envs\\bashlab_cogenv\\lib\\site-packages (from torch->torchviz==0.0.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lucas\\miniconda3\\envs\\bashlab_cogenv\\lib\\site-packages (from torch->torchviz==0.0.2) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lucas\\miniconda3\\envs\\bashlab_cogenv\\lib\\site-packages (from jinja2->torch->torchviz==0.0.2) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lucas\\miniconda3\\envs\\bashlab_cogenv\\lib\\site-packages (from sympy->torch->torchviz==0.0.2) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/szagoruyko/pytorchviz.git 'C:\\Users\\Lucas\\AppData\\Local\\Temp\\pip-req-build-4o6fcsgb'\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U git+https://github.com/szagoruyko/pytorchviz.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"PATH\"] += os.pathsep + 'C:\\\\Program Files\\\\Graphviz\\\\bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot, make_dot_from_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas\\miniconda3\\envs\\bashlab_cogenv\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0159,  0.0579, -0.0125,  ...,  0.0222,  0.0440, -0.0483],\n",
      "        [-0.0449,  0.0342, -0.0054,  ..., -0.0522, -0.0659, -0.0476],\n",
      "        [ 0.0020, -0.0351,  0.0172,  ...,  0.0771,  0.0233,  0.0302]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Mismatch in shape: grad_output[0] has a shape of torch.Size([]) and output[0] has a shape of torch.Size([3, 1000]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m loss \u001b[39m=\u001b[39m criterion(predictions, targets)\n\u001b[0;32m     20\u001b[0m model\u001b[39m.\u001b[39mbackpass(loss)\n\u001b[1;32m---> 22\u001b[0m model\u001b[39m.\u001b[39;49mpos_emb\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[0;32m     24\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mpos_emb)\n\u001b[0;32m     26\u001b[0m test_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\miniconda3\\envs\\bashlab_cogenv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\miniconda3\\envs\\bashlab_cogenv\\lib\\site-packages\\torch\\autograd\\__init__.py:193\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    189\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[0;32m    190\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[0;32m    192\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[1;32m--> 193\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    194\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[1;32mc:\\Users\\Lucas\\miniconda3\\envs\\bashlab_cogenv\\lib\\site-packages\\torch\\autograd\\__init__.py:71\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf `is_grads_batched=True`, we interpret the first \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mdimension of each grad_output as the batch dimension. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mThe sizes of the remaining dimensions are expected to match \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mIf you only want some tensors in `grad_output` to be considered \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mbatched, consider using vmap.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 71\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMismatch in shape: grad_output[\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m                            \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(grads\u001b[39m.\u001b[39mindex(grad)) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m] has a shape of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m                            \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(grad_shape) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m and output[\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m                            \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(outputs\u001b[39m.\u001b[39mindex(out)) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m] has a shape of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m                            \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(out_shape) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mis_complex \u001b[39m!=\u001b[39m grad\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mis_complex:\n\u001b[0;32m     77\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFor complex Tensors, both grad_output and output\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39m are required to have the same dtype.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39m Mismatch in dtype: grad_output[\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m                        \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(outputs\u001b[39m.\u001b[39mindex(out)) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m] has a dtype of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m                        \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(out\u001b[39m.\u001b[39mdtype) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Mismatch in shape: grad_output[0] has a shape of torch.Size([]) and output[0] has a shape of torch.Size([3, 1000])."
     ]
    }
   ],
   "source": [
    "test_count = 0\n",
    "for idx, batch in enumerate(iter(train_dataloader)):\n",
    "\n",
    "    if test_count == 1:\n",
    "        break\n",
    "\n",
    "    instruction = batch['instruction']\n",
    "    frames = batch['frames']\n",
    "    targets = batch['actions']\n",
    "    \n",
    "\n",
    "    padding_mask = generate_pad_mask(batch=frames)\n",
    "    pad_indexes = np.argwhere(np.array(padding_mask) == False)[:,1]\n",
    "\n",
    "    predictions = model(instruction, frames, mask, padding_mask)\n",
    "    predictions = predictions[:,pad_indexes]\n",
    "\n",
    "    loss = criterion(predictions, targets)\n",
    "\n",
    "    model.backpass(loss)\n",
    "\n",
    "    model.pos_emb.backward(loss)\n",
    "\n",
    "    print(model.pos_emb)\n",
    "\n",
    "    test_count += 1\n",
    "\n",
    "    graph = make_dot(predictions, params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "epoch=0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'episode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m epoch_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39miter\u001b[39m(train_dataloader)):\n\u001b[1;32m---> 17\u001b[0m     \u001b[39mprint\u001b[39m(batch[\u001b[39m'\u001b[39;49m\u001b[39mepisode\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     19\u001b[0m     \u001b[39m# Clear any past grads\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     model\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mKeyError\u001b[0m: 'episode'"
     ]
    }
   ],
   "source": [
    "# Training and validation loop\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "all_loss = {'train_loss':[], 'val_loss':[]}\n",
    "all_acc = {'train_acc':[], 'val_acc':[]}\n",
    "\n",
    "print(\"starting\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch={epoch}\")\n",
    "\n",
    "    # Epoch stat trackers\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_count = 0\n",
    "    for idx, batch in enumerate(iter(train_dataloader)):\n",
    "\n",
    "        print(batch['episode'].shape)\n",
    "\n",
    "        # Clear any past grads\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = model(batch['episode'].float().to(device), mask)\n",
    "\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        # Track stats\n",
    "        correct = predictions.argmax(axis=1) == labels\n",
    "        acc = correct.sum().item() / correct.size(0)\n",
    "        epoch_correct += correct.sum().item()\n",
    "        epoch_count += correct.size(0)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate on validation set every 5 epochs\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        # Turn off gradient calcs\n",
    "        with torch.no_grad():\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_correct = 0\n",
    "            val_epoch_count = 0\n",
    "\n",
    "            for idx, batch in enumerate(iter(val_dataloader)):\n",
    "                predictions = model(batch['episode'].float().to(device), mask)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                v_loss = criterion(predictions, labels)\n",
    "\n",
    "                correct = predictions.argmax(axis=1) == labels\n",
    "                acc = correct.sum().item() / correct.size(0)\n",
    "\n",
    "                val_epoch_correct += correct.sum().item()\n",
    "                val_epoch_count += correct.size(0)\n",
    "                val_epoch_loss += loss.item()\n",
    "\n",
    "        # Print losses and accuracies every 5 epochs\n",
    "        # print(f\"val_epoch_loss={val_epoch_loss}\")\n",
    "        # print(f\"val epoch accuracy: {val_epoch_correct / val_epoch_count}\")\n",
    "        # print(f\"epoch_loss={epoch_loss}\")\n",
    "        # print(f\"epoch accuracy: {epoch_correct / epoch_count}\")\n",
    "\n",
    "        # Track loss and acc ever 5 epochs\n",
    "        avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "        avg_val_loss = val_epoch_loss / len(val_dataloader)\n",
    "\n",
    "        all_loss['train_loss'].append(avg_train_loss)\n",
    "        all_loss['val_loss'].append(avg_val_loss)\n",
    "\n",
    "        all_acc['train_acc'].append(epoch_correct / epoch_count)\n",
    "        all_acc['val_acc'].append(val_epoch_correct / val_epoch_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bashlab_cogenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
