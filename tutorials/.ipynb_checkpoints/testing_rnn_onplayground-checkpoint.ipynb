{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Modeling with MULTFS\n",
    "\n",
    "#### Overview:\n",
    "This notebook aims to provided an example of training a PyTorch Model on a premade task dataset\n",
    "- Text-embedder: ```all-mpnet-base-v2 (pretrained Sentence Transformer)```\n",
    "- Image-embedder: ```vit_b_16 (pretrained Vision Transformer)``` \n",
    "- Decoder/Classifier: Transformer Decoder only Model trained on both text and image embeddings to out put action class\n",
    "\n",
    "\n",
    "#### Datasets/Training:\n",
    "- In this notebook we use pre-saved and generated datasets\n",
    "    - *See the ..._dataset_gen.ipynb notebooks for how to generate and save a dataset*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 17:05:29.306398: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-01 17:05:29.306537: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-01 17:05:29.306625: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-01 17:05:29.312581: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-01 17:05:30.426722: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import math\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from cognitive.task_bank import CompareLocTemporal\n",
    "from cognitive import task_generator as tg\n",
    "from cognitive import constants as const\n",
    "from cognitive import stim_generator as sg\n",
    "from cognitive import info_generator as ig\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = '../datasets/train_big'  # Training Dataset Directory\n",
    "VAL_DIR = '../datasets/val_big'  # Validation Dataset Directory\n",
    "TEST_DIR = '../datasets/val_big'  # Testing Dataset Directory\n",
    "LM_PATH = 'offline_models/all-mpnet-base-v2'\n",
    "IMGM_PATH = 'offline_models/resnet/resnet'\n",
    "EMB_DIR = '../datasets/embeddings'\n",
    "BATCH_SIZE = 128\n",
    "MAX_FRAMES = 2  # the max possible frames across tasks\n",
    "IMGM_OUT_DIM = 2048 # vision transformer output dimension\n",
    "LM_OUT_DIM = 768 # language model output dimension\n",
    "\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Request GPU device 0\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    # If no GPU is available, fall back to CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Language Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_embedder(instruction, encoder, tokenizer):\n",
    "    instruction = tokenizer(instruction, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    \n",
    "    #Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        lm_output = encoder(**instruction)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(lm_output, instruction['attention_mask'])\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        \n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Image Embedder\n",
    "- We pad frames based on max possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_embedder(frames, encoder):\n",
    "\n",
    "    activation = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    encoder.layer4[2].register_forward_hook(get_activation('layer'))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder(torch.tensor(frames))\n",
    "        out = activation['layer']\n",
    "        # out = torch.flatten(activation['layer'], start_dim=1, end_dim=3)\n",
    "        # out = torch.squeeze(activation['layer'], (2,3)) #torch.flatten(, start_dim=1, end_dim=2)\n",
    "\n",
    "    npads = MAX_FRAMES-len(out)\n",
    "    pad = torch.ones((npads, out.shape[1], out.shape[2], out.shape[3]))\n",
    "    out = torch.cat((out, pad.to(device)))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskDataset(Dataset):\n",
    "    # todo: this is not the most efficient way to access data, since each time it has to read from the directory \n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        # preprocessing steps for pretrained ResNet models\n",
    "        self.transform = transforms.Compose([\n",
    "                            transforms.Resize(224),\n",
    "                            transforms.CenterCrop(224), # todo: to delete for shapenet task; why?\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                        ])\n",
    "    \n",
    "\n",
    "        # check the size of the dataset\n",
    "        self.dataset_size = 0\n",
    "        items = os.listdir(self.root_dir)\n",
    "        for item in items:\n",
    "            item_path = os.path.join(self.root_dir, item)\n",
    "            # Check if the item is a directory\n",
    "            if os.path.isdir(item_path):\n",
    "                self.dataset_size += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trial_path = os.path.join(self.root_dir, \"trial%d\"%idx)\n",
    "        images = []\n",
    "        \n",
    "        for fp in os.listdir(trial_path):\n",
    "            fp = os.path.join(trial_path, fp)\n",
    "\n",
    "            if fp[-4:] == '.png':\n",
    "                img = Image.open(fp)\n",
    "                img = self.transform(img)\n",
    "                images.append(img)\n",
    "            elif 'trial_info' in fp:\n",
    "                info = json.load(open(fp))\n",
    "                \n",
    "                actions = self._action_map(info[\"answers\"])\n",
    "\n",
    "                # npads = MAX_FRAMES - len(actions)\n",
    "                # actions.extend([-1 for _ in range(0,npads)])\n",
    "                \n",
    "                instructions = info['instruction']\n",
    "        \n",
    "        images = np.stack(images) # (2*3*224*224)\n",
    "\n",
    "        return {'instructions':instructions, 'frames':torch.tensor(images).to(device), 'actions': torch.tensor(actions).to(device)}\n",
    "    \n",
    "    def _action_map(self, actions):\n",
    "        action_map = {'true': 0, 'false': 1, 'null': 2}\n",
    "        updated_actions = []\n",
    "        for action in actions:\n",
    "            updated_actions.append(action_map[action])\n",
    "\n",
    "        return updated_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_encoder = AutoModel.from_pretrained('offline_models/all-mpnet-base-v2').to(device).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('offline_models/all-mpnet-base-v2')\n",
    "\n",
    "img_encoder = torch.load(IMGM_PATH,map_location=device).to(device).eval()\n",
    "\n",
    "train_TD = TaskDataset(TRAIN_DIR)\n",
    "val_TD = TaskDataset(VAL_DIR)\n",
    "\n",
    "train_DL = DataLoader(train_TD, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_DL = DataLoader(val_TD, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 22.19 GiB total capacity; 21.05 GiB already allocated; 7.31 MiB free; 21.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m val_img_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m val_targets \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frames,instruction,actions \u001b[38;5;129;01min\u001b[39;00m train_DL:\n\u001b[1;32m     11\u001b[0m     frames \u001b[38;5;241m=\u001b[39m frames[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m     train_lm_embeddings\u001b[38;5;241m.\u001b[39mappend(lm_embedder(instruction, lm_encoder, tokenizer)\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/mlti-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/mlti-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/mlti-env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mlti-env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mlti-env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/mlti-env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/mlti-env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/mlti-env/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 22.19 GiB total capacity; 21.05 GiB already allocated; 7.31 MiB free; 21.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# train_lm_embeddings = []\n",
    "# train_img_embeddings = []\n",
    "# train_targets = []\n",
    "\n",
    "# val_lm_embeddings = []\n",
    "# val_img_embeddings = []\n",
    "# val_targets = []\n",
    "\n",
    "\n",
    "# for frames,instruction,actions in train_DL:\n",
    "#     frames = frames[0]\n",
    "\n",
    "#     train_lm_embeddings.append(lm_embedder(instruction, lm_encoder, tokenizer).cpu())\n",
    "#     train_img_embeddings.append(frames)\n",
    "#     # train_img_embeddings.append(img_embedder(frames, img_encoder).cpu())\n",
    "#     train_targets.append(actions[0])\n",
    "\n",
    "\n",
    "# for frames,instruction,actions in val_DL:\n",
    "#     frames = frames[0]\n",
    "\n",
    "#     val_lm_embeddings.append(lm_embedder(instruction, lm_encoder, tokenizer).cpu())\n",
    "#     val_img_embeddings.append(frames)\n",
    "#     # val_img_embeddings.append(img_embedder(frames, img_encoder).cpu())\n",
    "#     val_targets.append(actions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Embeddings Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsDataset(Dataset):\n",
    "  \"\"\"\n",
    "    Pytorch Dataset class to load the embedded data\n",
    "\n",
    "    Data members:\n",
    "      lm_embeddings: list of language model embeddings\n",
    "      img_embeddings: list of language model embeddings\n",
    "      n_embs: number of embeddings in the dataset\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __len__: returns n_ins\n",
    "      __getitem__: returns an instruction\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, lm_embeddings, img_embeddings, actions):\n",
    "\n",
    "    self.lm_embeddings = lm_embeddings\n",
    "    self.img_embeddings = img_embeddings\n",
    "    self.actions = actions\n",
    "\n",
    "    self.n_embs = len(self.lm_embeddings)\n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "      Returns number of instructions in the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return self.n_embs\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "      Given an index return a instruction at that index\n",
    "    \"\"\"\n",
    "\n",
    "    return {'instructions':self.lm_embeddings[idx].to(device), 'frames':self.img_embeddings[idx].to(device), 'actions':torch.tensor(self.actions[idx], dtype=torch.float32, device=device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pytorch dataset for train, val, test data\n",
    "train_dataset = EmbeddingsDataset(train_lm_embeddings, train_img_embeddings, train_targets)\n",
    "val_dataset = EmbeddingsDataset(val_lm_embeddings, val_img_embeddings, val_targets)\n",
    "# test_dataset  = EmbeddingsDataset(test_lm_embeddings, test_img_embeddings, test_targets)\n",
    "\n",
    "# Move pytorch datasets into dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import projections\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "class CNNRNNNet(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, dim_transformer_ffl=2048, nhead = 16, blocks=1, output_size = 3,):\n",
    "        super().__init__()\n",
    "        \n",
    "        # set up the CNN model\n",
    "        self.cnnmodel = torch.load(IMGM_PATH, map_location=device)\n",
    "        # freeze layers of cnn model\n",
    "        for paras in self.cnnmodel.parameters():\n",
    "            paras.requires_grad = False\n",
    "        # get relu activation of last block of resnet50\n",
    "        \n",
    "        self.cnnmodel.layer4[2].relu.register_forward_hook(get_activation('relu'))\n",
    "\n",
    "        self.cnnlayer = torch.nn.Conv2d(2048, hidden_size, 1) # we can also bring the resnet embedding dim to a number different from hidden size\n",
    "\n",
    "        self.input_size = hidden_size*7*7\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.in2hidden = nn.Linear(self.input_size, hidden_size)\n",
    "        self.layer_norm_in = nn.LayerNorm(self.hidden_size)\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = self.hidden_size, \n",
    "            hidden_size = self.hidden_size,\n",
    "            nonlinearity = \"relu\", # guarnatee positive activations\n",
    "            batch_first = True\n",
    "            )\n",
    "\n",
    "        self.pos_emb = PositionalEncoding(hidden_size, MAX_FRAMES)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, dim_feedforward=dim_transformer_ffl, nhead=nhead, batch_first=False)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=blocks)\n",
    "        \n",
    "#         self.mlp = nn.Sequential(\n",
    "#                 nn.Linear(self.hidden_size, 1024),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(1024, 2048),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(2048, hidden_size),\n",
    "#             )\n",
    "        \n",
    "        self.layer_norm_rnn = nn.LayerNorm(self.hidden_size)\n",
    "        self.hidden2output = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_img, hidden_state = None, is_noise = False,):\n",
    "        # preprocess image with resnet\n",
    "        self.batch_size = input_img.shape[0]\n",
    "        self.seq_len = input_img.shape[1]\n",
    "\n",
    "        # x = torch.swapaxes(input_img, 0, 1).float()# (seq_len, batchsize, nc, w, h)\n",
    "        \n",
    "        x = input_img.float()\n",
    "        \n",
    "        x_acts = []\n",
    "        for i in range(self.seq_len):\n",
    "            temp = self.cnnmodel(x[:,i,:,:,:])\n",
    "            x_act = self.cnnlayer(activation[\"relu\"])\n",
    "            x_acts.append(x_act) # (batchsize, nc, w, h) = (batchsize, 2048, 7,7)\n",
    "        \n",
    "        x_acts = torch.stack(x_acts, axis = 1) # (seqlen, batchsize,nc, w,h) rnn_activations\n",
    "        # self.cnn_acts = torch.stack(cnn_acts, axis = 0) # (seqlen, batchsize, nc, w,h) \n",
    "        # self.cnn_acts_down = x_acts\n",
    "        \n",
    "        \n",
    "        x_acts = x_acts.reshape(self.batch_size, x_acts.shape[1], -1) # flatten nc,w,h into one dim\n",
    "        \n",
    "        \n",
    "        \"\"\" RNN METHOD \"\"\"\n",
    "#         if hidden_state == None:\n",
    "#             self.hidden_state = self.init_hidden(batch_size = self.batch_size)\n",
    "#         hidden_x = self.layer_norm_in(torch.relu(self.in2hidden(x_acts.float()))).to(device)\n",
    "        \n",
    "#         rnn_output, _ = self.rnn(hidden_x, self.hidden_state.to(device))\n",
    "#         rnn_output = self.layer_norm_rnn(rnn_output)\n",
    "#         out = self.hidden2output(torch.tanh(rnn_output))\n",
    "        \n",
    "        \"\"\" TRANSFORMER METHOD \"\"\"\n",
    "        hidden_x = self.layer_norm_in(self.pos_emb(self.in2hidden(x_acts.float())))\n",
    "        encoder_output = self.encoder(hidden_x)\n",
    "        out = self.hidden2output(encoder_output)\n",
    "        \n",
    "        \"\"\" MLP METHOD \"\"\"\n",
    "        # hidden_x = self.pos_emb(self.layer_norm_in(torch.relu(self.in2hidden(x_acts.float()))).to(device))\n",
    "        # mlp_output = self.mlp(hidden_x)\n",
    "        # out = self.hidden2output(torch.tanh(mlp_output))\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1, batch_size, self.hidden_size))\n",
    "            \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNRNNNet(hidden_size = 512, dim_transformer_ffl=64, nhead = 16, blocks=1, output_size = 3,).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 5, T_mult=1 )\n",
    "                                                                 # torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training configurations\n",
    "epochs = 20\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "# lr = 1e-3\n",
    "# optimizer = torch.optim.AdamW((p for p in model.parameters() if p.requires_grad), lr = lr)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 3, T_mult=1 )\n",
    "\n",
    "# mask = generate_causal_mask(MAX_FRAMES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculates the number of correct null action predictions and the number of correct non-null action predictions\n",
    "def correct(preds, targs):\n",
    "    null_idxs = torch.where(targs.cpu() == 2)\n",
    "    non_null_idxs = torch.where(targs.cpu() < 2)\n",
    "    \n",
    "    null_preds = preds[null_idxs]\n",
    "    non_null_preds = preds[non_null_idxs]\n",
    "    \n",
    "    c_null = torch.sum(null_preds == targs[null_idxs])\n",
    "    n_null = len(null_preds)\n",
    "    null_acc = c_null/n_null\n",
    "    \n",
    "    c_non_null = torch.sum(non_null_preds == targs[non_null_idxs])\n",
    "    n_non_null = len(non_null_preds)\n",
    "    non_null_acc = c_non_null/n_non_null\n",
    "    \n",
    "    return null_acc, non_null_acc\n",
    "\n",
    "# Calculates the loss for a forward pass for both null and non-null action predictions (this is to avoid overfitting to null actions)\n",
    "def loss(preds, targs):\n",
    "    # Find indexes of null frames and non-null frames\n",
    "    null_idxs = torch.where(targs.cpu() == 2)\n",
    "    non_null_idxs = torch.where(targs.cpu() < 2)\n",
    "    \n",
    "    # Add batch dimension and reorder into (batch_size, n_classes, seq_len)\n",
    "    null_preds = preds[null_idxs]\n",
    "    non_null_preds = preds[non_null_idxs]\n",
    "\n",
    "    null_loss = criterion(null_preds, targs[null_idxs])\n",
    "    non_null_loss = criterion(non_null_preds, targs[non_null_idxs])\n",
    "    \n",
    "    return null_loss, non_null_loss, len(non_null_idxs)/len(null_idxs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training and validation loop\n",
    "\n",
    "# Store the average loss after each epoch\n",
    "all_loss = {'train_null_loss':[],'train_non_null_loss':[], 'val_null_loss':[], 'val_non_null_loss':[]}\n",
    "all_acc = {'train_null_acc':[], 'train_non_null_acc':[], 'val_null_acc':[], 'val_non_null_acc':[]}\n",
    "\n",
    "print(\"starting\")\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    \n",
    "    \n",
    "    # Epoch stat trackers\n",
    "    null_accs = []\n",
    "    non_null_accs = []\n",
    "    null_losses = []\n",
    "    non_null_losses = []\n",
    "    for idx, batch in enumerate(iter(train_dataloader)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Inputs and Targets\n",
    "        instruction = batch['instructions']\n",
    "        frames = batch['frames']\n",
    "        actions = batch['actions']\n",
    "\n",
    "        # Frame Padding\n",
    "        # padding_mask = generate_pad_mask(batch=frames)\n",
    "        # pad_indexes = torch.argwhere(padding_mask == True).cpu()\n",
    "        # pad_indexes = pad_indexes[:,0] * MAX_FRAMES + pad_indexes[:,1]\n",
    "\n",
    "        # Get predictions\n",
    "        output =  model(frames) # model(instruction, frames, mask, padding_mask)\n",
    "        \n",
    "        # Get Loss for both null and non-null actions\n",
    "        null_loss, non_null_loss, scale = loss(output.reshape(-1,3), actions.type(torch.LongTensor).reshape(-1).to(device))\n",
    "        null_losses.append(null_loss.item())\n",
    "        non_null_losses.append(non_null_loss.item())\n",
    "        \n",
    "        if idx%30 == 0 and epoch == 0 :\n",
    "            total_loss = null_loss + non_null_loss\n",
    "        else:\n",
    "            total_loss = non_null_loss\n",
    "        # null_loss = 0\n",
    "        \n",
    "\n",
    "        # Track stats\n",
    "        _, predicted = torch.max(output.data, 2)\n",
    "        \n",
    "        predicted = predicted.reshape(-1).cpu()\n",
    "        # predicted = torch.Tensor(np.delete(predicted.numpy(), pad_indexes.numpy())) # drop the pads \n",
    "        \n",
    "        actions = actions.reshape(-1).cpu()\n",
    "        # actions = torch.Tensor(np.delete(actions.numpy(), pad_indexes.numpy())) # drop the pads \n",
    "        \n",
    "        null_acc, non_null_acc = correct(predicted, actions)\n",
    "        null_accs.append(null_acc.cpu())\n",
    "        non_null_accs.append(non_null_acc.cpu())\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + idx / len(train_dataloader))\n",
    "        \n",
    "    print('epoch: ', epoch)\n",
    "    print('avg train null acc: ', sum(null_accs)/len(null_accs))\n",
    "    print('avg train non-null acc: ', sum(non_null_accs)/len(non_null_accs))\n",
    "\n",
    "    all_loss['train_null_loss'].append(sum(null_losses)/len(null_losses))\n",
    "    all_loss['train_non_null_loss'].append(sum(non_null_losses)/len(non_null_losses))\n",
    "    all_acc['train_null_acc'].append(sum(null_accs)/len(null_accs))\n",
    "    all_acc['train_non_null_acc'].append(sum(non_null_accs)/len(non_null_accs))\n",
    "        \n",
    "        \n",
    "    # Validate on validation set\n",
    "    # print(torch.norm(model.mlp[-3].weight.grad))\n",
    "    # Turn off gradient calcs\n",
    "    val_null_accs = []\n",
    "    val_non_null_accs = []\n",
    "    val_null_losses = []\n",
    "    val_non_null_losses = []\n",
    "\n",
    "    for idx, batch in enumerate(iter(val_dataloader)):\n",
    "        model.eval()\n",
    "        # Inputs and Targets\n",
    "        instruction = batch['instructions']\n",
    "        frames = batch['frames']\n",
    "        actions = batch['actions']\n",
    "\n",
    "        # Frame Padding\n",
    "        # padding_mask = generate_pad_mask(batch=frames)\n",
    "        # pad_indexes = torch.argwhere(padding_mask == True).cpu()\n",
    "        # pad_indexes = pad_indexes[:,0] * MAX_FRAMES + pad_indexes[:,1]\n",
    "\n",
    "        # Get predictions\n",
    "        output = model(frames) # model(instruction, frames, mask, padding_mask)\n",
    "\n",
    "        # Get Losses\n",
    "        null_loss, non_null_loss, scale = loss(output.reshape(-1,3), actions.type(torch.LongTensor).reshape(-1).to(device))\n",
    "        val_null_losses.append(null_loss.item())\n",
    "        val_non_null_losses.append(non_null_loss.item())\n",
    "\n",
    "        # Track Stats\n",
    "        _, predicted = torch.max(output.data, 2)\n",
    "\n",
    "        predicted = predicted.reshape(-1).cpu()\n",
    "        # predicted = torch.Tensor(np.delete(predicted.numpy(), pad_indexes.numpy())) # drop the pads\n",
    "\n",
    "        actions = actions.reshape(-1).cpu()\n",
    "        # actions = torch.Tensor(np.delete(actions.numpy(), pad_indexes.numpy())) # drop the pads\n",
    "\n",
    "        null_acc, non_null_acc = correct(predicted, actions)\n",
    "        val_null_accs.append(null_acc.cpu())\n",
    "        val_non_null_accs.append(non_null_acc.cpu())\n",
    "\n",
    "\n",
    "    # Track loss and acc ever 5 epochs\n",
    "    all_loss['val_null_loss'].append(sum(val_null_losses)/len(val_null_losses))\n",
    "    all_loss['val_non_null_loss'].append(sum(val_non_null_losses)/len(val_non_null_losses))\n",
    "    all_acc['val_null_acc'].append(sum(val_null_accs)/len(val_null_accs))\n",
    "    all_acc['val_non_null_acc'].append(sum(val_non_null_accs)/len(val_non_null_accs))\n",
    "\n",
    "    print(\"avg val null acc %.2f\" % (sum(val_null_accs)/len(val_null_accs)))\n",
    "    print(\"avg val non-null acc %.2f\" % (sum(val_non_null_accs)/len(val_non_null_accs)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_dict(dict_arrays, use_xlabel='Epochs', use_ylabel='Value', use_title=None):\n",
    "    # Font size select custom or adjusted on `magnify` value.\n",
    "    font_size = np.interp(0.1, [0.1,1], [10.5,50])\n",
    "\n",
    "    # Font variables dictionary. Keep it in this format for future updates.\n",
    "    font_dict = dict(\n",
    "        family='DejaVu Sans',\n",
    "        color='black',\n",
    "        weight='normal',\n",
    "        size=font_size,\n",
    "        )\n",
    "\n",
    "    # Single plot figure.\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    # Use maximum length of steps. In case each arrya has different lengths.\n",
    "    max_steps = []\n",
    "\n",
    "    # Plot each array.\n",
    "    for index, (use_label, array) in enumerate(dict_arrays.items()):\n",
    "        # Set steps plotted on x-axis - we can use step if 1 unit has different value.\n",
    "        if 0 > 0:\n",
    "            # Offset all steps by start_step.\n",
    "            steps = np.array(range(0, len(array))) * 1 + 0\n",
    "            max_steps = steps if len(max_steps) < len(steps) else max_steps\n",
    "        else:\n",
    "            steps = np.array(range(1, len(array) + 1)) * 1\n",
    "            max_steps = steps if len(max_steps) < len(steps) else max_steps\n",
    "\n",
    "        # Plot array as a single line.\n",
    "        plt.plot(steps, array, linestyle=(['-'] * len(dict_arrays))[index], label=use_label)\n",
    "\n",
    "        # Plots points values.\n",
    "        if ([False] * len(dict_arrays))[index]:\n",
    "            # Loop through each point and plot the label.\n",
    "            for x, y in zip(steps, array):\n",
    "                # Add text label to plot.\n",
    "                plt.text(x, y, str(round(y, 3)), fontdict=font_dict)\n",
    "\n",
    "    # Set horizontal axis name.\n",
    "    plt.xlabel(use_xlabel, fontdict=font_dict)\n",
    "\n",
    "    # Use x ticks with steps or labels.\n",
    "    plt.xticks(max_steps, None, rotation=0)\n",
    "\n",
    "    # Set vertical axis name.\n",
    "    plt.ylabel(use_ylabel, fontdict=font_dict)\n",
    "\n",
    "    # Adjust both axis labels font size at same time.\n",
    "    plt.tick_params(labelsize=font_dict['size'])\n",
    "\n",
    "    # Place legend best position.\n",
    "    plt.legend(loc='best', fontsize=font_dict['size'])\n",
    "\n",
    "    # Adjust font for title.\n",
    "    font_dict['size'] *= 1.8\n",
    "\n",
    "    # Set title of figure.\n",
    "    plt.title(use_title, fontdict=font_dict)\n",
    "\n",
    "    # Rescale `magnify` to be used on inches.\n",
    "    magnify = 0.1\n",
    "    magnify *= 15\n",
    "\n",
    "    # Display grid depending on `use_grid`.\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Make figure nice.\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Get figure object from plot.\n",
    "    fig = plt.gcf()\n",
    "\n",
    "    # Get size of figure.\n",
    "    figsize = fig.get_size_inches()\n",
    "\n",
    "    # Change size depending on height and width variables.\n",
    "    figsize = [figsize[0] * 3 * magnify, figsize[1] * 1 * magnify]\n",
    "\n",
    "    # Set the new figure size with magnify.\n",
    "    fig.set_size_inches(figsize)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dict(all_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dict(all_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "val_epoch_correct_non_null = 0\n",
    "\n",
    "val_epoch_count_non_null = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_correct_null = 0\n",
    "    test_correct_non_null = 0\n",
    "    test_count_null = 0\n",
    "    test_count_non_null = 0\n",
    "\n",
    "    for idx, batch in enumerate(iter(val_dataloader)):\n",
    "        # Inputs and Targets\n",
    "        instruction = batch['instruction']\n",
    "        frames = batch['frames']\n",
    "        targets = batch['actions'][:,-1]\n",
    "        print(targets)\n",
    "\n",
    "        # Frame Padding\n",
    "        padding_mask = generate_pad_mask(batch=frames)\n",
    "        pad_indexes = np.argwhere(np.array(padding_mask.cpu()) == False)[:,1]\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = model(instruction, frames, mask, padding_mask)\n",
    "        print(predictions)\n",
    "        predictions = predictions[:,pad_indexes]\n",
    "\n",
    "        # Get Losses\n",
    "        non_null_loss = criterion(predictions, targets.long())\n",
    "        # null_loss, non_null_loss, scale = loss(predictions, targets)\n",
    "        # total_loss = non_null_loss #null_loss*scale + non_null_loss*(1/scale)\n",
    "        # # total_loss = criterion(predictions.permute(0, 2, 1), targets.long())\n",
    "\n",
    "        # Track Stats\n",
    "        val_epoch_correct_non_null += (predictions.argmax(dim=-1).cpu() == targets.cpu()).sum().item()\n",
    "        val_epoch_count_non_null += (predictions.argmax(dim=-1).cpu() == targets.cpu()).size(0)\n",
    "\n",
    "print(\"Test Non-Null Accuracy: \", round(val_epoch_correct_non_null/val_epoch_count_non_null,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlti kernel",
   "language": "python",
   "name": "mlti-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
