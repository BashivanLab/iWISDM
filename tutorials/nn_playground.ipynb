{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Modeling with MULTFS\n",
    "\n",
    "#### Overview:\n",
    "This notebook aims to provided an example of training a PyTorch Model on a premade task dataset\n",
    "- Text-embedder: ```all-mpnet-base-v2 (pretrained Sentence Transformer)```\n",
    "- Image-embedder: ```vit_b_16 (pretrained Vision Transformer)``` \n",
    "- Decoder/Classifier: Transformer Decoder only Model trained on both text and image embeddings to out put action class\n",
    "\n",
    "\n",
    "#### Datasets/Training:\n",
    "- In this notebook we use pre-saved and generated datasets\n",
    "    - *See the ..._dataset_gen.ipynb notebooks for how to generate and save a dataset*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import math\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from cognitive.task_bank import CompareLocTemporal\n",
    "from cognitive import task_generator as tg\n",
    "from cognitive import constants as const\n",
    "from cognitive import stim_generator as sg\n",
    "from cognitive import info_generator as ig\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = '../datasets/train'  # Training Dataset Directory\n",
    "VAL_DIR = '../datasets/val'  # Validation Dataset Directory\n",
    "TEST_DIR = '../datasets/test'  # Testing Dataset Directory\n",
    "BATCH_SIZE = 1\n",
    "MAX_FRAMES = 6  # the max possible frames across tasks\n",
    "VIT_OUT_DIM = 1000 # vision transformer output dimension\n",
    "LM_OUT_DIM = 768 # language model output dimension\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "- Read in the pregenerated task trials organized into frames, instructions, and correct actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trials(path):\n",
    "    frames = []\n",
    "    infos = []\n",
    "\n",
    "    for trial_fp in os.listdir(path):\n",
    "        if 'trial' not in trial_fp:\n",
    "            continue\n",
    "\n",
    "        trial_fp = os.path.join(path, trial_fp)\n",
    "        imgs = []\n",
    "        info = None\n",
    "        \n",
    "        for fp in os.listdir(trial_fp):\n",
    "            fp = os.path.join(trial_fp, fp)\n",
    "            \n",
    "            if fp[-4:] == '.png':\n",
    "                img = np.rollaxis(np.array(Image.open(fp), dtype=np.float32),2,0)\n",
    "                imgs.append(img)\n",
    "            elif 'trial_info' in fp:\n",
    "                info = json.load(open(fp))\n",
    "                infos.append(info)\n",
    "                \n",
    "        if len(imgs) > MAX_FRAMES:\n",
    "            raise Exception(trial_fp + \" contains more frames than the set maximum (MAX_FRAMES) !!!\")\n",
    "        elif len(imgs) != len(info['answers']):\n",
    "            raise Exception(trial_fp + \" numbers of frames does not match number of actions\")\n",
    "            \n",
    "        frames.append(np.array(imgs))\n",
    "\n",
    "    return frames, infos\n",
    "\n",
    "train_frames, train_infos = read_trials(TRAIN_DIR)\n",
    "val_frames, val_infos = read_trials(VAL_DIR)\n",
    "test_frames, test_infos = read_trials(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ins = [x['instruction'] for x in train_infos]\n",
    "train_raw_targets = [x['answers'] for x in train_infos]\n",
    "\n",
    "val_ins = [x['instruction'] for x in val_infos]\n",
    "val_raw_targets = [x['answers'] for x in val_infos]\n",
    "\n",
    "test_ins = [x['instruction'] for x in test_infos]\n",
    "test_raw_targets = [x['answers'] for x in test_infos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Target Actions\n",
    "- Encodes the target actions into one hot encoding vectors corresponding to the actions ```true, false, and null```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_map = {'true': 0, 'false': 1, 'null': 2}\n",
    "\n",
    "def map_actions(amap, raw_actions):\n",
    "    target_actions = []\n",
    "\n",
    "    for actions in raw_actions:\n",
    "        encoded = []\n",
    "        for action in actions:\n",
    "            encoded.append(amap[action])\n",
    "        target_actions.append(encoded)\n",
    "    \n",
    "    return target_actions\n",
    "\n",
    "train_targets = map_actions(action_map, train_raw_targets)\n",
    "val_targets = map_actions(action_map, val_raw_targets)\n",
    "test_targets = map_actions(action_map, test_raw_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionsDataset(Dataset):\n",
    "  \"\"\"\n",
    "    Pytorch Dataset class to load the Instructions Data\n",
    "\n",
    "    Data members:\n",
    "      instructions: list of instructions\n",
    "      n_ins: number of instructions in the dataset\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __len__: returns n_ins\n",
    "      __getitem__: returns an instruction\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, x):\n",
    "\n",
    "    self.instructions = x\n",
    "\n",
    "    self.n_ins = len(self.instructions)\n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "      Returns number of instructions in the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return self.n_ins\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "      Given an index return a instruction at that index\n",
    "    \"\"\"\n",
    "\n",
    "    return self.instructions[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionsCollator(object):\n",
    "  \"\"\"\n",
    "    Data Collator used for GPT2 in a classificaiton tasks\n",
    "\n",
    "    Args:\n",
    "      use_tokenizer :\n",
    "        Transformer type tokenizer used to process raw text into numbers.\n",
    "\n",
    "    Data members:\n",
    "      use_tokenizer: Tokenizer to be used inside the class.\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __call__: tokenize input\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "  def __init__(self, use_tokenizer):\n",
    "\n",
    "    self.use_tokenizer = use_tokenizer\n",
    "\n",
    "    return\n",
    "\n",
    "  def __call__(self, instructions):\n",
    "    \"\"\"\n",
    "        Tokenizes input\n",
    "    \"\"\"\n",
    "\n",
    "    # Call tokenizer\n",
    "    inputs = self.use_tokenizer(instructions, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained Language Model and Tokenizer \n",
    "lm_encoder = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "tokenizer = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Uncomment for offline load of lang embedder\n",
    "# lm_encoder = AutoModel.from_pretrained('offline_models/all-mpnet-base-v2')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('offline_models/all-mpnet-base-v2')\n",
    "\n",
    "# Create data collator to encode text and labels into numbers.\n",
    "InstructionsCollator = InstructionsCollator(use_tokenizer=tokenizer)\n",
    "\n",
    "# Create pytorch datasets for instructions\n",
    "ins_train_dataset = InstructionsDataset(train_ins)\n",
    "ins_val_dataset = InstructionsDataset(val_ins)\n",
    "ins_test_dataset = InstructionsDataset(test_ins)\n",
    "\n",
    "# Move pytorch datasets into dataloaders\n",
    "ins_train_dataloader = DataLoader(ins_train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=InstructionsCollator)\n",
    "ins_val_dataloader = DataLoader(ins_val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=InstructionsCollator)\n",
    "ins_test_dataloader = DataLoader(ins_test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=InstructionsCollator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frames Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FramesDataset(Dataset):\n",
    "  \"\"\"\n",
    "    Pytorch Dataset class to load the Frame Data\n",
    "\n",
    "    Data members:\n",
    "      frames``ist of frames\n",
    "      n_imgs: number of iamges in the dataset\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __len__: returns n_imgs\n",
    "      __getitem__: returns an frame\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, x):\n",
    "\n",
    "    self.frames = x\n",
    "\n",
    "    self.n_imgs = len(self.frames)\n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "      Returns number of frames in the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return self.n_imgs\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "      Given an index return a frame\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.tensor(self.frames[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained Vision Transformer\n",
    "vit_encoder = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.DEFAULT)\n",
    "\n",
    "# Uncommnent for offline load of vit\n",
    "# vit_encoder = torch.load('offline_models/vit_b_16/vit_b_16')\n",
    "\n",
    "# Create pytorch datasets for instructions\n",
    "frames_train_dataset = FramesDataset(train_frames)\n",
    "frames_val_dataset = FramesDataset(val_frames)\n",
    "frames_test_dataset = FramesDataset(test_frames)\n",
    "\n",
    "# Move pytorch datasets into dataloaders\n",
    "frames_train_dataloader = DataLoader(frames_train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "frames_val_dataloader = DataLoader(frames_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "frames_test_dataloader = DataLoader(frames_test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_embedder(instruction, encoder):\n",
    "    #Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        lm_output = encoder(**instruction)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(lm_output, instruction['attention_mask'])\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        \n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Embedder\n",
    "- We pad frames based on max possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_embedder(frames, encoder):\n",
    "    with torch.no_grad():\n",
    "        vit_out = encoder(torch.tensor(frames))\n",
    "\n",
    "    npads = MAX_FRAMES-len(vit_out)\n",
    "    pad = torch.ones((npads, vit_out.shape[1]))\n",
    "    vit_out = torch.cat((vit_out, pad))\n",
    "\n",
    "    return vit_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_92838/3025187430.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vit_out = encoder(torch.tensor(frames))\n"
     ]
    }
   ],
   "source": [
    "train_lm_embeddings = []\n",
    "train_img_embeddings = []\n",
    "\n",
    "val_lm_embeddings = []\n",
    "val_img_embeddings = []\n",
    "\n",
    "test_lm_embeddings = []\n",
    "test_img_embeddings = []\n",
    "\n",
    "for train_i in ins_train_dataloader:\n",
    "    train_f = train_f[0]\n",
    "\n",
    "    train_lm_embeddings.append(lm_embedder(train_i, lm_encoder))\n",
    "    train_img_embeddings.append(img_embedder(train_f, vit_encoder))\n",
    "    \n",
    "for val_i,val_f, test_i,test_f in zip(ins_val_dataloader,frames_val_dataloader, ins_test_dataloader,frames_test_dataloader):\n",
    "    val_f = val_f[0]\n",
    "    test_f = test_f[0]\n",
    "\n",
    "    val_lm_embeddings.append(lm_embedder(val_i, lm_encoder))\n",
    "    val_img_embeddings.append(img_embedder(val_f, vit_encoder))\n",
    "    \n",
    "    test_lm_embeddings.append(lm_embedder(test_i, lm_encoder))\n",
    "    test_img_embeddings.append(img_embedder(test_f, vit_encoder))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lm_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsDataset(Dataset):\n",
    "  \"\"\"\n",
    "    Pytorch Dataset class to load the embedded data\n",
    "\n",
    "    Data members:\n",
    "      lm_embeddings: list of language model embeddings\n",
    "      img_embeddings: list of language model embeddings\n",
    "      n_embs: number of embeddings in the dataset\n",
    "\n",
    "    Member functions:\n",
    "      __init__: ctor\n",
    "      __len__: returns n_ins\n",
    "      __getitem__: returns an instruction\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, lm_embeddings, img_embeddings, actions):\n",
    "\n",
    "    self.lm_embeddings = lm_embeddings\n",
    "    self.img_embeddings = img_embeddings\n",
    "    self.actions = actions\n",
    "\n",
    "    self.n_embs = len(self.lm_embeddings)\n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "      Returns number of instructions in the Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return self.n_embs\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "      Given an index return a instruction at that index\n",
    "    \"\"\"\n",
    "\n",
    "    return {'instruction':self.lm_embeddings[idx], 'frames':self.img_embeddings[idx], 'actions':torch.tensor(self.actions[idx], dtype=torch.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pytorch dataset for train, val, test data\n",
    "train_dataset = EmbeddingsDataset(train_lm_embeddings, train_img_embeddings, train_targets)\n",
    "val_dataset = EmbeddingsDataset(val_lm_embeddings, val_img_embeddings, val_targets)\n",
    "test_dataset  = EmbeddingsDataset(test_lm_embeddings, test_img_embeddings, test_targets)\n",
    "\n",
    "# Move pytorch datasets into dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import projections\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CausalMatchTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch based transformer decoder model\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize Model with Params\n",
    "    def __init__(self, nframes=MAX_FRAMES, blocks=3, nhead=5, emb_dim=VIT_OUT_DIM, classes=3, device=device):\n",
    "        super().__init__()\n",
    "\n",
    "        # Device\n",
    "        self.device = device\n",
    "\n",
    "        # Embedding Dimension\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        # Number of frames\n",
    "        self.nframes = nframes\n",
    "\n",
    "        # Frame Position Embedder Layer\n",
    "        self.pos_emb = nn.Parameter(torch.Tensor(nframes,emb_dim)).to(device)\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "           self.pos_emb,\n",
    "           gain=torch.nn.init.calculate_gain(\"linear\"))\n",
    "\n",
    "        # Instruction Dim Projection Layer\n",
    "        self.lm_linear_layer = nn.Linear(LM_OUT_DIM, emb_dim).to(device)\n",
    "\n",
    "        # Decoder Layers\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=nhead, batch_first=True).to(device)\n",
    "        self.decoder_layers = _get_clones(self.decoder_layer, blocks)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layers, num_layers=blocks).to(device)\n",
    "\n",
    "        # Action classifier\n",
    "        self.classifier = nn.Linear(emb_dim, classes).to(device)\n",
    "\n",
    "    # Function for forward pass\n",
    "    def forward(self, instruction, frames, mask, padding_mask):\n",
    "\n",
    "        # Project instruction embedding\n",
    "        instruction = self.lm_linear_layer(instruction)\n",
    "\n",
    "        # Add the frame position embedding\n",
    "        for i in range(len(frames)):\n",
    "            frames[i] += self.pos_emb[i,:]\n",
    "\n",
    "        # Apply each Decoder Layer (block)\n",
    "        for layer in self.decoder_layers:\n",
    "            frames = layer(frames, instruction, tgt_mask=mask, tgt_key_padding_mask=padding_mask, ) \n",
    "\n",
    "        # Pass through linear layer for classification\n",
    "        output = self.classifier(frames)\n",
    "\n",
    "        return output\n",
    " \n",
    "# Creates a list of torch duplicate torch modules\n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "# Creates a square Sequential/Causal mask of size sz\n",
    "def generate_causal_mask(sz: int) -> Tensor:\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "# Generates a padding masks for each sequence in a batch\n",
    "def generate_pad_mask(batch):\n",
    "\n",
    "    pad_tensor = torch.ones((batch.shape[2])).to(device)\n",
    "\n",
    "    mask = np.zeros((batch.shape[0],batch.shape[1]))\n",
    "\n",
    "    for s in range(0, batch.shape[0]):\n",
    "        for v in range(0, batch[s].shape[0]):\n",
    "            new_s = torch.all(batch[s][v] == pad_tensor)\n",
    "            mask[s][v] = new_s\n",
    "\n",
    "    return torch.tensor(mask).bool().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CausalMatchTransformer(nframes=MAX_FRAMES,\n",
    "                               blocks=3,\n",
    "                               nhead=8,\n",
    "                               emb_dim=VIT_OUT_DIM,\n",
    "                               classes=len(action_map.keys()),\n",
    "                               device=device).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configurations\n",
    "epochs = 15\n",
    "\n",
    "# weights = [2, 2, 1]\n",
    "# class_weights = torch.FloatTensor(weights)\n",
    "# criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='sum')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 1e-5\n",
    "optimizer = torch.optim.Adam(\n",
    "    (p for p in model.parameters() if p.requires_grad), lr=lr\n",
    ")\n",
    "\n",
    "mask = generate_causal_mask(MAX_FRAMES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the number of correct null action predictions and the number of correct non-null action predictions\n",
    "def correct(preds, targs):\n",
    "    c_null = (preds == targs).sum().item()\n",
    "    c_non_null = 0\n",
    "    \n",
    "    # Get indexs of non-null target actions\n",
    "    idxs = np.where(targs < 2)\n",
    "    \n",
    "    # Count correct and totals\n",
    "    c_non_null = (preds[idxs] == targs[idxs]).sum().item()\n",
    "    n_non_null = (preds[idxs] == targs[idxs]).size(0)\n",
    "    c_null -= c_non_null\n",
    "    n_null = (preds == targs).size(1) - n_non_null\n",
    "    \n",
    "    return c_null,n_null, c_non_null,n_non_null\n",
    "\n",
    "# Calculates the loss for a forward pass for both null and non-null action predictions (this is to avoid overfitting to null actions)\n",
    "def loss(preds, targs):\n",
    "    null_idxs = np.where(targs == 2)\n",
    "    non_null_idxs = np.where(targs < 2)\n",
    "    \n",
    "    # We use a permute to get the correct predictions shape (batch_size, n_classes, seq_len)\n",
    "    null_loss = criterion(preds[null_idxs].unsqueeze(0).permute(0, 2, 1), targs[null_idxs].unsqueeze(0).long())\n",
    "    non_null_loss = criterion(preds[non_null_idxs].unsqueeze(0).permute(0, 2, 1), targs[non_null_idxs].unsqueeze(0).long())\n",
    "    \n",
    "    return null_loss, non_null_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "epoch=0\n",
      "epoch=1\n",
      "{'train_null_acc': [0.4078398665554629], 'train_non_null_acc': [0.48875], 'val_null_acc': [0.3824561403508772], 'val_non_null_acc': [0.53]}\n",
      "epoch=2\n",
      "epoch=3\n",
      "{'train_null_acc': [0.4078398665554629, 0.4132610508757298], 'train_non_null_acc': [0.48875, 0.44375], 'val_null_acc': [0.3824561403508772, 0.3508771929824561], 'val_non_null_acc': [0.53, 0.49]}\n",
      "epoch=4\n",
      "epoch=5\n",
      "{'train_null_acc': [0.4078398665554629, 0.4132610508757298, 0.37447873227689743], 'train_non_null_acc': [0.48875, 0.44375, 0.48125], 'val_null_acc': [0.3824561403508772, 0.3508771929824561, 0.49122807017543857], 'val_non_null_acc': [0.53, 0.49, 0.46]}\n",
      "epoch=6\n",
      "epoch=7\n",
      "{'train_null_acc': [0.4078398665554629, 0.4132610508757298, 0.37447873227689743, 0.3686405337781485], 'train_non_null_acc': [0.48875, 0.44375, 0.48125, 0.46125], 'val_null_acc': [0.3824561403508772, 0.3508771929824561, 0.49122807017543857, 0.4456140350877193], 'val_non_null_acc': [0.53, 0.49, 0.46, 0.43]}\n",
      "epoch=8\n",
      "epoch=9\n",
      "{'train_null_acc': [0.4078398665554629, 0.4132610508757298, 0.37447873227689743, 0.3686405337781485, 0.371976647206005], 'train_non_null_acc': [0.48875, 0.44375, 0.48125, 0.46125, 0.48125], 'val_null_acc': [0.3824561403508772, 0.3508771929824561, 0.49122807017543857, 0.4456140350877193, 0.3719298245614035], 'val_non_null_acc': [0.53, 0.49, 0.46, 0.43, 0.42]}\n",
      "epoch=10\n",
      "epoch=11\n",
      "{'train_null_acc': [0.4078398665554629, 0.4132610508757298, 0.37447873227689743, 0.3686405337781485, 0.371976647206005, 0.3623853211009174], 'train_non_null_acc': [0.48875, 0.44375, 0.48125, 0.46125, 0.48125, 0.49375], 'val_null_acc': [0.3824561403508772, 0.3508771929824561, 0.49122807017543857, 0.4456140350877193, 0.3719298245614035, 0.5614035087719298], 'val_non_null_acc': [0.53, 0.49, 0.46, 0.43, 0.42, 0.46]}\n",
      "epoch=12\n",
      "epoch=13\n",
      "{'train_null_acc': [0.4078398665554629, 0.4132610508757298, 0.37447873227689743, 0.3686405337781485, 0.371976647206005, 0.3623853211009174, 0.3519599666388657], 'train_non_null_acc': [0.48875, 0.44375, 0.48125, 0.46125, 0.48125, 0.49375, 0.49875], 'val_null_acc': [0.3824561403508772, 0.3508771929824561, 0.49122807017543857, 0.4456140350877193, 0.3719298245614035, 0.5614035087719298, 0.35789473684210527], 'val_non_null_acc': [0.53, 0.49, 0.46, 0.43, 0.42, 0.46, 0.45]}\n",
      "epoch=14\n"
     ]
    }
   ],
   "source": [
    "# Training and validation loop\n",
    "\n",
    "# Store the average loss after each epoch\n",
    "all_loss = {'train_loss':[], 'val_loss':[]}\n",
    "all_acc = {'train_null_acc':[], 'train_non_null_acc':[], 'val_null_acc':[], 'val_non_null_acc':[]}\n",
    "\n",
    "print(\"starting\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch={epoch}\")\n",
    "\n",
    "    # Epoch stat trackers\n",
    "    epoch_loss = 0\n",
    "    epoch_correct_null = 0\n",
    "    epoch_correct_non_null = 0\n",
    "    epoch_count_null = 0\n",
    "    epoch_count_non_null = 0\n",
    "    epoch_count = 0\n",
    "    for idx, batch in enumerate(iter(train_dataloader)):\n",
    "\n",
    "        # Inputs and Targets\n",
    "        instruction = batch['instruction']\n",
    "        frames = batch['frames']\n",
    "        targets = batch['actions']\n",
    "        \n",
    "        # Frame Padding\n",
    "        padding_mask = generate_pad_mask(batch=frames)\n",
    "        pad_indexes = np.argwhere(np.array(padding_mask) == False)[:,1]\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = model(instruction, frames, mask, padding_mask)\n",
    "        predictions = predictions[:,pad_indexes]\n",
    "        \n",
    "        # Get Loss for both null and non-null actions\n",
    "        # null_loss, non_null_loss = loss(predictions, targets)\n",
    "        # total_loss = null_loss + non_null_loss\n",
    "        total_loss = criterion(predictions.permute(0, 2, 1), targets.long())\n",
    "\n",
    "        # Track stats\n",
    "        correct_counts = correct(predictions.argmax(dim=-1), targets)\n",
    "        epoch_correct_null += correct_counts[0]\n",
    "        epoch_count_null += correct_counts[1]\n",
    "        epoch_correct_non_null += correct_counts[2]\n",
    "        epoch_count_non_null += correct_counts[3]\n",
    "        \n",
    "        epoch_loss += total_loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Validate on validation set every 5 epochs\n",
    "    if (epoch+1) % 2 == 0 or epoch == epochs:\n",
    "        # Turn off gradient calcs\n",
    "        with torch.no_grad():\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_correct_null = 0\n",
    "            val_epoch_correct_non_null = 0\n",
    "            val_epoch_count_null = 0\n",
    "            val_epoch_count_non_null = 0\n",
    "\n",
    "            for idx, batch in enumerate(iter(val_dataloader)):\n",
    "                # Inputs and Targets\n",
    "                instruction = batch['instruction']\n",
    "                frames = batch['frames']\n",
    "                targets = batch['actions']\n",
    "\n",
    "                # Frame Padding\n",
    "                padding_mask = generate_pad_mask(batch=frames)\n",
    "                pad_indexes = np.argwhere(np.array(padding_mask) == False)[:,1]\n",
    "            \n",
    "                # Get predictions\n",
    "                predictions = model(instruction, frames, mask, padding_mask)\n",
    "                predictions = predictions[:,pad_indexes]\n",
    "\n",
    "                # Get Losses\n",
    "                # null_loss, non_null_loss = loss(predictions, targets)\n",
    "                # total_loss = null_loss + non_null_loss\n",
    "                total_loss = criterion(predictions.permute(0, 2, 1), targets.long())\n",
    "                \n",
    "                # Track Stats\n",
    "                val_correct_counts = correct(predictions.argmax(dim=-1), targets)\n",
    "                val_epoch_correct_null += val_correct_counts[0]\n",
    "                val_epoch_count_null += val_correct_counts[1]\n",
    "                val_epoch_correct_non_null += val_correct_counts[2]\n",
    "                val_epoch_count_non_null += val_correct_counts[3]\n",
    "\n",
    "                val_epoch_loss += total_loss.item()\n",
    "\n",
    "        # Track loss and acc ever 5 epochs\n",
    "        avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "        avg_val_loss = val_epoch_loss / len(val_dataloader)\n",
    "\n",
    "        all_loss['val_loss'].append(avg_val_loss)\n",
    "        all_acc['val_null_acc'].append(val_epoch_correct_null / val_epoch_count_null)\n",
    "        all_acc['val_non_null_acc'].append(val_epoch_correct_non_null / val_epoch_count_non_null)\n",
    "        \n",
    "        all_loss['train_loss'].append(avg_train_loss)\n",
    "        all_acc['train_null_acc'].append(epoch_correct_null / epoch_count_null)\n",
    "        all_acc['train_non_null_acc'].append(epoch_correct_non_null / epoch_count_non_null)\n",
    "\n",
    "        print(all_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# p = torch.tensor([[[ 0.0548,  1.0788,  0.4486],\n",
    "#                    [-0.1559,  1.4204,  0.5418],\n",
    "#                    [ 0.1171,  1.3842, -0.5025]]])\n",
    "# t = torch.tensor([[2., 2., 1.]])\n",
    "\n",
    "# loss(p,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_null_acc': [0.4078398665554629,\n",
       "  0.4132610508757298,\n",
       "  0.37447873227689743,\n",
       "  0.3686405337781485,\n",
       "  0.371976647206005,\n",
       "  0.3623853211009174,\n",
       "  0.3519599666388657],\n",
       " 'train_non_null_acc': [0.48875,\n",
       "  0.44375,\n",
       "  0.48125,\n",
       "  0.46125,\n",
       "  0.48125,\n",
       "  0.49375,\n",
       "  0.49875],\n",
       " 'val_null_acc': [0.3824561403508772,\n",
       "  0.3508771929824561,\n",
       "  0.49122807017543857,\n",
       "  0.4456140350877193,\n",
       "  0.3719298245614035,\n",
       "  0.5614035087719298,\n",
       "  0.35789473684210527],\n",
       " 'val_non_null_acc': [0.53, 0.49, 0.46, 0.43, 0.42, 0.46, 0.45]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Null Accuracy:  0.339\n",
      "Test Non-Null Accuracy:  0.57\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_correct_null = 0\n",
    "    test_correct_non_null = 0\n",
    "    test_count_null = 0\n",
    "    test_count_non_null = 0\n",
    "\n",
    "    for idx, batch in enumerate(iter(test_dataloader)):\n",
    "        # Inputs and Targets\n",
    "        instruction = batch['instruction']\n",
    "        frames = batch['frames']\n",
    "        targets = batch['actions']\n",
    "\n",
    "        # Frame Padding\n",
    "        padding_mask = generate_pad_mask(batch=frames)\n",
    "        pad_indexes = np.argwhere(np.array(padding_mask) == False)[:,1]\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = model(instruction, frames, mask, padding_mask)\n",
    "        predictions = predictions[:,pad_indexes]\n",
    "\n",
    "        test_correct_counts = correct(predictions.argmax(dim=-1), targets)\n",
    "\n",
    "        test_correct_null += test_correct_counts[0]\n",
    "        test_count_null += test_correct_counts[1]\n",
    "        test_correct_non_null += test_correct_counts[2]\n",
    "        test_count_non_null += test_correct_counts[3]\n",
    "\n",
    "print(\"Test Null Accuracy: \", round(test_correct_null/test_count_null,4))\n",
    "print(\"Test Non-Null Accuracy: \", round(test_correct_non_null/test_count_non_null,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi",
   "language": "python",
   "name": "multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
