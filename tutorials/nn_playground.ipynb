{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Modeling with iWISDM\n",
    "### Overview:\n",
    "\n",
    "This notebook aims to provided an example of training a PyTorch Model (transformer) on a premade task dataset\n",
    "- Text-embedder: all-mpnet-base-v2 (pretrained Sentence Transformer)\n",
    "- Image-embedder:  EfficientNetV2 (pretrained)\n",
    "- Decoder/Classifier: Transformer Decoder only Model trained on both text and image embeddings to out put action class\n",
    "\n",
    "Datasets/Training:\n",
    "- See the task_dataset_gen.ipynb notebooks for how to generate and save a dataset for training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For Instruction Encoding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# For Image Encoding\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data\n",
    "TRAIN_PATH = \"outputs/trials/train/\"\n",
    "VAL_PATH = \"outputs/trials/validation/\"\n",
    "MAX_FRAMES = 3\n",
    "ACTIONS = {\"null\": 0, \"true\": 1, \"false\": 2}\n",
    "\n",
    "# Instruction Encoder\n",
    "INS_ENCODER_DIM = 768\n",
    "\n",
    "# Image Encoder\n",
    "IMG_ENCODER_DIM = 2048\n",
    "\n",
    "# Decoder\n",
    "DECODER_HIDDEN = 64\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fbc7021aed0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = T.Compose([\n",
    "            T.Resize(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "img_encoder = torchvision.models.efficientnet_v2_s(weights=torchvision.models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "img_encoder = img_encoder.eval().to(device)\n",
    "\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# # Set hook and freeze layers\n",
    "for paras in img_encoder.parameters():\n",
    "    paras.requires_grad = False\n",
    "# img_encoder.features[8][2].register_forward_hook(get_activation('activation'))\n",
    "img_encoder.avgpool.register_forward_hook(get_activation('activation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 trials\n",
      "Processed 100 trials\n",
      "Processed 200 trials\n",
      "Processed 300 trials\n",
      "Processed 400 trials\n",
      "Processed 500 trials\n",
      "Processed 600 trials\n",
      "Processed 700 trials\n",
      "Processed 800 trials\n",
      "Processed 900 trials\n",
      "Processed 1000 trials\n",
      "Processed 1100 trials\n",
      "Processed 1200 trials\n",
      "Processed 1300 trials\n",
      "Processed 1400 trials\n",
      "Processed 1500 trials\n",
      "Processed 1600 trials\n",
      "Processed 1700 trials\n",
      "Processed 1800 trials\n",
      "Processed 1900 trials\n",
      "Processed 2000 trials\n",
      "Processed 2100 trials\n",
      "Processed 2200 trials\n",
      "Processed 2300 trials\n",
      "Processed 2400 trials\n",
      "Processed 2500 trials\n",
      "Processed 2600 trials\n",
      "Processed 2700 trials\n",
      "Processed 2800 trials\n",
      "Processed 2900 trials\n",
      "Processed 3000 trials\n",
      "Processed 3100 trials\n",
      "Processed 3200 trials\n",
      "Processed 3300 trials\n",
      "Processed 3400 trials\n",
      "Processed 3500 trials\n",
      "Processed 3600 trials\n",
      "Processed 3700 trials\n",
      "Processed 3800 trials\n",
      "Processed 3900 trials\n",
      "Processed 4000 trials\n",
      "Processed 4100 trials\n",
      "Processed 4200 trials\n",
      "Processed 4300 trials\n",
      "Processed 4400 trials\n",
      "Processed 4500 trials\n",
      "Processed 4600 trials\n",
      "Processed 4700 trials\n",
      "Processed 4800 trials\n",
      "Processed 4900 trials\n",
      "Processed 5000 trials\n",
      "Processed 5100 trials\n",
      "Processed 5200 trials\n",
      "Processed 5300 trials\n",
      "Processed 5400 trials\n",
      "Processed 5500 trials\n",
      "Processed 5600 trials\n",
      "Processed 5700 trials\n",
      "Processed 5800 trials\n",
      "Processed 5900 trials\n",
      "Processed 6000 trials\n",
      "Processed 6100 trials\n",
      "Processed 6200 trials\n",
      "Processed 6300 trials\n",
      "Processed 6400 trials\n",
      "Processed 6500 trials\n",
      "Processed 6600 trials\n",
      "Processed 6700 trials\n",
      "Processed 6800 trials\n",
      "Processed 6900 trials\n",
      "Processed 7000 trials\n",
      "Processed 7100 trials\n",
      "Processed 7200 trials\n",
      "Processed 7300 trials\n",
      "Processed 7400 trials\n",
      "Processed 7500 trials\n",
      "Processed 7600 trials\n",
      "Processed 7700 trials\n",
      "Processed 7800 trials\n",
      "Processed 7900 trials\n",
      "Processed 8000 trials\n",
      "Processed 8100 trials\n",
      "Processed 8200 trials\n",
      "Processed 8300 trials\n",
      "Processed 8400 trials\n",
      "Processed 8500 trials\n",
      "Processed 8600 trials\n",
      "Processed 8700 trials\n",
      "Processed 8800 trials\n",
      "Processed 8900 trials\n",
      "Processed 9000 trials\n",
      "Processed 9100 trials\n",
      "Processed 9200 trials\n",
      "Processed 9300 trials\n",
      "Processed 9400 trials\n",
      "Processed 9500 trials\n",
      "Processed 9600 trials\n",
      "Processed 9700 trials\n",
      "Processed 9800 trials\n",
      "Processed 9900 trials\n",
      "Processed 0 trials\n",
      "Processed 100 trials\n",
      "Processed 200 trials\n",
      "Processed 300 trials\n",
      "Processed 400 trials\n",
      "Processed 500 trials\n",
      "Processed 600 trials\n",
      "Processed 700 trials\n",
      "Processed 800 trials\n",
      "Processed 900 trials\n",
      "Processed 1000 trials\n",
      "Processed 1100 trials\n",
      "Processed 1200 trials\n",
      "Processed 1300 trials\n",
      "Processed 1400 trials\n",
      "Processed 1500 trials\n",
      "Processed 1600 trials\n",
      "Processed 1700 trials\n",
      "Processed 1800 trials\n",
      "Processed 1900 trials\n"
     ]
    }
   ],
   "source": [
    "def read_trials(path):\n",
    "    embeddings = torch.tensor([]).to(device)\n",
    "    infos = []\n",
    "\n",
    "    for i, trial_fp in enumerate(os.listdir(path)):\n",
    "        if 'trial' not in trial_fp:\n",
    "            continue\n",
    "\n",
    "        trial_fp = os.path.join(path, trial_fp)\n",
    "        imgs = torch.tensor([])\n",
    "        info = None\n",
    "        \n",
    "        for fp in os.listdir(trial_fp):\n",
    "            fp = os.path.join(trial_fp, fp)\n",
    "            \n",
    "            if fp[-4:] == '.png':\n",
    "                img = Image.open(fp)\n",
    "                img = transform(img)\n",
    "                imgs = torch.cat((imgs, img.unsqueeze(0)), 0)\n",
    "            elif 'trial_info' in fp:\n",
    "                info = json.load(open(fp))\n",
    "                infos.append(info)\n",
    "\n",
    "        if imgs.shape[0] > MAX_FRAMES:\n",
    "            raise Exception(trial_fp + \" contains more frames than the set maximum (MAX_FRAMES) !!!\")\n",
    "        elif imgs.shape[0] != len(info['answers']):\n",
    "            raise Exception(trial_fp + \" numbers of frames does not match number of actions\")\n",
    "        \n",
    "        # Encode images\n",
    "        img_encoder(imgs.to(device))\n",
    "        img_embs = activation['activation']\n",
    "        embeddings = torch.cat((embeddings, img_embs), 0)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i} trials\")\n",
    "        \n",
    "    return embeddings, infos\n",
    "\n",
    "train_frame_embs, train_infos = read_trials(TRAIN_PATH)\n",
    "val_frame_embs, val_infos = read_trials(VAL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_actions(infos):\n",
    "    actions = []\n",
    "    for info in train_infos:\n",
    "        # Map actions to integers\n",
    "        actions.append([ACTIONS[a] for a in info['answers']])\n",
    "\n",
    "    return torch.tensor(actions).to(device)\n",
    "\n",
    "train_actions = map_actions(train_infos)\n",
    "val_actions = map_actions(val_infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iWISDM_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "        desc: Torch Dataset inherited class which dynamically generates task samples\n",
    "\n",
    "        args:\n",
    "            - image embeddings (Tensor): image embeddings\n",
    "            - instructions (list): list of instructions\n",
    "            - answers (list): list of answers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_embs: Tensor, instructions: list, answers: list):\n",
    "        self.image_embs = image_embs\n",
    "        self.instructions = instructions\n",
    "        self.answers = answers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instructions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_embs[idx], self.instructions[idx], self.answers[idx]  \n",
    "\n",
    "train_dataset = iWISDM_Dataset(train_frame_embs, [info['instruction'] for info in train_infos], train_actions)\n",
    "val_dataset = iWISDM_Dataset(val_frame_embs, [info['instruction'] for info in val_infos], val_actions)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc04b79e5c364a25b8628c9e6b7d8add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75849cb7d0144e33a830afd7b289b592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242bb7502596444d8f6e4a8c35ce514e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6c9865d7464318a69becb17496ef8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc3fd04b8e1465c92dfa44ab4dcc11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f1549b4fd5423c88d1a0bcdc728b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3acd4f6b9c746e3a4641afb5c744c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61add1673d640e98f7a164d6b84aba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81eec41453604187abaca686864ac9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166b8a723fdd4a6e97200701ac4af288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d9bb788fd14ee3bf59920adee0376d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ins_encoder = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position Encoder + Maskers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a square Sequential/Causal mask of size sz\n",
    "def generate_causal_mask(sz: int, device:str) -> Tensor:\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(device)\n",
    "\n",
    "# Generates a padding masks for each sequence in a batch\n",
    "def generate_pad_mask(batch, pad, device):\n",
    "\n",
    "    pad_tensor = pad.to(device)\n",
    "\n",
    "    mask = np.zeros((batch.shape[0],batch.shape[1]))\n",
    "\n",
    "    for s in range(0, batch.shape[0]):\n",
    "        for v in range(0, batch[s].shape[0]):\n",
    "            new_s = torch.all(batch[s][v] == pad_tensor)\n",
    "            mask[s][v] = new_s\n",
    "\n",
    "    return torch.tensor(mask).bool().to(device)\n",
    "\n",
    "# Sinusoidal Positional Encoding Module\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding module taken from PyTorch Tutorial\n",
    "    # Link: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "        desc: Torch nn.Module inherited Transformer Decoder Only class\n",
    "        args:\n",
    "            - hidden_dim (int): dimension of hidden layers\n",
    "            - device : torch device\n",
    "            - img_hidden (int): dimension of the image encoder's layer\n",
    "            - dim_transformer_ffl (int): dimension of the decoder's feedfoward layers\n",
    "            - nhead (int): number of attention heads per decoder block\n",
    "            - blocks (int): number of decoder blocks to stack\n",
    "            - output_dim (int): number of classes\n",
    "            - max_frames (int): sequence length\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, device, img_hidden=1280, dim_transformer_ffl=2048, nhead = 16, blocks=2, max_frames=6, output_dim = 3, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.max_frames = max_frames\n",
    "    \n",
    "        # Convolutional layer \n",
    "        self.cnnlayer = torch.nn.Conv3d(img_hidden, hidden_dim, 1)\n",
    "\n",
    "\n",
    "        # Dimensions\n",
    "        self.input_dim = hidden_dim*7*7\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Linear layers (instructions, in, & out)\n",
    "        self.ins_hidden = nn.Linear(768, hidden_dim)\n",
    "        self.in2hidden = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.hidden2output = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "        # Input layer norm\n",
    "        self.layer_norm_in = nn.LayerNorm(self.hidden_dim)\n",
    "        \n",
    "        # Position Encoding class\n",
    "        self.pos_emb = PositionalEncoding(hidden_dim, dropout, self.max_frames)\n",
    "\n",
    "        # Encoder only transformer \n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, dim_feedforward=dim_transformer_ffl, nhead=nhead, batch_first=False, dropout=dropout)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=blocks)\n",
    "\n",
    "    # One forward pass through the model\n",
    "    def forward(self, frames, instructions=None):\n",
    "        # Get batch size and sequence length\n",
    "        self.batch_size = frames.shape[0]\n",
    "        self.seq_len = frames.shape[1]\n",
    "        \n",
    "        # Generate masks\n",
    "        causal_mask = generate_causal_mask(self.seq_len, self.device)\n",
    "        pad = torch.ones((frames.shape[2],frames.shape[3], frames.shape[4]))\n",
    "        padding_mask = generate_pad_mask(frames, pad, self.device)\n",
    "\n",
    "        # Swap axes to get (seq_len, batchsize, nc, w, h)\n",
    "        x = torch.swapaxes(frames, 0, 1).float() \n",
    "\n",
    "        # Get activations from each frame\n",
    "        x_acts = torch.tensor([]).to(self.device)\n",
    "        for i in range(x.shape[0]):\n",
    "            temp = self.cnnlayer(x[i, :, :, :, :])\n",
    "            x_acts = torch.cat((x_acts, temp.unsqueeze(0)), dim=0)\n",
    "\n",
    "        # Flatten nc, w, h into one dim\n",
    "        x_acts = x_acts.reshape(x_acts.shape[0], x_acts.shape[1], -1)\n",
    "\n",
    "        # Pass through input linear layer and prepend instructions\n",
    "        ins_embeddings = self.ins_hidden(instructions).unsqueeze(0)\n",
    "        hidden_x = self.in2hidden(x_acts.float())\n",
    "\n",
    "        # Add positional encoding and normalize\n",
    "        hidden_x = self.layer_norm_in(self.pos_emb(hidden_x))\n",
    "\n",
    "        # Pass through transformer\n",
    "        decoder_output = self.decoder(hidden_x, ins_embeddings, tgt_mask=causal_mask, tgt_key_padding_mask=padding_mask)\n",
    "\n",
    "        # Pass through output linear layer\n",
    "        out = self.hidden2output(decoder_output)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFDecoder(hidden_dim=DECODER_HIDDEN, img_hidden=IMG_ENCODER_DIM, device=device, max_frames=MAX_FRAMES, output_dim=len(ACTIONS)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bashlab_cogenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
