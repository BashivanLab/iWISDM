{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f149fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torchvision.models import resnet50\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b1c545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Request GPU device 0\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    # If no GPU is available, fall back to CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c5ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskDataset(Dataset):\n",
    "    # todo: this is not the most efficient way to access data, since each time it has to read from the directory \n",
    "    def __init__(self, root_dir,):\n",
    "        self.root_dir = root_dir\n",
    "        # preprocessing steps for pretrained ResNet models\n",
    "        self.transform = transforms.Compose([\n",
    "                            transforms.Resize(224),\n",
    "                            transforms.CenterCrop(224), # todo: to delete for shapenet task; why?\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                        ])\n",
    "\n",
    "        # check the size of the dataset\n",
    "        self.dataset_size = 0\n",
    "        items = os.listdir(self.root_dir)\n",
    "        for item in items:\n",
    "            item_path = os.path.join(self.root_dir, item)\n",
    "            # Check if the item is a directory\n",
    "            if os.path.isdir(item_path):\n",
    "                self.dataset_size += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trial_path = os.path.join(self.root_dir, \"trial%d\"%idx)\n",
    "        # todo: only consider 2 frames in this case => need to make it generalizable\n",
    "        images = []\n",
    "        for i in range(2):\n",
    "            image = Image.open(os.path.join(trial_path, \"epoch%d.png\"%i))\n",
    "            image = self.transform(image)\n",
    "            \n",
    "            images.append(image)\n",
    "        images = np.stack(images) # (2*3*224*224)\n",
    "        with open(os.path.join(trial_path, \"trial_info\"), 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            # 'data' now contains the contents of the JSON file as a Python dictionary\n",
    "            actions = self._action_map(data[\"answers\"])[-1]\n",
    "\n",
    "        return images, torch.tensor(actions)\n",
    "    \n",
    "    def _action_map(self, actions):\n",
    "        updated_actions = []\n",
    "        for action in actions:\n",
    "            if action == \"null\":\n",
    "                updated_actions.append(2)\n",
    "            elif action == \"false\":\n",
    "                updated_actions.append(0)\n",
    "            elif action == \"true\":\n",
    "                updated_actions.append(1)\n",
    "        return updated_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "275fb447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network\n",
    "IMGM_PATH = 'tutorials/offline_models/resnet/resnet'\n",
    "\n",
    "\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "\n",
    "class CNNRNNNet(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, output_size = 3,):\n",
    "        super().__init__()\n",
    "\n",
    "        # set up the CNN model\n",
    "        self.cnnmodel = torch.load(IMGM_PATH, map_location=device)\n",
    "        # freeze layers of cnn model\n",
    "        for paras in self.cnnmodel.parameters():\n",
    "            paras.requires_grad = False\n",
    "        # get relu activation of last block of resnet50\n",
    "        \n",
    "        self.cnnmodel.layer4[2].relu.register_forward_hook(get_activation('relu'))\n",
    "\n",
    "        self.cnnlayer = torch.nn.Conv2d(2048, hidden_size, 1) # we can also bring the resnet embedding dim to a number different from hidden size\n",
    "\n",
    "        self.input_size = hidden_size*7*7\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.in2hidden = nn.Linear(self.input_size, hidden_size)\n",
    "        self.layer_norm_in = nn.LayerNorm(self.hidden_size)\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = self.hidden_size, \n",
    "            hidden_size = self.hidden_size,\n",
    "            nonlinearity = \"relu\", # guarnatee positive activations\n",
    "            )\n",
    "\n",
    "        self.layer_norm_rnn = nn.LayerNorm(self.hidden_size)\n",
    "        self.hidden2output = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, input_img, hidden_state = None, is_noise = False,):\n",
    "        # preprocess image with resnet\n",
    "        self.batch_size = input_img.shape[0]\n",
    "        self.seq_len = input_img.shape[1]\n",
    "        \n",
    "        x = torch.swapaxes(input_img, 0, 1).float()# (seq_len, batchsize, nc, w, h)\n",
    "        \n",
    "        x_acts = []\n",
    "        cnn_acts = []\n",
    "        for i in range(self.seq_len):\n",
    "            temp = self.cnnmodel(x[i,:,:,:,:])\n",
    "            cnn_acts.append(activation[\"relu\"])\n",
    "            x_act = self.cnnlayer(activation[\"relu\"])\n",
    "            x_acts.append(x_act) # (batchsize, nc, w, h) = (batchsize, 2048, 7,7)\n",
    "        \n",
    "        x_acts = torch.stack(x_acts, axis = 0) # (seqlen, batchsize,nc, w,h)\n",
    "        self.cnn_acts = torch.stack(cnn_acts, axis = 0) # (seqlen, batchsize, nc, w,h)\n",
    "        self.cnn_acts_down = x_acts\n",
    "        \n",
    "        x_acts = x_acts.reshape(x_acts.shape[0], self.batch_size, -1)\n",
    "        \n",
    "        # if hidden_state == None:\n",
    "        #     self.hidden_state = self.init_hidden(batch_size = self.batch_size)\n",
    "        # hidden_x = self.layer_norm_in(torch.relu(self.in2hidden(x_acts.float()))).to(device)\n",
    "        # rnn_output, _ = self.rnn(hidden_x, self.hidden_state.to(device))\n",
    "        # rnn_output = self.layer_norm_rnn(rnn_output)\n",
    "        # out = self.hidden2output(torch.tanh(rnn_output))\n",
    "        \n",
    "        hidden_x = self.layer_norm_in(self.pos_emb(self.in2hidden(x_acts.float())))\n",
    "        encoder_output = self.encoder(hidden_x)\n",
    "        out = self.hidden2output(encoder_output)\n",
    "        \n",
    "        \n",
    "        return out[-1, :, :]\n",
    "        \n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1, batch_size, self.hidden_size))\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Positional encoding module taken from PyTorch Tutorial\n",
    "    # Link: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 2):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b88101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "train_TD = TaskDataset(\"datasets/train_big\")\n",
    "val_TD = TaskDataset(\"datasets/val_big\")\n",
    "                 \n",
    "batch_size = 256\n",
    "data_loaders = [DataLoader(train_TD, batch_size=batch_size, shuffle=True),\n",
    "                DataLoader(val_TD, batch_size=batch_size, shuffle=False)]\n",
    "                \n",
    "model = CNNRNNNet(hidden_size = 512, output_size = 3,).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 5, T_mult=1 )\n",
    "\n",
    "print(len(train_TD))\n",
    "print(len(val_TD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcbe7344-51ef-41cb-ab48-c2521ecc67a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the number of correct null action predictions and the number of correct non-null action predictions\n",
    "def correct(preds, targs):\n",
    "    null_idxs = torch.where(targs.cpu() == 2)\n",
    "    non_null_idxs = torch.where(targs.cpu() < 2)\n",
    "    \n",
    "    null_preds = preds[null_idxs]\n",
    "    non_null_preds = preds[non_null_idxs]\n",
    "    \n",
    "    c_null = torch.sum(null_preds == targs[null_idxs])\n",
    "    n_null = len(null_preds)\n",
    "    null_acc = c_null/n_null\n",
    "    \n",
    "    c_non_null = torch.sum(non_null_preds == targs[non_null_idxs])\n",
    "    n_non_null = len(non_null_preds)\n",
    "    non_null_acc = c_non_null/n_non_null\n",
    "    \n",
    "    return null_acc, non_null_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f581fced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, current loss 0.77, TRAIN current acc 0.51\n",
      "epoch 0, current loss 0.69, VAL current acc 0.51\n",
      "epoch 1, current loss 0.69, TRAIN current acc 0.52\n",
      "epoch 1, current loss 0.70, VAL current acc 0.50\n",
      "epoch 2, current loss 0.69, TRAIN current acc 0.54\n",
      "epoch 2, current loss 0.70, VAL current acc 0.49\n",
      "epoch 3, current loss 0.67, TRAIN current acc 0.58\n",
      "epoch 3, current loss 0.71, VAL current acc 0.49\n",
      "epoch 4, current loss 0.65, TRAIN current acc 0.63\n",
      "epoch 4, current loss 0.73, VAL current acc 0.50\n",
      "epoch 5, current loss 0.69, TRAIN current acc 0.55\n",
      "epoch 5, current loss 0.71, VAL current acc 0.50\n",
      "epoch 6, current loss 0.62, TRAIN current acc 0.64\n",
      "epoch 6, current loss 0.63, VAL current acc 0.60\n",
      "epoch 7, current loss 0.47, TRAIN current acc 0.76\n",
      "epoch 7, current loss 0.72, VAL current acc 0.61\n",
      "epoch 8, current loss 0.34, TRAIN current acc 0.84\n",
      "epoch 8, current loss 0.85, VAL current acc 0.62\n",
      "epoch 9, current loss 0.25, TRAIN current acc 0.90\n",
      "epoch 9, current loss 0.82, VAL current acc 0.65\n",
      "epoch 10, current loss 0.36, TRAIN current acc 0.84\n",
      "epoch 10, current loss 0.71, VAL current acc 0.69\n",
      "epoch 11, current loss 0.22, TRAIN current acc 0.91\n",
      "epoch 11, current loss 0.77, VAL current acc 0.71\n",
      "epoch 12, current loss 0.14, TRAIN current acc 0.95\n",
      "epoch 12, current loss 0.94, VAL current acc 0.71\n",
      "epoch 13, current loss 0.06, TRAIN current acc 0.98\n",
      "epoch 13, current loss 1.05, VAL current acc 0.72\n",
      "epoch 14, current loss 0.04, TRAIN current acc 0.99\n",
      "epoch 14, current loss 1.05, VAL current acc 0.72\n",
      "epoch 15, current loss 0.17, TRAIN current acc 0.93\n",
      "epoch 15, current loss 0.94, VAL current acc 0.72\n",
      "epoch 16, current loss 0.08, TRAIN current acc 0.97\n",
      "epoch 16, current loss 0.95, VAL current acc 0.74\n",
      "epoch 17, current loss 0.05, TRAIN current acc 0.98\n",
      "epoch 17, current loss 1.35, VAL current acc 0.71\n",
      "epoch 18, current loss 0.02, TRAIN current acc 0.99\n",
      "epoch 18, current loss 1.20, VAL current acc 0.73\n",
      "epoch 19, current loss 0.01, TRAIN current acc 1.00\n",
      "epoch 19, current loss 1.22, VAL current acc 0.73\n",
      "epoch 20, current loss 0.10, TRAIN current acc 0.96\n",
      "epoch 20, current loss 1.18, VAL current acc 0.71\n",
      "epoch 21, current loss 0.06, TRAIN current acc 0.98\n",
      "epoch 21, current loss 1.07, VAL current acc 0.74\n",
      "epoch 22, current loss 0.03, TRAIN current acc 0.99\n",
      "epoch 22, current loss 1.30, VAL current acc 0.74\n",
      "epoch 23, current loss 0.02, TRAIN current acc 1.00\n",
      "epoch 23, current loss 1.30, VAL current acc 0.73\n",
      "epoch 24, current loss 0.01, TRAIN current acc 1.00\n",
      "epoch 24, current loss 1.30, VAL current acc 0.73\n",
      "epoch 25, current loss 0.04, TRAIN current acc 0.98\n",
      "epoch 25, current loss 1.01, VAL current acc 0.75\n",
      "epoch 26, current loss 0.05, TRAIN current acc 0.98\n",
      "epoch 26, current loss 1.45, VAL current acc 0.70\n",
      "epoch 27, current loss 0.04, TRAIN current acc 0.99\n",
      "epoch 27, current loss 1.14, VAL current acc 0.75\n",
      "epoch 28, current loss 0.02, TRAIN current acc 0.99\n",
      "epoch 28, current loss 1.35, VAL current acc 0.73\n",
      "epoch 29, current loss 0.01, TRAIN current acc 1.00\n",
      "epoch 29, current loss 1.32, VAL current acc 0.74\n",
      "epoch 30, current loss 0.03, TRAIN current acc 0.99\n",
      "epoch 30, current loss 1.35, VAL current acc 0.73\n",
      "epoch 31, current loss 0.04, TRAIN current acc 0.99\n",
      "epoch 31, current loss 1.31, VAL current acc 0.73\n",
      "epoch 32, current loss 0.01, TRAIN current acc 1.00\n",
      "epoch 32, current loss 1.36, VAL current acc 0.74\n",
      "epoch 33, current loss 0.01, TRAIN current acc 1.00\n",
      "epoch 33, current loss 1.39, VAL current acc 0.74\n",
      "epoch 34, current loss 0.01, TRAIN current acc 1.00\n",
      "epoch 34, current loss 1.42, VAL current acc 0.74\n",
      "epoch 35, current loss 0.01, TRAIN current acc 1.00\n",
      "epoch 35, current loss 1.30, VAL current acc 0.75\n",
      "epoch 36, current loss 0.04, TRAIN current acc 0.99\n",
      "epoch 36, current loss 1.25, VAL current acc 0.74\n",
      "epoch 37, current loss 0.02, TRAIN current acc 0.99\n",
      "epoch 37, current loss 1.35, VAL current acc 0.75\n",
      "epoch 38, current loss 0.01, TRAIN current acc 1.00\n",
      "epoch 38, current loss 1.45, VAL current acc 0.73\n",
      "epoch 39, current loss 0.01, TRAIN current acc 1.00\n",
      "epoch 39, current loss 1.37, VAL current acc 0.74\n",
      "epoch 40, current loss 0.04, TRAIN current acc 0.99\n",
      "epoch 40, current loss 1.26, VAL current acc 0.74\n",
      "epoch 41, current loss 0.03, TRAIN current acc 0.99\n",
      "epoch 41, current loss 1.10, VAL current acc 0.76\n",
      "epoch 42, current loss 0.02, TRAIN current acc 0.99\n",
      "epoch 42, current loss 1.32, VAL current acc 0.75\n",
      "epoch 43, current loss 0.01, TRAIN current acc 1.00\n",
      "epoch 43, current loss 1.40, VAL current acc 0.75\n",
      "epoch 44, current loss 0.01, TRAIN current acc 1.00\n",
      "epoch 44, current loss 1.36, VAL current acc 0.75\n",
      "epoch 45, current loss 0.04, TRAIN current acc 0.99\n",
      "epoch 45, current loss 1.23, VAL current acc 0.73\n",
      "epoch 46, current loss 0.04, TRAIN current acc 0.99\n",
      "epoch 46, current loss 1.34, VAL current acc 0.74\n",
      "epoch 47, current loss 0.02, TRAIN current acc 0.99\n",
      "epoch 47, current loss 1.19, VAL current acc 0.77\n",
      "epoch 48, current loss 0.01, TRAIN current acc 1.00\n",
      "epoch 48, current loss 1.26, VAL current acc 0.76\n",
      "epoch 49, current loss 0.01, TRAIN current acc 1.00\n",
      "epoch 49, current loss 1.24, VAL current acc 0.76\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    c = 0\n",
    "    for i, data_loader in enumerate(data_loaders):\n",
    "        if i == 0: mode = \"train\"\n",
    "        else: mode = \"val\"\n",
    "        \n",
    "        accs = []\n",
    "        train_losses = []\n",
    "        for images, actions in data_loader:\n",
    "            if mode == \"train\":\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                model.eval()\n",
    "            output = model(images.to(device))\n",
    "            train_loss = criterion(output, actions.type(torch.LongTensor).to(device))\n",
    "            train_losses.append(train_loss.item())\n",
    "            if mode == \"train\":\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step(epoch + c / len(data_loaders[0]))\n",
    "                c += 1\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            acc = torch.sum(predicted == actions.to(device))/(len(predicted))\n",
    "            accs.append(acc)\n",
    "            \n",
    "        if mode == \"train\":\n",
    "            print(\"epoch %d, current loss %.2f, TRAIN current acc %.2f\" % (epoch, sum(train_losses)/len(train_losses), sum(accs)/len(accs)))\n",
    "        elif mode == \"val\":\n",
    "            print(\"epoch %d, current loss %.2f, VAL current acc %.2f\" % (epoch, sum(train_losses)/len(train_losses), sum(accs)/len(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444bd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d386e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlti kernel",
   "language": "python",
   "name": "mlti-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
